{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/K8V+MKrPq2wrzZdDgXfX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ae83559b11b4e2d9fe58aa6abc5a96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6db2186b6a83489ab5b476a0bd3ae2aa",
              "IPY_MODEL_05878862954541ddb77ab7edb4769de3",
              "IPY_MODEL_2e8b23b2999f4abbbaaaccecf94df051"
            ],
            "layout": "IPY_MODEL_954b09471cca45ab9eaa0dab9c68cecb"
          }
        },
        "6db2186b6a83489ab5b476a0bd3ae2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c8d2d3899c54a57a56b1757dcb17d2c",
            "placeholder": "​",
            "style": "IPY_MODEL_6fc159fb437d47ed8a894c2e0b795a43",
            "value": ""
          }
        },
        "05878862954541ddb77ab7edb4769de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2e34f148ad439f908db7f436019881",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ae0725ccdf646fdaa1aedcd91a69bd2",
            "value": 0
          }
        },
        "2e8b23b2999f4abbbaaaccecf94df051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af7636b7d4b645b7957fb06f66650618",
            "placeholder": "​",
            "style": "IPY_MODEL_65669b9abcba45f1a0c5af707083d570",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "954b09471cca45ab9eaa0dab9c68cecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8d2d3899c54a57a56b1757dcb17d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc159fb437d47ed8a894c2e0b795a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2e34f148ad439f908db7f436019881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8ae0725ccdf646fdaa1aedcd91a69bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af7636b7d4b645b7957fb06f66650618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65669b9abcba45f1a0c5af707083d570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "427d921f616d48c68b84b48630124248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aa7e087aaf24babb528053a52e192fe",
              "IPY_MODEL_3a423c98c1ff4fb7a15b235466891973",
              "IPY_MODEL_25e276cc46ee46198fc9365bb669f555"
            ],
            "layout": "IPY_MODEL_d3adb812a122499a8558c635f1470c22"
          }
        },
        "7aa7e087aaf24babb528053a52e192fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_290e1d4708f047319e201b798ee48885",
            "placeholder": "​",
            "style": "IPY_MODEL_fa7aecf3211f4db4b2d271e3c6d9e830",
            "value": "Downloading (…)a8e1d/.gitattributes: 100%"
          }
        },
        "3a423c98c1ff4fb7a15b235466891973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221686fea867482cb861de64385ed116",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7472c0bc72564089ab3e090362c81254",
            "value": 1175
          }
        },
        "25e276cc46ee46198fc9365bb669f555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d148c3f128b64e02a2ad8fd0d8b2f2ad",
            "placeholder": "​",
            "style": "IPY_MODEL_20e7b14bb67144ea847e14923070d624",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "d3adb812a122499a8558c635f1470c22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290e1d4708f047319e201b798ee48885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7aecf3211f4db4b2d271e3c6d9e830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "221686fea867482cb861de64385ed116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7472c0bc72564089ab3e090362c81254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d148c3f128b64e02a2ad8fd0d8b2f2ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e7b14bb67144ea847e14923070d624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e01c27cbaa24825a54b45125e00390b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9f1b0ab81d44cb9bab76bb5323d9273",
              "IPY_MODEL_df6866b93b9e4bc79fc460d36774f823",
              "IPY_MODEL_2aa68c08af4d4f3a82db39f8fc7a1686"
            ],
            "layout": "IPY_MODEL_0041eb297cfb4f6e828a996be211b972"
          }
        },
        "b9f1b0ab81d44cb9bab76bb5323d9273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9775f0fe8ebe45988073e6acf28adf80",
            "placeholder": "​",
            "style": "IPY_MODEL_23bfcb1ca1b5448ea505d8a912f21f4a",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "df6866b93b9e4bc79fc460d36774f823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1098809972f4468e99abc0d771ea0e52",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85119b164c95411a8eae81a68c91368a",
            "value": 190
          }
        },
        "2aa68c08af4d4f3a82db39f8fc7a1686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ee2363791b47e2a9bd947aec05cc6b",
            "placeholder": "​",
            "style": "IPY_MODEL_628a7c2718a7443e83edbe6764daca1e",
            "value": " 190/190 [00:00&lt;00:00, 2.83kB/s]"
          }
        },
        "0041eb297cfb4f6e828a996be211b972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9775f0fe8ebe45988073e6acf28adf80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23bfcb1ca1b5448ea505d8a912f21f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1098809972f4468e99abc0d771ea0e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85119b164c95411a8eae81a68c91368a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9ee2363791b47e2a9bd947aec05cc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "628a7c2718a7443e83edbe6764daca1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f76d1eb1ee3144449d573a48f37d3710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d151d926d2f54699a606a30ac056113c",
              "IPY_MODEL_12f5471e921247a28f302884e6246ab0",
              "IPY_MODEL_721146ab982647c79c28a81bd4f75fdb"
            ],
            "layout": "IPY_MODEL_cfc2dd3861e94e998a3323a6e25f3ba0"
          }
        },
        "d151d926d2f54699a606a30ac056113c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba2dcc52c87f42dc9cea3db0b4fea8bd",
            "placeholder": "​",
            "style": "IPY_MODEL_684c9dc166184257932407ddb0e8a26c",
            "value": "Downloading (…)b20bca8e1d/README.md: 100%"
          }
        },
        "12f5471e921247a28f302884e6246ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb9d6a6bbf314577b5c8379148a2b145",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7948445092d849bc8d7a829d883350df",
            "value": 10571
          }
        },
        "721146ab982647c79c28a81bd4f75fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc4fb855adca467ea825eebc2ff3ce08",
            "placeholder": "​",
            "style": "IPY_MODEL_d5b466e610844ea5834868223a88e4e3",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 194kB/s]"
          }
        },
        "cfc2dd3861e94e998a3323a6e25f3ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2dcc52c87f42dc9cea3db0b4fea8bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684c9dc166184257932407ddb0e8a26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb9d6a6bbf314577b5c8379148a2b145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7948445092d849bc8d7a829d883350df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc4fb855adca467ea825eebc2ff3ce08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b466e610844ea5834868223a88e4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f703c56820d441d89a48cf35ec8db68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8505f58481f44b7a9a6afa0b9b70cf5a",
              "IPY_MODEL_7bfa69fc752f42c9a6a72392064ca9e6",
              "IPY_MODEL_7410aa9c44844766a3aa22b696967480"
            ],
            "layout": "IPY_MODEL_46a844384240456b8b91a27f9ad831cb"
          }
        },
        "8505f58481f44b7a9a6afa0b9b70cf5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f3cb85b626d4210b9b738265f7988ff",
            "placeholder": "​",
            "style": "IPY_MODEL_4da96f47c43d4b369e9101151da9ee90",
            "value": "Downloading (…)0bca8e1d/config.json: 100%"
          }
        },
        "7bfa69fc752f42c9a6a72392064ca9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_176191747b2346c0a538f61cfb6fe51d",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b091be617d94383a7f2d3c4634268fe",
            "value": 571
          }
        },
        "7410aa9c44844766a3aa22b696967480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836d83ae97284e4f91742951a6a75989",
            "placeholder": "​",
            "style": "IPY_MODEL_076570802c8c48609a561a2faf9b9005",
            "value": " 571/571 [00:00&lt;00:00, 9.41kB/s]"
          }
        },
        "46a844384240456b8b91a27f9ad831cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f3cb85b626d4210b9b738265f7988ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da96f47c43d4b369e9101151da9ee90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "176191747b2346c0a538f61cfb6fe51d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b091be617d94383a7f2d3c4634268fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "836d83ae97284e4f91742951a6a75989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "076570802c8c48609a561a2faf9b9005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ebfc2038f0d48c4b632454221d2be0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a65ce49a42324fc09fc9b175721464ff",
              "IPY_MODEL_be9a63ee2c55416291299e2b03118fa9",
              "IPY_MODEL_db0e45988f2e48f7b3a59fdec3ad363e"
            ],
            "layout": "IPY_MODEL_47935e21ff624e94afd1af095931e472"
          }
        },
        "a65ce49a42324fc09fc9b175721464ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ffa7e95d96446d893b0d541694b9629",
            "placeholder": "​",
            "style": "IPY_MODEL_b56e5f2036b24f65a6fad456a6300e67",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "be9a63ee2c55416291299e2b03118fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_597af8d2fe924cf290d59c4f7752ae22",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_543ad99e70e44c69a0576f6fa29849b1",
            "value": 116
          }
        },
        "db0e45988f2e48f7b3a59fdec3ad363e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cc5921396f94d63892b5f0e32f98447",
            "placeholder": "​",
            "style": "IPY_MODEL_f2c4f6e031af49b1a3c4c32b96c6fae7",
            "value": " 116/116 [00:00&lt;00:00, 1.63kB/s]"
          }
        },
        "47935e21ff624e94afd1af095931e472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffa7e95d96446d893b0d541694b9629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56e5f2036b24f65a6fad456a6300e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "597af8d2fe924cf290d59c4f7752ae22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543ad99e70e44c69a0576f6fa29849b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cc5921396f94d63892b5f0e32f98447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c4f6e031af49b1a3c4c32b96c6fae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab32ca5ebd204e438672cb9e7e0d4485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79056073a7fb4fe1957c13b49c7cf131",
              "IPY_MODEL_741e1a3133db4b1b8964455b49463f8e",
              "IPY_MODEL_13e86a17c5b6494b9a68bc7abc995770"
            ],
            "layout": "IPY_MODEL_a112a6c5294f4f57b404dc35a0c2a831"
          }
        },
        "79056073a7fb4fe1957c13b49c7cf131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390917ee25204728aa3305a392d21689",
            "placeholder": "​",
            "style": "IPY_MODEL_2adf7572db9648b98106d504d5570507",
            "value": "Downloading (…)e1d/data_config.json: 100%"
          }
        },
        "741e1a3133db4b1b8964455b49463f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79a6cea97682478fa31d638681679212",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_477941937d294f1a8da25a5495e4736d",
            "value": 39265
          }
        },
        "13e86a17c5b6494b9a68bc7abc995770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3df91721a25a4ae4a4cf66e881c8df9f",
            "placeholder": "​",
            "style": "IPY_MODEL_966c40e2fb1c4a5da068720ee9e61be7",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 963kB/s]"
          }
        },
        "a112a6c5294f4f57b404dc35a0c2a831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "390917ee25204728aa3305a392d21689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2adf7572db9648b98106d504d5570507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79a6cea97682478fa31d638681679212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477941937d294f1a8da25a5495e4736d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3df91721a25a4ae4a4cf66e881c8df9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "966c40e2fb1c4a5da068720ee9e61be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7b50b28062c465f80b10f5ef4d9f9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdb0f63e5a404d248aa191a081b089e5",
              "IPY_MODEL_5a943c13502b4078af9c3bee4d78ea8c",
              "IPY_MODEL_62209b0e1a7649668c2e4a8cc3d81a0e"
            ],
            "layout": "IPY_MODEL_e90ac26826954c57a90b3947f3974f05"
          }
        },
        "fdb0f63e5a404d248aa191a081b089e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebbcda309d374d23933207c22eb24d64",
            "placeholder": "​",
            "style": "IPY_MODEL_f5820023ca1b405cbbe338c89d657396",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "5a943c13502b4078af9c3bee4d78ea8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5e350e53fc45378d3e79611adb66a6",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c321aaa2ef9452aadecea8551316968",
            "value": 438011953
          }
        },
        "62209b0e1a7649668c2e4a8cc3d81a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b04a6da66d4989915ea791108117e7",
            "placeholder": "​",
            "style": "IPY_MODEL_5dd7eb4878a6420586b4c5de5dddf2a4",
            "value": " 438M/438M [00:03&lt;00:00, 122MB/s]"
          }
        },
        "e90ac26826954c57a90b3947f3974f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebbcda309d374d23933207c22eb24d64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5820023ca1b405cbbe338c89d657396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5e350e53fc45378d3e79611adb66a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c321aaa2ef9452aadecea8551316968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38b04a6da66d4989915ea791108117e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd7eb4878a6420586b4c5de5dddf2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42d509907d394f0293fa2e11b4afcd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4a9bad6a3d84a0aa82523f0092fb903",
              "IPY_MODEL_0c638477821b4971ae2f3bb1e3bef43b",
              "IPY_MODEL_4731078865384c4b809e0aa1a1d5aa6f"
            ],
            "layout": "IPY_MODEL_ac06166affca477dbc183cf2449f7e8c"
          }
        },
        "e4a9bad6a3d84a0aa82523f0092fb903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a36d495595974c63af1d4aee6c5d1d74",
            "placeholder": "​",
            "style": "IPY_MODEL_3d1ff1d4ff8742fc8815e7690d9f3179",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "0c638477821b4971ae2f3bb1e3bef43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0aa7f08ec04861933d6e93ab04f79e",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dffcb7e870144c29a2de4860f85f67d9",
            "value": 53
          }
        },
        "4731078865384c4b809e0aa1a1d5aa6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35aaa1637818468c81b7775f221cc267",
            "placeholder": "​",
            "style": "IPY_MODEL_543774c7710342b7b2a165d764d28e7f",
            "value": " 53.0/53.0 [00:00&lt;00:00, 894B/s]"
          }
        },
        "ac06166affca477dbc183cf2449f7e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a36d495595974c63af1d4aee6c5d1d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d1ff1d4ff8742fc8815e7690d9f3179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d0aa7f08ec04861933d6e93ab04f79e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dffcb7e870144c29a2de4860f85f67d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35aaa1637818468c81b7775f221cc267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543774c7710342b7b2a165d764d28e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b95e39f47f34f3498ea301c69350285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d54e9f4e53784237a2299a8a2bf61e37",
              "IPY_MODEL_54b9a22613c5493380a7964b52be7899",
              "IPY_MODEL_88d281dde0624e5680061fceae2dd931"
            ],
            "layout": "IPY_MODEL_03385f540b3643848dc6d6aa9601b21e"
          }
        },
        "d54e9f4e53784237a2299a8a2bf61e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8610bedc97842dea1764afaa1ed33b8",
            "placeholder": "​",
            "style": "IPY_MODEL_00726846fbda49a596341894a0e5a5e8",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "54b9a22613c5493380a7964b52be7899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac96dec71104e49a191c38d31b28c7b",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5341a643726046218510ef6a96a185fa",
            "value": 239
          }
        },
        "88d281dde0624e5680061fceae2dd931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b842632ebb1a4a84a0fb58af01b5392f",
            "placeholder": "​",
            "style": "IPY_MODEL_6e2bcc65628e461e87d9ac4a1fab680d",
            "value": " 239/239 [00:00&lt;00:00, 6.42kB/s]"
          }
        },
        "03385f540b3643848dc6d6aa9601b21e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8610bedc97842dea1764afaa1ed33b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00726846fbda49a596341894a0e5a5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ac96dec71104e49a191c38d31b28c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5341a643726046218510ef6a96a185fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b842632ebb1a4a84a0fb58af01b5392f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2bcc65628e461e87d9ac4a1fab680d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83fdc910aff749e881a3d4dd8d215273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_592b6291fbb1464ea1c1cebb34fe3ae5",
              "IPY_MODEL_14101fa6beaa4e1fadd05e160db47c09",
              "IPY_MODEL_d1e19a8e0a8145debe512aefe5a3b404"
            ],
            "layout": "IPY_MODEL_05e78e3776a9446a8e75382cbead308a"
          }
        },
        "592b6291fbb1464ea1c1cebb34fe3ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8309279ce7542499b9d0aba749a37e9",
            "placeholder": "​",
            "style": "IPY_MODEL_473d1e9252e94ac6aca022ff4aa51b56",
            "value": "Downloading (…)a8e1d/tokenizer.json: 100%"
          }
        },
        "14101fa6beaa4e1fadd05e160db47c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a90cfba72064dc791ac54028c8940c5",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3add2f864f54ce2893dd5131073959b",
            "value": 466021
          }
        },
        "d1e19a8e0a8145debe512aefe5a3b404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3306b42d03d04e0abd8627c5cb74ac3e",
            "placeholder": "​",
            "style": "IPY_MODEL_918379d2943749f7aca904453e31fb05",
            "value": " 466k/466k [00:00&lt;00:00, 3.14MB/s]"
          }
        },
        "05e78e3776a9446a8e75382cbead308a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8309279ce7542499b9d0aba749a37e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473d1e9252e94ac6aca022ff4aa51b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a90cfba72064dc791ac54028c8940c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3add2f864f54ce2893dd5131073959b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3306b42d03d04e0abd8627c5cb74ac3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "918379d2943749f7aca904453e31fb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca80c7f809624476bcd482dc07d294f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62dbeee55eee47e3b3340622a9811df9",
              "IPY_MODEL_ee2c803db17546308b128b1d004404e1",
              "IPY_MODEL_86db3a836e214fcbb315c9582051e934"
            ],
            "layout": "IPY_MODEL_082fb62422a843c19bee6cd0e3aafec2"
          }
        },
        "62dbeee55eee47e3b3340622a9811df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ff121b30bd647f1a7b0f7c53e250c02",
            "placeholder": "​",
            "style": "IPY_MODEL_05ba914bec2a4adc96a4b4c58ae672b5",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "ee2c803db17546308b128b1d004404e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c767ebaa57c34eb7b7f714a518db91f0",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95bee6c62a7c4ef4b07caa9799b36d4f",
            "value": 363
          }
        },
        "86db3a836e214fcbb315c9582051e934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46b957bcb16d479e9a7c5cb9b1fbb3f7",
            "placeholder": "​",
            "style": "IPY_MODEL_48ea82547ceb4acab2c91fdcfc2db363",
            "value": " 363/363 [00:00&lt;00:00, 5.90kB/s]"
          }
        },
        "082fb62422a843c19bee6cd0e3aafec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ff121b30bd647f1a7b0f7c53e250c02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05ba914bec2a4adc96a4b4c58ae672b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c767ebaa57c34eb7b7f714a518db91f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95bee6c62a7c4ef4b07caa9799b36d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46b957bcb16d479e9a7c5cb9b1fbb3f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48ea82547ceb4acab2c91fdcfc2db363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e939bc23b744f8a9f1730870d0716d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f38db89804d84aa8a1e3c0532cb47068",
              "IPY_MODEL_5ebfa2b84a4e4a31a0888d94fef5deee",
              "IPY_MODEL_69ac8f580561422b91670de79cd03a4d"
            ],
            "layout": "IPY_MODEL_a1cd7fc336944d549a7cf70a06b64203"
          }
        },
        "f38db89804d84aa8a1e3c0532cb47068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f83c525c1277487b8fb8c60e6f5eac52",
            "placeholder": "​",
            "style": "IPY_MODEL_ea9efaebe0e4498cbfa78c9dea0d0293",
            "value": "Downloading (…)8e1d/train_script.py: 100%"
          }
        },
        "5ebfa2b84a4e4a31a0888d94fef5deee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b28c2d608ea47268c1dbaa16e52ae30",
            "max": 13123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bfbe2c767144efabacd4dd1eefe2e14",
            "value": 13123
          }
        },
        "69ac8f580561422b91670de79cd03a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc8aa6729eb34d57806d40c6cdfc6ad4",
            "placeholder": "​",
            "style": "IPY_MODEL_48b12856a1204004a2c82cbe7187e74f",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 138kB/s]"
          }
        },
        "a1cd7fc336944d549a7cf70a06b64203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83c525c1277487b8fb8c60e6f5eac52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea9efaebe0e4498cbfa78c9dea0d0293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b28c2d608ea47268c1dbaa16e52ae30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bfbe2c767144efabacd4dd1eefe2e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc8aa6729eb34d57806d40c6cdfc6ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b12856a1204004a2c82cbe7187e74f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe01942c9a0b44db93d9ece417efbd26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27a2a8a8f1aa4b6bb8df0c7a4510cd8f",
              "IPY_MODEL_df5c130b98174099ab89d030c6fe48f2",
              "IPY_MODEL_c28ca8700c564cd694e71f4f86da8a4c"
            ],
            "layout": "IPY_MODEL_f8e86f396c2845a2b2806f9c8b09b1be"
          }
        },
        "27a2a8a8f1aa4b6bb8df0c7a4510cd8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccdb7f2a4cba4bc4a3e2b87429194d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_19e70173fe674d6187315d64f58c8883",
            "value": "Downloading (…)b20bca8e1d/vocab.txt: 100%"
          }
        },
        "df5c130b98174099ab89d030c6fe48f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41b58d0c6a914168a7c67d1ba51b1ddb",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c10a9d2060a41c6b5289aae6a6f297f",
            "value": 231536
          }
        },
        "c28ca8700c564cd694e71f4f86da8a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32fb99b839a946d48f627aaafede39af",
            "placeholder": "​",
            "style": "IPY_MODEL_c0a421556c454d22adf4bd82c174dfa3",
            "value": " 232k/232k [00:00&lt;00:00, 1.62MB/s]"
          }
        },
        "f8e86f396c2845a2b2806f9c8b09b1be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdb7f2a4cba4bc4a3e2b87429194d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e70173fe674d6187315d64f58c8883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41b58d0c6a914168a7c67d1ba51b1ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c10a9d2060a41c6b5289aae6a6f297f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32fb99b839a946d48f627aaafede39af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a421556c454d22adf4bd82c174dfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c87a103d23cc47158c689b3e7fe75ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee9a98f6e1504ecfb0e942f5d3c856ee",
              "IPY_MODEL_274007015e9f4894a93813c55ca93c97",
              "IPY_MODEL_8e4870c4bab940caaa299a9feaffeea2"
            ],
            "layout": "IPY_MODEL_1a84477a0a944827913ac7901041d1f1"
          }
        },
        "ee9a98f6e1504ecfb0e942f5d3c856ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cfb9d9d0df3408e9de65c6cff594968",
            "placeholder": "​",
            "style": "IPY_MODEL_26f4013d8e874336973dc8205c324fd7",
            "value": "Downloading (…)bca8e1d/modules.json: 100%"
          }
        },
        "274007015e9f4894a93813c55ca93c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f79d2b47604365a12b77fdba061aa4",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a804ce499c3a4a9d8a0092e750ab4d70",
            "value": 349
          }
        },
        "8e4870c4bab940caaa299a9feaffeea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4471bc04619e4a83b57eb0be223141b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e661b6176921470fb57076213bf5c318",
            "value": " 349/349 [00:00&lt;00:00, 5.86kB/s]"
          }
        },
        "1a84477a0a944827913ac7901041d1f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cfb9d9d0df3408e9de65c6cff594968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f4013d8e874336973dc8205c324fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55f79d2b47604365a12b77fdba061aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a804ce499c3a4a9d8a0092e750ab4d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4471bc04619e4a83b57eb0be223141b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e661b6176921470fb57076213bf5c318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoxiang66/research-trends-analysis/blob/experiments/experiments/clustering_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TrendFlow\n",
        "!pip install ml_leoxiang66==0.6.1 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd5XHjLBaCZ7",
        "outputId": "9538e933-86e0-4420-e768-8fcc4b86b492"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting TrendFlow\n",
            "  Downloading TrendFlow-0.1.2-py3-none-any.whl (22 kB)\n",
            "Collecting evaluate==0.2.2\n",
            "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting KeyBartAdapter==0.1.12\n",
            "  Downloading KeyBartAdapter-0.1.12-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from TrendFlow) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.8/dist-packages (from TrendFlow) (1.0.2)\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kmeans-pytorch==0.3\n",
            "  Downloading kmeans_pytorch-0.3-py3-none-any.whl (4.4 kB)\n",
            "Collecting ml-leoxiang66==0.5.3\n",
            "  Downloading ml_leoxiang66-0.5.3-py3-none-any.whl (8.6 kB)\n",
            "Collecting datasets==2.5.2\n",
            "  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.7/432.7 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolkit-stable==0.8.0\n",
            "  Downloading requests_toolkit_stable-0.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting textdistance==4.5.0\n",
            "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
            "Collecting transformers==4.22.1\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from TrendFlow) (1.13.1+cu116)\n",
            "Requirement already satisfied: yellowbrick==1.5 in /usr/local/lib/python3.8/dist-packages (from TrendFlow) (1.5)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (2023.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (3.8.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->TrendFlow) (2.25.1)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from ml-leoxiang66==0.5.3->TrendFlow) (2.11.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from ml-leoxiang66==0.5.3->TrendFlow) (3.5.3)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from requests-toolkit-stable==0.8.0->TrendFlow) (4.9.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->TrendFlow) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->TrendFlow) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->TrendFlow) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers==2.2.2->TrendFlow) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers==2.2.2->TrendFlow) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.22.1->TrendFlow) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.22.1->TrendFlow) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.22.1->TrendFlow) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from yellowbrick==1.5->TrendFlow) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->TrendFlow) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->TrendFlow) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->TrendFlow) (2.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (3.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->TrendFlow) (4.0.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml-leoxiang66==0.5.3->TrendFlow) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml-leoxiang66==0.5.3->TrendFlow) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml-leoxiang66==0.5.3->TrendFlow) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml-leoxiang66==0.5.3->TrendFlow) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->TrendFlow) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->TrendFlow) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->TrendFlow) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->TrendFlow) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->TrendFlow) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers==2.2.2->TrendFlow) (7.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (1.51.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (3.4.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (0.4.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->ml-leoxiang66==0.5.3->TrendFlow) (5.4.8)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb->ml-leoxiang66==0.5.3->TrendFlow) (1.4.4)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (6.0.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (3.14.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->ml-leoxiang66==0.5.3->TrendFlow) (3.2.2)\n",
            "Building wheels for collected packages: sentence-transformers, pathtools\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=3b9324b6d7c1981fa6b92ef74b6416693e2fc0d32aaa76286ccb128da54c1bf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=57556eb8be0f49d57adb103f4d48ec3ecf73b773573eb1387aa508b245b39314\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built sentence-transformers pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, pathtools, xxhash, xmltodict, urllib3, textdistance, smmap, setproctitle, kmeans-pytorch, docker-pycreds, dill, sentry-sdk, multiprocess, gitdb, responses, requests-toolkit-stable, huggingface-hub, GitPython, wandb, transformers, datasets, sentence-transformers, KeyBartAdapter, evaluate, ml-leoxiang66, TrendFlow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed GitPython-3.1.31 KeyBartAdapter-0.1.12 TrendFlow-0.1.2 datasets-2.5.2 dill-0.3.5.1 docker-pycreds-0.4.0 evaluate-0.2.2 gitdb-4.0.10 huggingface-hub-0.12.1 kmeans-pytorch-0.3 ml-leoxiang66-0.5.3 multiprocess-0.70.13 pathtools-0.1.2 requests-toolkit-stable-0.8.0 responses-0.18.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 textdistance-4.5.0 tokenizers-0.12.1 transformers-4.22.1 urllib3-1.26.14 wandb-0.13.10 xmltodict-0.13.0 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml_leoxiang66==0.6.1\n",
            "  Downloading ml_leoxiang66-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (from ml_leoxiang66==0.6.1) (0.13.10)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from ml_leoxiang66==0.6.1) (2.11.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from ml_leoxiang66==0.6.1) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ml_leoxiang66==0.6.1) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from ml_leoxiang66==0.6.1) (3.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from ml_leoxiang66==0.6.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (4.38.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ml_leoxiang66==0.6.1) (1.4.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->ml_leoxiang66==0.6.1) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->ml_leoxiang66==0.6.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->ml_leoxiang66==0.6.1) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (1.51.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (2.25.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (3.19.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (2.16.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->ml_leoxiang66==0.6.1) (0.4.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->ml_leoxiang66==0.6.1) (4.5.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (1.3.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (1.4.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (1.16.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (5.4.8)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->ml_leoxiang66==0.6.1) (3.1.31)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb->ml_leoxiang66==0.6.1) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb->ml_leoxiang66==0.6.1) (4.0.10)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->ml_leoxiang66==0.6.1) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->ml_leoxiang66==0.6.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->ml_leoxiang66==0.6.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->ml_leoxiang66==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard->ml_leoxiang66==0.6.1) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->ml_leoxiang66==0.6.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->ml_leoxiang66==0.6.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->ml_leoxiang66==0.6.1) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->ml_leoxiang66==0.6.1) (2.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->ml_leoxiang66==0.6.1) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->ml_leoxiang66==0.6.1) (3.14.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->ml_leoxiang66==0.6.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->ml_leoxiang66==0.6.1) (3.2.2)\n",
            "Installing collected packages: ml_leoxiang66\n",
            "  Attempting uninstall: ml_leoxiang66\n",
            "    Found existing installation: ml-leoxiang66 0.5.3\n",
            "    Uninstalling ml-leoxiang66-0.5.3:\n",
            "      Successfully uninstalled ml-leoxiang66-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trendflow 0.1.2 requires ml-leoxiang66==0.5.3, but you have ml-leoxiang66 0.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml_leoxiang66-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/datasets/Adapting/Abstracts-for-Clustering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUHTSV0eWwIN",
        "outputId": "3f99e410-49de-4bb6-a426-573c8f8052a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'Abstracts-for-Clustering'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 35 (delta 17), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), 79.71 KiB | 3.07 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pL87U_2fXEVF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df = pd.read_csv('/content/Abstracts-for-Clustering/CV abstracts.csv')\n",
        "nlp_df = pd.read_csv('/content/Abstracts-for-Clustering/NLP abstracts.csv')\n",
        "audio_df = pd.read_csv('/content/Abstracts-for-Clustering/audio abstracts.csv')"
      ],
      "metadata": {
        "id": "0Ba7dM3hXVN1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "WZyUEBy8XiNm",
        "outputId": "3796cb97-f7e9-4d65-9b81-f8ace668eec4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0\n",
              "0   Humans can easily imagine the complete 3D geom...\n",
              "1   Single object tracking is a well-known and cha...\n",
              "2   Cracks play a crucial role in assessing the sa...\n",
              "3   Fine-grained anomaly detection has recently be...\n",
              "4   Synthetic image generation has opened up new o...\n",
              "..                                                ...\n",
              "87  Current Deep Network (DN) visualization and in...\n",
              "88  Visibility underwater is challenging, and degr...\n",
              "89  Malignant mesothelioma is classified into thre...\n",
              "90  Omnidirectional image quality assessment (OIQA...\n",
              "91  Layer-wise relevance propagation (LRP) is a wi...\n",
              "\n",
              "[92 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e421e537-1813-4a46-8ce3-1e072339eadf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Humans can easily imagine the complete 3D geom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Single object tracking is a well-known and cha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cracks play a crucial role in assessing the sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fine-grained anomaly detection has recently be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Synthetic image generation has opened up new o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Current Deep Network (DN) visualization and in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Visibility underwater is challenging, and degr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Malignant mesothelioma is classified into thre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>Omnidirectional image quality assessment (OIQA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Layer-wise relevance propagation (LRP) is a wi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>92 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e421e537-1813-4a46-8ce3-1e072339eadf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e421e537-1813-4a46-8ce3-1e072339eadf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e421e537-1813-4a46-8ce3-1e072339eadf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df['label'] = 0\n",
        "cv_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xEd2vpm1sRer",
        "outputId": "0f5f33f2-145f-4e67-bd34-d2c6d2b73ef1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0  label\n",
              "0   Humans can easily imagine the complete 3D geom...      0\n",
              "1   Single object tracking is a well-known and cha...      0\n",
              "2   Cracks play a crucial role in assessing the sa...      0\n",
              "3   Fine-grained anomaly detection has recently be...      0\n",
              "4   Synthetic image generation has opened up new o...      0\n",
              "..                                                ...    ...\n",
              "87  Current Deep Network (DN) visualization and in...      0\n",
              "88  Visibility underwater is challenging, and degr...      0\n",
              "89  Malignant mesothelioma is classified into thre...      0\n",
              "90  Omnidirectional image quality assessment (OIQA...      0\n",
              "91  Layer-wise relevance propagation (LRP) is a wi...      0\n",
              "\n",
              "[92 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b3e93ed-1240-4df9-b6dd-dc36e15d8092\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Humans can easily imagine the complete 3D geom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Single object tracking is a well-known and cha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cracks play a crucial role in assessing the sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fine-grained anomaly detection has recently be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Synthetic image generation has opened up new o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Current Deep Network (DN) visualization and in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Visibility underwater is challenging, and degr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Malignant mesothelioma is classified into thre...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>Omnidirectional image quality assessment (OIQA...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Layer-wise relevance propagation (LRP) is a wi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>92 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b3e93ed-1240-4df9-b6dd-dc36e15d8092')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b3e93ed-1240-4df9-b6dd-dc36e15d8092 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b3e93ed-1240-4df9-b6dd-dc36e15d8092');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df['label'] = 1\n",
        "nlp_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XYOZrlJyss1w",
        "outputId": "82071c7f-23a0-4a8b-cf18-9e1152096d7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0  label\n",
              "0   This paper describes our participation in SemE...      1\n",
              "1   Clinical prediction is an essential task in th...      1\n",
              "2   Knowledge graphs (KGs) have received increasin...      1\n",
              "3   Transformer-based pre-trained models have achi...      1\n",
              "4   Large language models (LLMs), such as ChatGPT,...      1\n",
              "..                                                ...    ...\n",
              "62  The widespread availability of internet access...      1\n",
              "63  Current captioning datasets, focus on object-c...      1\n",
              "64  In this paper, we summarize the current state ...      1\n",
              "65  The multi-sentential long sequence textual dat...      1\n",
              "66  Natural language provides a powerful modality ...      1\n",
              "\n",
              "[67 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a1696ca-f367-4914-a69c-e00a75f23887\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This paper describes our participation in SemE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clinical prediction is an essential task in th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Knowledge graphs (KGs) have received increasin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transformer-based pre-trained models have achi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Large language models (LLMs), such as ChatGPT,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>The widespread availability of internet access...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>Current captioning datasets, focus on object-c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>In this paper, we summarize the current state ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>The multi-sentential long sequence textual dat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>Natural language provides a powerful modality ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a1696ca-f367-4914-a69c-e00a75f23887')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a1696ca-f367-4914-a69c-e00a75f23887 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a1696ca-f367-4914-a69c-e00a75f23887');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_df['label'] = 2\n",
        "audio_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XIRkP7Nfs88L",
        "outputId": "cf21628d-eba2-46ea-bb73-1a4331505072"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0  label\n",
              "0   We present ProsAudit, a benchmark in English t...      2\n",
              "1   The increasing reliability of automatic speech...      2\n",
              "2   Deep neural network based speech enhancement t...      2\n",
              "3   Deep neural network based speech enhancement a...      2\n",
              "4   Monaural speech enhancement has been widely st...      2\n",
              "5   Transformer based models have provided signifi...      2\n",
              "6   In this work, we propose a frequency bin-wise ...      2\n",
              "7   In this study, we present an approach to train...      2\n",
              "8   Speech utterances recorded under differing con...      2\n",
              "9   In this work, we propose a frequency bin-wise ...      2\n",
              "10  In this study, we present an approach to train...      2\n",
              "11  Speech utterances recorded under differing con...      2\n",
              "12  We present ProsAudit, a benchmark in English t...      2\n",
              "13  The increasing reliability of automatic speech...      2\n",
              "14  Deep neural network based speech enhancement t...      2\n",
              "15  Deep neural network based speech enhancement a...      2\n",
              "16  Monaural speech enhancement has been widely st...      2\n",
              "17  Transformer based models have provided signifi...      2\n",
              "18  Multilingual Automatic Speech Recognition (ASR...      2\n",
              "19  We study multi-task learning for two orthogona...      2\n",
              "20  Conventional methods for speaker diarization i...      2\n",
              "21  Voice conversion (VC) techniques can be abused...      2\n",
              "22  The recent progress in text-based audio retrie...      2\n",
              "23  Distilled self-supervised models have shown co...      2\n",
              "24  Previous pitch-controllable text-to-speech (TT...      2\n",
              "25  This paper presents a novel optimization frame...      2\n",
              "26  Distilled self-supervised models have shown co...      2\n",
              "27  Previous pitch-controllable text-to-speech (TT...      2\n",
              "28  This paper presents a novel optimization frame...      2\n",
              "29  Multilingual Automatic Speech Recognition (ASR...      2\n",
              "30  We study multi-task learning for two orthogona...      2\n",
              "31  Conventional methods for speaker diarization i...      2\n",
              "32  Voice conversion (VC) techniques can be abused...      2\n",
              "33  The recent progress in text-based audio retrie...      2\n",
              "34  In recent years, reinforcement learning and ba...      2\n",
              "35  Existing deep learning based speech enhancemen...      2\n",
              "36  Stuttering is a neuro-developmental speech imp...      2\n",
              "37  Visual speech (i.e., lip motion) is highly rel...      2\n",
              "38  End-to-end automatic speech recognition (ASR) ...      2\n",
              "39  We previously proposed contextual spelling cor...      2\n",
              "40  Orcinus orca (killer whales) exhibit complex c...      2\n",
              "41  Speaker diarization is a task to label an audi...      2\n",
              "42  Visual speech recognition models extract visua...      2\n",
              "43  Speech enhancement (SE) is proved effective in...      2\n",
              "44  A study is presented in which a contrastive le...      2\n",
              "45  Word-piece models (WPMs) are commonly used sub...      2\n",
              "46  Recent studies in neural network-based monaura...      2\n",
              "47  Speech enhancement (SE) is proved effective in...      2\n",
              "48  A study is presented in which a contrastive le...      2\n",
              "49  Word-piece models (WPMs) are commonly used sub...      2\n",
              "50  Recent studies in neural network-based monaura...      2\n",
              "51  Existing deep learning based speech enhancemen...      2\n",
              "52  Stuttering is a neuro-developmental speech imp...      2\n",
              "53  Visual speech (i.e., lip motion) is highly rel...      2\n",
              "54  End-to-end automatic speech recognition (ASR) ...      2\n",
              "55  We previously proposed contextual spelling cor...      2\n",
              "56  Orcinus orca (killer whales) exhibit complex c...      2\n",
              "57  Speaker diarization is a task to label an audi...      2\n",
              "58  Visual speech recognition models extract visua...      2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf0dd6f6-bcfc-4ddf-bc45-d5832245a57a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We present ProsAudit, a benchmark in English t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The increasing reliability of automatic speech...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Deep neural network based speech enhancement t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Deep neural network based speech enhancement a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Monaural speech enhancement has been widely st...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Transformer based models have provided signifi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In this work, we propose a frequency bin-wise ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>In this study, we present an approach to train...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Speech utterances recorded under differing con...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>In this work, we propose a frequency bin-wise ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>In this study, we present an approach to train...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Speech utterances recorded under differing con...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>We present ProsAudit, a benchmark in English t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The increasing reliability of automatic speech...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Deep neural network based speech enhancement t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Deep neural network based speech enhancement a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Monaural speech enhancement has been widely st...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Transformer based models have provided signifi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Multilingual Automatic Speech Recognition (ASR...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>We study multi-task learning for two orthogona...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Conventional methods for speaker diarization i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Voice conversion (VC) techniques can be abused...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>The recent progress in text-based audio retrie...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Distilled self-supervised models have shown co...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Previous pitch-controllable text-to-speech (TT...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>This paper presents a novel optimization frame...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Distilled self-supervised models have shown co...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Previous pitch-controllable text-to-speech (TT...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>This paper presents a novel optimization frame...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Multilingual Automatic Speech Recognition (ASR...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>We study multi-task learning for two orthogona...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Conventional methods for speaker diarization i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Voice conversion (VC) techniques can be abused...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>The recent progress in text-based audio retrie...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>In recent years, reinforcement learning and ba...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Existing deep learning based speech enhancemen...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Stuttering is a neuro-developmental speech imp...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Visual speech (i.e., lip motion) is highly rel...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>End-to-end automatic speech recognition (ASR) ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>We previously proposed contextual spelling cor...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Orcinus orca (killer whales) exhibit complex c...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Speaker diarization is a task to label an audi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Visual speech recognition models extract visua...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Speech enhancement (SE) is proved effective in...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>A study is presented in which a contrastive le...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Word-piece models (WPMs) are commonly used sub...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Recent studies in neural network-based monaura...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Speech enhancement (SE) is proved effective in...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>A study is presented in which a contrastive le...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Word-piece models (WPMs) are commonly used sub...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Recent studies in neural network-based monaura...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Existing deep learning based speech enhancemen...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Stuttering is a neuro-developmental speech imp...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Visual speech (i.e., lip motion) is highly rel...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>End-to-end automatic speech recognition (ASR) ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>We previously proposed contextual spelling cor...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>Orcinus orca (killer whales) exhibit complex c...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Speaker diarization is a task to label an audi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>Visual speech recognition models extract visua...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf0dd6f6-bcfc-4ddf-bc45-d5832245a57a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf0dd6f6-bcfc-4ddf-bc45-d5832245a57a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf0dd6f6-bcfc-4ddf-bc45-d5832245a57a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = pd.concat([cv_df, nlp_df, audio_df], ignore_index=True)\n",
        "# all_df = all_df.sample(frac=1, ignore_index=True)"
      ],
      "metadata": {
        "id": "2DKhWjTJX8lp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9CBcaSOcYrVl",
        "outputId": "1e9b4ba2-a158-49c8-e698-2a53fdbd7ed8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     0  label\n",
              "0    Humans can easily imagine the complete 3D geom...      0\n",
              "1    Single object tracking is a well-known and cha...      0\n",
              "2    Cracks play a crucial role in assessing the sa...      0\n",
              "3    Fine-grained anomaly detection has recently be...      0\n",
              "4    Synthetic image generation has opened up new o...      0\n",
              "..                                                 ...    ...\n",
              "213  End-to-end automatic speech recognition (ASR) ...      2\n",
              "214  We previously proposed contextual spelling cor...      2\n",
              "215  Orcinus orca (killer whales) exhibit complex c...      2\n",
              "216  Speaker diarization is a task to label an audi...      2\n",
              "217  Visual speech recognition models extract visua...      2\n",
              "\n",
              "[218 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60e58e18-3459-4d40-a821-4d6d25c28e0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Humans can easily imagine the complete 3D geom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Single object tracking is a well-known and cha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cracks play a crucial role in assessing the sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fine-grained anomaly detection has recently be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Synthetic image generation has opened up new o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>End-to-end automatic speech recognition (ASR) ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>We previously proposed contextual spelling cor...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>Orcinus orca (killer whales) exhibit complex c...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>Speaker diarization is a task to label an audi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>Visual speech recognition models extract visua...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>218 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60e58e18-3459-4d40-a821-4d6d25c28e0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60e58e18-3459-4d40-a821-4d6d25c28e0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60e58e18-3459-4d40-a821-4d6d25c28e0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(all_df['0'])\n",
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpWpX0C1Z7lf",
        "outputId": "93817085-8df9-4d7a-acdb-7c14a39f8aa5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show that VoxFormer outperforms the state of the art with a relative improvement of 20.0% in geometry and 18.1% in semantics and reduces GPU memory during training by ~45% to less than 16GB. Our code is available on https://github.com/NVlabs/VoxFormer.',\n",
              " 'Single object tracking is a well-known and challenging research topic in computer vision. Over the last two decades, numerous researchers have proposed various algorithms to solve this problem and achieved promising results. Recently, Transformer-based tracking approaches have ushered in a new era in single object tracking due to their superior tracking robustness. Although several survey studies have been conducted to analyze the performance of trackers, there is a need for another survey study after the introduction of Transformers in single object tracking. In this survey, we aim to analyze the literature and performances of Transformer tracking approaches. Therefore, we conduct an in-depth literature analysis of Transformer tracking approaches and evaluate their tracking robustness and computational efficiency on challenging benchmark datasets. In addition, we have measured their performances on different tracking scenarios to find their strength and weaknesses. Our survey provides insights into the underlying principles of Transformer tracking approaches, the challenges they face, and their future directions.',\n",
              " 'Cracks play a crucial role in assessing the safety and durability of manufactured buildings. However, the long and sharp topological features and complex background of cracks make the task of crack segmentation extremely challenging. In this paper, we propose a novel convolutional-transformer network based on encoder-decoder architecture to solve this challenge. Particularly, we designed a Dilated Residual Block (DRB) and a Boundary Awareness Module (BAM). The DRB pays attention to the local detail of cracks and adjusts the feature dimension for other blocks as needed. And the BAM learns the boundary features from the dilated crack label. Furthermore, the DRB is combined with a lightweight transformer that captures global information to serve as an effective encoder. Experimental results show that the proposed network performs better than state-of-the-art algorithms on two typical datasets. Datasets, code, and trained models are available for research at https://github.com/HqiTao/CT-crackseg.',\n",
              " 'Fine-grained anomaly detection has recently been dominated by segmentation based approaches. These approaches first classify each element of the sample (e.g., image patch) as normal or anomalous and then classify the entire sample as anomalous if it contains anomalous elements. However, such approaches do not extend to scenarios where the anomalies are expressed by an unusual combination of normal elements. In this paper, we overcome this limitation by proposing set features that model each sample by the distribution its elements. We compute the anomaly score of each sample using a simple density estimation method. Our simple-to-implement approach outperforms the state-of-the-art in image-level logical anomaly detection (+3.4%) and sequence-level time-series anomaly detection (+2.4%).',\n",
              " 'Synthetic image generation has opened up new opportunities but has also created threats in regard to privacy, authenticity, and security. Detecting fake images is of paramount importance to prevent illegal activities, and previous research has shown that generative models leave unique patterns in their synthetic images that can be exploited to detect them. However, the fundamental problem of generalization remains, as even state-of-the-art detectors encounter difficulty when facing generators never seen during training. To assess the generalizability and robustness of synthetic image detectors in the face of real-world impairments, this paper presents a large-scale dataset named ArtiFact, comprising diverse generators, object categories, and real-world challenges. Moreover, the proposed multi-class classification scheme, combined with a filter stride reduction strategy addresses social platform impairments and effectively detects synthetic images from both seen and unseen generators. The proposed solution outperforms other teams by 8.34% on Test 1, 1.26% on Test 2, and 15.08% on Test 3 in the IEEE VIP CUP at ICIP 2022.',\n",
              " \"The health and safety hazards posed by worn crane lifting ropes mandate periodic inspection for damage. This task is time-consuming, prone to human error, halts operation, and may result in the premature disposal of ropes. Therefore, we propose using deep learning and computer vision methods to automate the process of detecting damaged ropes. Specifically, we present a novel vision-based system for detecting damage in synthetic fiber rope images using convolutional neural networks (CNN). We use a camera-based apparatus to photograph the lifting rope's surface, while in operation, and capture the progressive wear-and-tear as well as the more significant degradation in the rope's health state. Experts from Konecranes annotate the collected images in accordance with the rope's condition; normal or damaged. Then, we pre-process the images, design a CNN model in a systematic manner, evaluate its detection and prediction performance, analyze its computational complexity, and compare it with various other models. Experimental results show the proposed model outperforms other techniques with 96.4% accuracy, 95.8% precision, 97.2% recall, 96.5% F1-score, and 99.2% AUC. Besides, they demonstrate the model's real-time operation, low memory footprint, robustness to various environmental and operational conditions, and adequacy for deployment in industrial systems.\",\n",
              " \"In the forensic studies of painting masterpieces, the analysis of the support is of major importance. For plain weave fabrics, the densities of vertical and horizontal threads are used as main features, while angle deviations from the vertical and horizontal axis are also of help. These features can be studied locally through the canvas. In this work, deep learning is proposed as a tool to perform these local densities and angle studies. We trained the model with samples from 36 paintings by Vel\\\\'azquez, Rubens or Ribera, among others. The data preparation and augmentation are dealt with at a first stage of the pipeline. We then focus on the supervised segmentation of crossing points between threads. The U-Net with inception and Dice loss are presented as good choices for this task. Densities and angles are then estimated based on the segmented crossing points. We report test results of the analysis of a few canvases and a comparison with methods in the frequency domain, widely used in this problem. We concluded that this new approach succeeds in some cases where the frequency analysis tools fail, while improving the results in others. Besides, our proposal does not need the labeling of part of the to-be-processed image. As case studies, we apply this novel algorithm to the analysis of two pairs of canvases by Vel\\\\'azquez and Murillo, to conclude that the fabrics used came from the same roll.\",\n",
              " 'When deployed for risk-sensitive tasks, deep neural networks must be able to detect instances with labels from outside the distribution for which they were trained. In this paper we present a novel framework to benchmark the ability of image classifiers to detect class-out-of-distribution instances (i.e., instances whose true labels do not appear in the training distribution) at various levels of detection difficulty. We apply this technique to ImageNet, and benchmark 525 pretrained, publicly available, ImageNet-1k classifiers. The code for generating a benchmark for any ImageNet-1k classifier, along with the benchmarks prepared for the above-mentioned 525 models is available at https://github.com/mdabbah/COOD_benchmarking.   The usefulness of the proposed framework and its advantage over alternative existing benchmarks is demonstrated by analyzing the results obtained for these models, which reveals numerous novel observations including: (1) knowledge distillation consistently improves class-out-of-distribution (C-OOD) detection performance; (2) a subset of ViTs performs better C-OOD detection than any other model; (3) the language--vision CLIP model achieves good zero-shot detection performance, with its best instance outperforming 96% of all other models evaluated; (4) accuracy and in-distribution ranking are positively correlated to C-OOD detection; and (5) we compare various confidence functions for C-OOD detection. Our companion paper, also published in ICLR 2023 (What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers), examines the uncertainty estimation performance (ranking, calibration, and selective prediction performance) of these classifiers in an in-distribution setting.',\n",
              " 'In recent years, deep convolutional neural networks (CNN) have significantly advanced face detection. In particular, lightweight CNNbased architectures have achieved great success due to their lowcomplexity structure facilitating real-time detection tasks. However, current lightweight CNN-based face detectors trading accuracy for efficiency have inadequate capability in handling insufficient feature representation, faces with unbalanced aspect ratios and occlusion. Consequently, they exhibit deteriorated performance far lagging behind the deep heavy detectors. To achieve efficient face detection without sacrificing accuracy, we design an efficient deep face detector termed EfficientFace in this study, which contains three modules for feature enhancement. To begin with, we design a novel cross-scale feature fusion strategy to facilitate bottom-up information propagation, such that fusing low-level and highlevel features is further strengthened. Besides, this is conducive to estimating the locations of faces and enhancing the descriptive power of face features. Secondly, we introduce a Receptive Field Enhancement module to consider faces with various aspect ratios. Thirdly, we add an Attention Mechanism module for improving the representational capability of occluded faces. We have evaluated EfficientFace on four public benchmarks and experimental results demonstrate the appealing performance of our method. In particular, our model respectively achieves 95.1% (Easy), 94.0% (Medium) and 90.1% (Hard) on validation set of WIDER Face dataset, which is competitive with heavyweight models with only 1/15 computational costs of the state-of-the-art MogFace detector.',\n",
              " 'With the rapid development of intelligent transportation system applications, a tremendous amount of multi-view video data has emerged to enhance vehicle perception. However, performing video analytics efficiently by exploiting the spatial-temporal redundancy from video data remains challenging. Accordingly, we propose a novel traffic-related framework named CEVAS to achieve efficient object detection using multi-view video data. Briefly, a fine-grained input filtering policy is introduced to produce a reasonable region of interest from the captured images. Also, we design a sharing object manager to manage the information of objects with spatial redundancy and share their results with other vehicles. We further derive a content-aware model selection policy to select detection methods adaptively. Experimental results show that our framework significantly reduces response latency while achieving the same detection accuracy as the state-of-the-art methods.',\n",
              " 'Open-world object detection (OWOD) is a challenging problem that combines object detection with incremental learning and open-set learning. Compared to standard object detection, the OWOD setting is task to: 1) detect objects seen during training while identifying unseen classes, and 2) incrementally learn the knowledge of the identified unknown objects when the corresponding annotations is available. We propose a novel and efficient OWOD solution from a prototype perspective, which we call OCPL: Open-world object detection via discriminative Class Prototype Learning, which consists of a Proposal Embedding Aggregator (PEA), an Embedding Space Compressor (ESC) and a Cosine Similarity-based Classifier (CSC). All our proposed modules aim to learn the discriminative embeddings of known classes in the feature space to minimize the overlapping distributions of known and unknown classes, which is beneficial to differentiate known and unknown classes. Extensive experiments performed on PASCAL VOC and MS-COCO benchmark demonstrate the effectiveness of our proposed method.',\n",
              " 'In this work, we propose a new Dual Min-Max Games (DMMG) based self-supervised skeleton action recognition method by augmenting unlabeled data in a contrastive learning framework. Our DMMG consists of a viewpoint variation min-max game and an edge perturbation min-max game. These two min-max games adopt an adversarial paradigm to perform data augmentation on the skeleton sequences and graph-structured body joints, respectively. Our viewpoint variation min-max game focuses on constructing various hard contrastive pairs by generating skeleton sequences from various viewpoints. These hard contrastive pairs help our model learn representative action features, thus facilitating model transfer to downstream tasks. Moreover, our edge perturbation min-max game specializes in building diverse hard contrastive samples through perturbing connectivity strength among graph-based body joints. The connectivity-strength varying contrastive pairs enable the model to capture minimal sufficient information of different actions, such as representative gestures for an action while preventing the model from overfitting. By fully exploiting the proposed DMMG, we can generate sufficient challenging contrastive pairs and thus achieve discriminative action feature representations from unlabeled skeleton data in a self-supervised manner. Extensive experiments demonstrate that our method achieves superior results under various evaluation protocols on widely-used NTU-RGB+D and NTU120-RGB+D datasets.',\n",
              " 'When deployed for risk-sensitive tasks, deep neural networks must include an uncertainty estimation mechanism. Here we examine the relationship between deep architectures and their respective training regimes, with their corresponding selective prediction and uncertainty estimation performance. We consider some of the most popular estimation performance metrics previously proposed including AUROC, ECE, AURC as well as coverage for selective accuracy constraint. We present a novel and comprehensive study of selective prediction and the uncertainty estimation performance of 523 existing pretrained deep ImageNet classifiers that are available in popular repositories. We identify numerous and previously unknown factors that affect uncertainty estimation and examine the relationships between the different metrics. We find that distillation-based training regimes consistently yield better uncertainty estimations than other training schemes such as vanilla training, pretraining on a larger dataset and adversarial training. Moreover, we find a subset of ViT models that outperform any other models in terms of uncertainty estimation performance. For example, we discovered an unprecedented 99% top-1 selective accuracy on ImageNet at 47% coverage (and 95% top-1 accuracy at 80%) for a ViT model, whereas a competing EfficientNet-V2-XL cannot obtain these accuracy constraints at any level of coverage. Our companion paper, also published in ICLR 2023 (A framework for benchmarking class-out-of-distribution detection and its application to ImageNet), examines the performance of these classifiers in a class-out-of-distribution setting.',\n",
              " 'Existing deep learning-based hyperspectral image (HSI) classification works still suffer from the limitation of the fixed-sized receptive field, leading to difficulties in distinctive spectral-spatial features for ground objects with various sizes and arbitrary shapes. Meanwhile, plenty of previous works ignore asymmetric spectral-spatial dimensions in HSI. To address the above issues, we propose a multi-stage search architecture in order to overcome asymmetric spectral-spatial dimensions and capture significant features. First, the asymmetric pooling on the spectral-spatial dimension maximally retains the essential features of HSI. Then, the 3D convolution with a selectable range of receptive fields overcomes the constraints of fixed-sized convolution kernels. Finally, we extend these two searchable operations to different layers of each stage to build the final architecture. Extensive experiments are conducted on two challenging HSI benchmarks including Indian Pines and Houston University, and results demonstrate the effectiveness of the proposed method with superior performance compared with the related works.',\n",
              " 'Chest X-ray images are commonly used in medical diagnosis, and AI models have been developed to assist with the interpretation of these images. However, many of these models rely on information from a single view of the X-ray, while multiple views may be available. In this work, we propose a novel approach for combining information from multiple views to improve the performance of X-ray image classification. Our approach is based on the use of a convolutional neural network to extract feature maps from each view, followed by an attention mechanism implemented using a Vision Transformer. The resulting model is able to perform multi-label classification on 41 labels and outperforms both single-view models and traditional multi-view classification architectures. We demonstrate the effectiveness of our approach through experiments on a dataset of 363,000 X-ray images.',\n",
              " \"Motion-based association for Multi-Object Tracking (MOT) has recently re-achieved prominence with the rise of powerful object detectors. Despite this, little work has been done to incorporate appearance cues beyond simple heuristic models that lack robustness to feature degradation. In this paper, we propose a novel way to leverage objects' appearances to adaptively integrate appearance matching into existing high-performance motion-based methods. Building upon the pure motion-based method OC-SORT, we achieve 1st place on MOT20 and 2nd place on MOT17 with 63.9 and 64.9 HOTA, respectively. We also achieve 61.3 HOTA on the challenging DanceTrack benchmark as a new state-of-the-art even compared to more heavily-designed methods. The code and models are available at \\\\url{https://github.com/GerardMaggiolino/Deep-OC-SORT}.\",\n",
              " 'This paper presents a new framework for open-vocabulary semantic segmentation with the pre-trained vision-language model, named Side Adapter Network (SAN). Our approach models the semantic segmentation task as a region recognition problem. A side network is attached to a frozen CLIP model with two branches: one for predicting mask proposals, and the other for predicting attention bias which is applied in the CLIP model to recognize the class of masks. This decoupled design has the benefit CLIP in recognizing the class of mask proposals. Since the attached side network can reuse CLIP features, it can be very light. In addition, the entire network can be trained end-to-end, allowing the side network to be adapted to the frozen CLIP model, which makes the predicted mask proposals CLIP-aware. Our approach is fast, accurate, and only adds a few additional trainable parameters. We evaluate our approach on multiple semantic segmentation benchmarks. Our method significantly outperforms other counterparts, with up to 18 times fewer trainable parameters and 19 times faster inference speed. We hope our approach will serve as a solid baseline and help ease future research in open-vocabulary semantic segmentation. The code will be available at https://github.com/MendelXu/SAN.',\n",
              " \"RGB-D semantic segmentation can be advanced with convolutional neural networks due to the availability of Depth data. Although objects cannot be easily discriminated by just the 2D appearance, with the local pixel difference and geometric patterns in Depth, they can be well separated in some cases. Considering the fixed grid kernel structure, CNNs are limited to lack the ability to capture detailed, fine-grained information and thus cannot achieve accurate pixel-level semantic segmentation. To solve this problem, we propose a Pixel Difference Convolutional Network (PDCNet) to capture detailed intrinsic patterns by aggregating both intensity and gradient information in the local range for Depth data and global range for RGB data, respectively. Precisely, PDCNet consists of a Depth branch and an RGB branch. For the Depth branch, we propose a Pixel Difference Convolution (PDC) to consider local and detailed geometric information in Depth data via aggregating both intensity and gradient information. For the RGB branch, we contribute a lightweight Cascade Large Kernel (CLK) to extend PDC, namely CPDC, to enjoy global contexts for RGB data and further boost performance. Consequently, both modal data's local and global pixel differences are seamlessly incorporated into PDCNet during the information propagation process. Experiments on two challenging benchmark datasets, i.e., NYUDv2 and SUN RGB-D reveal that our PDCNet achieves state-of-the-art performance for the semantic segmentation task.\",\n",
              " 'Accurate and fast segmentation of medical images is clinically essential, yet current research methods include convolutional neural networks with fast inference speed but difficulty in learning image contextual features, and transformer with good performance but high hardware requirements. In this paper, we present a Patch Network (PNet) that incorporates the Swin Transformer notion into a convolutional neural network, allowing it to gather richer contextual information while achieving the balance of speed and accuracy. We test our PNet on Polyp(CVC-ClinicDB and ETIS- LaribPolypDB), Skin(ISIC-2018 Skin lesion segmentation challenge dataset) segmentation datasets. Our PNet achieves SOTA performance in both speed and accuracy.',\n",
              " 'Semantic segmentation is a pixel-level prediction task to classify each pixel of the input image. Deep learning models, such as convolutional neural networks (CNNs), have been extremely successful in achieving excellent performances in this domain. However, mobile application, such as autonomous driving, demand real-time processing of incoming stream of images. Hence, achieving efficient architectures along with enhanced accuracy is of paramount importance. Since, accuracy and model size of CNNs are intrinsically contentious in nature, the challenge is to achieve a decent trade-off between accuracy and model size. To address this, we propose a novel Factorized Pyramidal Learning (FPL) module to aggregate rich contextual information in an efficient manner. On one hand, it uses a bank of convolutional filters with multiple dilation rates which leads to multi-scale context aggregation; crucial in achieving better accuracy. On the other hand, parameters are reduced by a careful factorization of the employed filters; crucial in achieving lightweight models. Moreover, we decompose the spatial pyramid into two stages which enables a simple and efficient feature fusion within the module to solve the notorious checkerboard effect. We also design a dedicated Feature-Image Reinforcement (FIR) unit to carry out the fusion operation of shallow and deep features with the downsampled versions of the input image. This gives an accuracy enhancement without increasing model parameters. Based on the FPL module and FIR unit, we propose an ultra-lightweight real-time network, called FPLNet, which achieves state-of-the-art accuracy-efficiency trade-off. More specifically, with only less than 0.5 million parameters, the proposed network achieves 66.93\\\\% and 66.28\\\\% mIoU on Cityscapes validation and test set, respectively. Moreover, FPLNet has a processing speed of 95.5 frames per second (FPS).',\n",
              " 'Underwater images typically experience mixed degradations of brightness and structure caused by the absorption and scattering of light by suspended particles. To address this issue, we propose a Real-time Spatial and Frequency Domains Modulation Network (RSFDM-Net) for the efficient enhancement of colors and details in underwater images. Specifically, our proposed conditional network is designed with Adaptive Fourier Gating Mechanism (AFGM) and Multiscale Convolutional Attention Module (MCAM) to generate vectors carrying low-frequency background information and high-frequency detail features, which effectively promote the network to model global background information and local texture details. To more precisely correct the color cast and low saturation of the image, we introduce a Three-branch Feature Extraction (TFE) block in the primary net that processes images pixel by pixel to integrate the color information extended by the same channel (R, G, or B). This block consists of three small branches, each of which has its own weights. Extensive experiments demonstrate that our network significantly outperforms over state-of-the-art methods in both visual quality and quantitative metrics.',\n",
              " \"We tackle the domain generalisation (DG) problem by posing it as a domain adaptation (DA) task where we adversarially synthesise the worst-case target domain and adapt a model to that worst-case domain, thereby improving the model's robustness. To synthesise data that is challenging yet semantics-preserving, we generate Fourier amplitude images and combine them with source domain phase images, exploiting the widely believed conjecture from signal processing that amplitude spectra mainly determines image style, while phase data mainly captures image semantics. To synthesise a worst-case domain for adaptation, we train the classifier and the amplitude generator adversarially. Specifically, we exploit the maximum classifier discrepancy (MCD) principle from DA that relates the target domain performance to the discrepancy of classifiers in the model hypothesis space. By Bayesian hypothesis modeling, we express the model hypothesis space effectively as a posterior distribution over classifiers given the source domains, making adversarial MCD minimisation feasible. On the DomainBed benchmark including the large-scale DomainNet dataset, the proposed approach yields significantly improved domain generalisation performance over the state-of-the-art.\",\n",
              " 'Unsupervised domain adaptation addresses the problem of classifying data in an unlabeled target domain, given labeled source domain data that share a common label space but follow a different distribution. Most of the recent methods take the approach of explicitly aligning feature distributions between the two domains. Differently, motivated by the fundamental assumption for domain adaptability, we re-cast the domain adaptation problem as discriminative clustering of target data, given strong privileged information provided by the closely related, labeled source data. Technically, we use clustering objectives based on a robust variant of entropy minimization that adaptively filters target data, a soft Fisher-like criterion, and additionally the cluster ordering via centroid classification. To distill discriminative source information for target clustering, we propose to jointly train the network using parallel, supervised learning objectives over labeled source data. We term our method of distilled discriminative clustering for domain adaptation as DisClusterDA. We also give geometric intuition that illustrates how constituent objectives of DisClusterDA help learn class-wisely pure, compact feature distributions. We conduct careful ablation studies and extensive experiments on five popular benchmark datasets, including a multi-source domain adaptation one. Based on commonly used backbone networks, DisClusterDA outperforms existing methods on these benchmarks. It is also interesting to observe that in our DisClusterDA framework, adding an additional loss term that explicitly learns to align class-level feature distributions across domains does harm to the adaptation performance, though more careful studies in different algorithmic frameworks are to be conducted.',\n",
              " 'Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fail to randomize domain-dependent features, and domain-invariant augmentations, which randomize all domain-dependent features, both perform poorly OOD. In experiments on three real-world datasets, we show that targeted augmentations set new states-of-the-art for OOD performance by 3.2-15.2%.',\n",
              " 'Object pose estimation is a non-trivial task that enables robotic manipulation, bin picking, augmented reality, and scene understanding, to name a few use cases. Monocular object pose estimation gained considerable momentum with the rise of high-performing deep learning-based solutions and is particularly interesting for the community since sensors are inexpensive and inference is fast. Prior works establish the comprehensive state of the art for diverse pose estimation problems. Their broad scopes make it difficult to identify promising future directions. We narrow down the scope to the problem of single-shot monocular 6D object pose estimation, which is commonly used in robotics, and thus are able to identify such trends. By reviewing recent publications in robotics and computer vision, the state of the art is established at the union of both fields. Following that, we identify promising research directions in order to help researchers to formulate relevant research ideas and effectively advance the state of the art. Findings include that methods are sophisticated enough to overcome the domain shift and that occlusion handling is a fundamental challenge. We also highlight problems such as novel object pose estimation and challenging materials handling as central challenges to advance robotics.',\n",
              " 'Over the past decade, domain adaptation has become a widely studied branch of transfer learning that aims to improve performance on target domains by leveraging knowledge from the source domain. Conventional domain adaptation methods often assume access to both source and target domain data simultaneously, which may not be feasible in real-world scenarios due to privacy and confidentiality concerns. As a result, the research of Source-Free Domain Adaptation (SFDA) has drawn growing attention in recent years, which only utilizes the source-trained model and unlabeled target data to adapt to the target domain. Despite the rapid explosion of SFDA work, yet there has no timely and comprehensive survey in the field. To fill this gap, we provide a comprehensive survey of recent advances in SFDA and organize them into a unified categorization scheme based on the framework of transfer learning. Instead of presenting each approach independently, we modularize several components of each method to more clearly illustrate their relationships and mechanics in light of the composite properties of each method. Furthermore, we compare the results of more than 30 representative SFDA methods on three popular classification benchmarks, namely Office-31, Office-home, and VisDA, to explore the effectiveness of various technical routes and the combination effects among them. Additionally, we briefly introduce the applications of SFDA and related fields. Drawing from our analysis of the challenges facing SFDA, we offer some insights into future research directions and potential settings.',\n",
              " 'Image manipulation under the guidance of textual descriptions has recently received a broad range of attention. In this study, we focus on the regional editing of images with the guidance of given text prompts. Different from current mask-based image editing methods, we propose a novel region-aware diffusion model (RDM) for entity-level image editing, which could automatically locate the region of interest and replace it following given text prompts. To strike a balance between image fidelity and inference speed, we design the intensive diffusion pipeline by combing latent space diffusion and enhanced directional guidance. In addition, to preserve image content in non-edited regions, we introduce regional-aware entity editing to modify the region of interest and preserve the out-of-interest region. We validate the proposed RDM beyond the baseline methods through extensive qualitative and quantitative experiments. The results show that RDM outperforms the previous approaches in terms of visual quality, overall harmonization, non-editing region content preservation, and text-image semantic consistency. The codes are available at https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model.',\n",
              " \"Converting a parametric curve into the implicit form, which is called implicitization, has always been a popular but challenging problem in geometric modeling and related applications. However, the existing methods mostly suffer from the problems of maintaining geometric features and choosing a reasonable implicit degree. The present paper has two contributions. We first introduce a new regularization constraint(called the weak gradient constraint) for both polynomial and non-polynomial curves, which efficiently possesses shape preserving. We then propose two adaptive algorithms of approximate implicitization for polynomial and non-polynomial curves respectively, which find the ``optimal'' implicit degree based on the behavior of the weak gradient constraint. More precisely, the idea is gradually increasing the implicit degree, until there is no obvious improvement in the weak gradient loss of the outputs. Experimental results have shown the effectiveness and high quality of our proposed methods.\",\n",
              " 'Image-to-image translation is a fundamental task in computer vision. It transforms images from one domain to images in another domain so that they have particular domain-specific characteristics. Most prior works train a generative model to learn the mapping from a source domain to a target domain. However, learning such mapping between domains is challenging because data from different domains can be highly unbalanced in terms of both quality and quantity. To address this problem, we propose a new approach to extract image features by learning the similarities and differences of samples within the same data distribution via a novel contrastive learning framework, which we call Auto-Contrastive-Encoder (ACE). ACE learns the content code as the similarity between samples with the same content information and different style perturbations. The design of ACE enables us to achieve zero-shot image-to-image translation with no training on image translation tasks for the first time.   Moreover, our learning method can learn the style features of images on different domains effectively. Consequently, our model achieves competitive results on multimodal image translation tasks with zero-shot learning as well. Additionally, we demonstrate the potential of our method in transfer learning. With fine-tuning, the quality of translated images improves in unseen domains. Even though we use contrastive learning, all of our training can be performed on a single GPU with the batch size of 8.',\n",
              " 'We propose a novel framework for the task of object-centric video prediction, i.e., extracting the compositional structure of a video sequence, as well as modeling objects dynamics and interactions from visual observations in order to predict the future object states, from which we can then generate subsequent video frames. With the goal of learning meaningful spatio-temporal object representations and accurately forecasting object states, we propose two novel object-centric video predictor (OCVP) transformer modules, which decouple the processing of temporal dynamics and object interactions, thus presenting an improved prediction performance. In our experiments, we show how our object-centric prediction framework utilizing our OCVP predictors outperforms object-agnostic video prediction models on two different datasets, while maintaining consistent and accurate object representations.',\n",
              " 'Generated synthetic data in medical research can substitute privacy and security-sensitive data with a large-scale curated dataset, reducing data collection and annotation costs. As part of this effort, we propose UniXGen, a unified chest X-ray and report generation model, with the following contributions. First, we design a unified model for bidirectional chest X-ray and report generation by adopting a vector quantization method to discretize chest X-rays into discrete visual tokens and formulating both tasks as sequence generation tasks. Second, we introduce several special tokens to generate chest X-rays with specific views that can be useful when the desired views are unavailable. Furthermore, UniXGen can flexibly take various inputs from single to multiple views to take advantage of the additional findings available in other X-ray views. We adopt an efficient transformer for computational and memory efficiency to handle the long-range input sequence of multi-view chest X-rays with high resolution and long paragraph reports. In extensive experiments, we show that our unified model has a synergistic effect on both generation tasks, as opposed to training only the task-specific models. We also find that view-specific special tokens can distinguish between different views and properly generate specific views even if they do not exist in the dataset, and utilizing multi-view chest X-rays can faithfully capture the abnormal findings in the additional X-rays. The source code is publicly available at: https://github.com/ttumyche/UniXGen.',\n",
              " 'Heterogeneous data is endemic due to the use of diverse models and settings of devices by hospitals in the field of medical imaging. However, there are few open-source frameworks for federated heterogeneous medical image analysis with personalization and privacy protection simultaneously without the demand to modify the existing model structures or to share any private data. In this paper, we proposed PPPML-HMI, an open-source learning paradigm for personalized and privacy-preserving federated heterogeneous medical image analysis. To our best knowledge, personalization and privacy protection were achieved simultaneously for the first time under the federated scenario by integrating the PerFedAvg algorithm and designing our novel cyclic secure aggregation with the homomorphic encryption algorithm. To show the utility of PPPML-HMI, we applied it to a simulated classification task namely the classification of healthy people and patients from the RAD-ChestCT Dataset, and one real-world segmentation task namely the segmentation of lung infections from COVID-19 CT scans. For the real-world task, PPPML-HMI achieved $\\\\sim$5\\\\% higher Dice score on average compared to conventional FL under the heterogeneous scenario. Meanwhile, we applied the improved deep leakage from gradients to simulate adversarial attacks and showed the solid privacy-preserving capability of PPPML-HMI. By applying PPPML-HMI to both tasks with different neural networks, a varied number of users, and sample sizes, we further demonstrated the strong robustness of PPPML-HMI.',\n",
              " 'Close-up facial images captured at close distances often suffer from perspective distortion, resulting in exaggerated facial features and unnatural/unattractive appearances. We propose a simple yet effective method for correcting perspective distortions in a single close-up face. We first perform GAN inversion using a perspective-distorted input facial image by jointly optimizing the camera intrinsic/extrinsic parameters and face latent code. To address the ambiguity of joint optimization, we develop focal length reparametrization, optimization scheduling, and geometric regularization. Re-rendering the portrait at a proper focal length and camera distance effectively corrects these distortions and produces more natural-looking results. Our experiments show that our method compares favorably against previous approaches regarding visual quality. We showcase numerous examples validating the applicability of our method on portrait photos in the wild.',\n",
              " 'The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \\\\eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \\\\emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\\\\eg, DINO), and Vision-language models (\\\\eg, CLIP) to black-box \\\\emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \\\\emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. Our attack results indicate that the attacker does not need specialized architectures, \\\\eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at https://bit.ly/3Xd9gRQ',\n",
              " 'Using real road testing to optimize autonomous driving algorithms is time-consuming and capital-intensive. To solve this problem, we propose a GAN-based model that is capable of generating high-quality images across different domains. We further leverage Contrastive Learning to train the model in a self-supervised way using image data acquired in the real world using real sensors and simulated images from 3D games. In this paper, we also apply an Attention Mechanism module to emphasize features that contain more information about the source domain according to their measurement of significance. Finally, the generated images are used as datasets to train neural networks to perform a variety of downstream tasks to verify that the approach can fill in the gaps between the virtual and real worlds.',\n",
              " \"Denoising Diffusion models have shown remarkable performance in generating diverse, high quality images from text. Numerous techniques have been proposed on top of or in alignment with models like Stable Diffusion and Imagen that generate images directly from text. A lesser explored approach is DALLE-2's two step process comprising a Diffusion Prior that generates a CLIP image embedding from text and a Diffusion Decoder that generates an image from a CLIP image embedding. We explore the capabilities of the Diffusion Prior and the advantages of an intermediate CLIP representation. We observe that Diffusion Prior can be used in a memory and compute efficient way to constrain the generation to a specific domain without altering the larger Diffusion Decoder. Moreover, we show that the Diffusion Prior can be trained with additional conditional information such as color histogram to further control the generation. We show quantitatively and qualitatively that the proposed approaches perform better than prompt engineering for domain specific generation and existing baselines for color conditioned generation. We believe that our observations and results will instigate further research into the diffusion prior and uncover more of its capabilities.\",\n",
              " 'Text-to-image personalization aims to teach a pre-trained diffusion model to reason about novel, user provided concepts, embedding them into new scenes guided by natural language prompts. However, current personalization approaches struggle with lengthy training times, high storage requirements or loss of identity. To overcome these limitations, we propose an encoder-based domain-tuning approach. Our key insight is that by underfitting on a large set of concepts from a given domain, we can improve generalization and create a model that is more amenable to quickly adding novel concepts from the same domain. Specifically, we employ two components: First, an encoder that takes as an input a single image of a target concept from a given domain, e.g. a specific face, and learns to map it into a word-embedding representing the concept. Second, a set of regularized weight-offsets for the text-to-image model that learn how to effectively ingest additional concepts. Together, these components are used to guide the learning of unseen concepts, allowing us to personalize a model using only a single image and as few as 5 training steps - accelerating personalization from dozens of minutes to seconds, while preserving quality.',\n",
              " 'Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the human-labeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve image-text alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.',\n",
              " 'Simulating the effects of skincare products on face is a potential new way to communicate the efficacy of skincare products in skin diagnostics and product recommendations. Furthermore, such simulations enable one to anticipate his/her skin conditions and better manage skin health. However, there is a lack of effective simulations today. In this paper, we propose the first simulation model to reveal facial pore changes after using skincare products. Our simulation pipeline consists of 2 steps: training data establishment and facial pore simulation. To establish training data, we collect face images with various pore quality indexes from short-term (8-weeks) clinical studies. People often experience significant skin fluctuations (due to natural rhythms, external stressors, etc.,), which introduces large perturbations in clinical data. To address this problem, we propose a sliding window mechanism to clean data and select representative index(es) to represent facial pore changes. Facial pore simulation stage consists of 3 modules: UNet-based segmentation module to localize facial pores; regression module to predict time-dependent warping hyperparameters; and deformation module, taking warping hyperparameters and pore segmentation labels as inputs, to precisely deform pores accordingly. The proposed simulation is able to render realistic facial pore changes. And this work will pave the way for future research in facial skin simulation and skincare product developments.',\n",
              " \"Deep learning algorithms have achieved remarkable results in medical image segmentation in recent years. These networks are unable to handle with image boundaries and details with enormous parameters, resulting in poor segmentation results. To address the issue, we develop atrous spatial pyramid pooling (ASPP) and combine it with the Squeeze-and-Excitation block (SE block), as well as present the PS module, which employs a broader and multi-scale receptive field at the network's bottom to obtain more detailed semantic information. We also propose the Local Guided block (LG block) and also its combination with the SE block to form the LS block, which can obtain more abundant local features in the feature map, so that more edge information can be retained in each down sampling process, thereby improving the performance of boundary segmentation. We propose PLU-Net and integrate our PS module and LS block into U-Net. We put our PLU-Net to the test on three benchmark datasets, and the results show that by fewer parameters and FLOPs, it outperforms on medical semantic segmentation tasks.\",\n",
              " \"Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive results on novel view synthesis tasks. NeRFs learn a scene's color and density fields by minimizing the photometric discrepancy between training views and differentiable renders of the scene. Once trained from a sufficient set of views, NeRFs can generate novel views from arbitrary camera positions. However, the scene geometry and color fields are severely under-constrained, which can lead to artifacts, especially when trained with few input views.   To alleviate this problem we learn a prior over scene geometry and color, using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of the synthetic Hypersim dataset and can be used to predict the gradient of the logarithm of a joint probability distribution of color and depth patches. We show that, during NeRF training, these gradients of logarithms of RGBD patch priors serve to regularize geometry and color for a scene. During NeRF training, random RGBD patches are rendered and the estimated gradients of the log-likelihood are backpropagated to the color and density fields. Evaluations on LLFF, the most relevant dataset, show that our learned prior achieves improved quality in the reconstructed geometry and improved generalization to novel views. Evaluations on DTU show improved reconstruction quality among NeRF methods.\",\n",
              " 'Although an object may appear in numerous contexts, we often describe it in a limited number of ways. This happens because language abstracts away visual variation to represent and communicate concepts. Building on this intuition, we propose an alternative approach to visual learning: using language similarity to sample semantically similar image pairs for contrastive learning. Our approach deviates from image-based contrastive learning by using language to sample pairs instead of hand-crafted augmentations or learned clusters. Our approach also deviates from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than minimize a cross-modal similarity. Through a series of experiments, we show that language-guided learning can learn better features than both image-image and image-text representation learning approaches.',\n",
              " 'The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.',\n",
              " 'This paper addresses the challenge of quickly reconstructing free-viewpoint videos of dynamic humans from sparse multi-view videos. Some recent works represent the dynamic human as a canonical neural radiance field (NeRF) and a motion field, which are learned from videos through differentiable rendering. But the per-scene optimization generally requires hours. Other generalizable NeRF models leverage learned prior from datasets and reduce the optimization time by only finetuning on new scenes at the cost of visual fidelity. In this paper, we propose a novel method for learning neural volumetric videos of dynamic humans from sparse view videos in minutes with competitive visual quality. Specifically, we define a novel part-based voxelized human representation to better distribute the representational power of the network to different human parts. Furthermore, we propose a novel 2D motion parameterization scheme to increase the convergence rate of deformation field learning. Experiments demonstrate that our model can be learned 100 times faster than prior per-scene optimization methods while being competitive in the rendering quality. Training our model on a $512 \\\\times 512$ video with 100 frames typically takes about 5 minutes on a single RTX 3090 GPU. The code will be released on our $\\\\href{https://zju3dv.github.io/instant_nvr}{project~page}$.',\n",
              " 'Deep Learning-based Computer Vision field has recently been trying to explore larger kernels for convolution to effectively scale up Convolutional Neural Networks. Simultaneously, new paradigm of models such as Vision Transformers find it difficult to scale up to larger higher resolution images due to their quadratic complexity in terms of input sequence. In this report, Fast Fourier Transform is utilised in various ways to provide some solutions to these issues.',\n",
              " \"In recent years, large strides have been taken in developing machine learning methods for dermatological applications, supported in part by the success of deep learning (DL). To date, diagnosing diseases from images is one of the most explored applications of DL within dermatology. Convolutional neural networks (ConvNets) are the most common (DL) method in medical imaging due to their training efficiency and accuracy, although they are often described as black boxes because of their limited explainability. One popular way to obtain insight into a ConvNet's decision mechanism is gradient class activation maps (Grad-CAM). A quantitative evaluation of the Grad-CAM explainability has been recently made possible by the release of DermXDB, a skin disease diagnosis explainability dataset which enables explainability benchmarking of ConvNet architectures. In this paper, we perform a literature review to identify the most common ConvNet architectures used for this task, and compare their Grad-CAM explanations with the explanation maps provided by DermXDB. We identified 11 architectures: DenseNet121, EfficientNet-B0, InceptionV3, InceptionResNetV2, MobileNet, MobileNetV2, NASNetMobile, ResNet50, ResNet50V2, VGG16, and Xception. We pre-trained all architectures on an clinical skin disease dataset, and fine-tuned them on a DermXDB subset. Validation results on the DermXDB holdout subset show an explainability F1 score of between 0.35-0.46, with Xception displaying the highest explainability performance. NASNetMobile reports the highest characteristic-level explainability sensitivity, despite it's mediocre diagnosis performance. These results highlight the importance of choosing the right architecture for the desired application and target market, underline need for additional explainability datasets, and further confirm the need for explainability benchmarking that relies on quantitative analyses.\",\n",
              " 'Large language models have demonstrated an emergent capability in answering knowledge intensive questions. With recent progress on web-scale visual and language pre-training, do these models also understand how to answer visual information seeking questions? To answer this question, we present InfoSeek, a Visual Question Answering dataset that focuses on asking information-seeking questions, where the information can not be answered by common sense knowledge. We perform a multi-stage human annotation to collect a natural distribution of high-quality visual information seeking question-answer pairs. We also construct a large-scale, automatically collected dataset by combining existing visual entity recognition datasets and Wikidata, which provides over one million examples for model fine-tuning and validation. Based on InfoSeek, we analyzed various pre-trained Visual QA systems to gain insights into the characteristics of different pre-trained models. Our analysis shows that it is challenging for the state-of-the-art multi-modal pre-trained models to answer visual information seeking questions, but this capability is improved through fine-tuning on the automated InfoSeek dataset. We hope our analysis paves the way to understand and develop the next generation of multi-modal pre-training.',\n",
              " \"To design with AI models, user experience (UX) designers must assess the fit between the model and user needs. Based on user research, they need to contextualize the model's behavior and potential failures within their product-specific data instances and user scenarios. However, our formative interviews with ten UX professionals revealed that such a proactive discovery of model limitations is challenging and time-intensive. Furthermore, designers often lack technical knowledge of AI and accessible exploration tools, which challenges their understanding of model capabilities and limitations. In this work, we introduced a failure-driven design approach to AI, a workflow that encourages designers to explore model behavior and failure patterns early in the design process. The implementation of fAIlureNotes, a designer-centered failure exploration and analysis tool, supports designers in evaluating models and identifying failures across diverse user groups and scenarios. Our evaluation with UX practitioners shows that fAIlureNotes outperforms today's interactive model cards in assessing context-specific model performance.\",\n",
              " 'We introduce a novel energy formulation for Plug- and-Play (PnP) image recovery. Traditional PnP methods that use a convolutional neural network (CNN) do not have an energy based formulation. The primary focus of this work is to introduce an energy-based PnP formulation, which relies on a CNN that learns the log of the image prior from training data. The score function is evaluated as the gradient of the energy model, which resembles a UNET with shared encoder and decoder weights. The proposed score function is thus constrained to a conservative vector field, which is the key difference with classical PnP models. The energy-based formulation offers algorithms with convergence guarantees, even when the learned score model is not a contraction. The relaxation of the contraction constraint allows the proposed model to learn more complex priors, thus offering improved performance over traditional PnP schemes. Our experiments in magnetic resonance image reconstruction demonstrates the improved performance offered by the proposed energy model over traditional PnP methods.',\n",
              " 'Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly effective even without any group information. Finally, unlike existing works that solely aim to improve worst-group accuracy (WGA), we demonstrate the fundamental tradeoff between WGA and other important metrics, highlighting the need to carefully choose testing metrics. Code and data are available at: https://github.com/YyzHarry/SubpopBench.',\n",
              " 'Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.',\n",
              " 'Current captioning datasets, focus on object-centric captions, describing the visible objects in the image, often ending up stating the obvious (for humans), e.g. \"people eating food in a park\". Although these datasets are useful to evaluate the ability of Vision & Language models to recognize the visual content, they lack in expressing trivial abstract concepts, e.g. \"people having a picnic\". Such concepts are licensed by human\\'s personal experience and contribute to forming common sense assumptions. We present the High-Level Dataset; a dataset extending 14997 images of the COCO dataset with 134973 human-annotated (high-level) abstract captions collected along three axes: scenes, actions and rationales. We describe and release such dataset and we show how it can be used to assess models\\' multimodal grounding of abstract concepts and enrich models\\' visio-lingusitic representations. Moreover, we describe potential tasks enabled by this dataset involving high- and low-level concepts interactions.',\n",
              " 'Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limitation - they fail to encapsulate compositional concepts such as counting. We introduce a simple yet effective method to improve the quantitative understanding of VLMs, while maintaining their overall performance on common benchmarks. Specifically, we propose a new counting-contrastive loss used to finetune a pre-trained VLM in tandem with its original objective. Our counting loss is deployed over automatically-created counterfactual examples, each consisting of an image and a caption containing an incorrect object count. For example, an image depicting three dogs is paired with the caption \"Six dogs playing in the yard\". Our loss encourages discrimination between the correct caption and its counterfactual variant which serves as a hard negative example. To the best of our knowledge, this work is the first to extend CLIP\\'s capabilities to object counting. Furthermore, we introduce \"CountBench\" - a new image-text counting benchmark for evaluating a model\\'s understanding of object counting. We demonstrate a significant improvement over state-of-the-art baseline models on this task. Finally, we leverage our count-aware CLIP model for image retrieval and text-conditioned image generation, demonstrating that our model can produce specific counts of objects more reliably than existing ones.',\n",
              " 'Accurately estimating the shape of objects in dense clutters makes important contribution to robotic packing, because the optimal object arrangement requires the robot planner to acquire shape information of all existed objects. However, the objects for packing are usually piled in dense clutters with severe occlusion, and the object shape varies significantly across different instances for the same category. They respectively cause large object segmentation errors and inaccurate shape recovery on unseen instances, which both degrade the performance of shape estimation during deployment. In this paper, we propose a category-level shape estimation method for densely cluttered objects. Our framework partitions each object in the clutter via the multi-view visual information fusion to achieve high segmentation accuracy, and the instance shape is recovered by deforming the category templates with diverse geometric transformations to obtain strengthened generalization ability. Specifically, we first collect the multi-view RGB-D images of the object clutters for point cloud reconstruction. Then we fuse the feature maps representing the visual information of multi-view RGB images and the pixel affinity learned from the clutter point cloud, where the acquired instance segmentation masks of multi-view RGB images are projected to partition the clutter point cloud. Finally, the instance geometry information is obtained from the partially observed instance point cloud and the corresponding category template, and the deformation parameters regarding the template are predicted for shape estimation. Experiments in the simulated environment and real world show that our method achieves high shape estimation accuracy for densely cluttered everyday objects with various shapes.',\n",
              " \"Biological processes like growth, aging, and disease progression are generally studied with follow-up scans taken at different time points, i.e., with image time series (TS) based analysis. Comparison between TS representing a biological process of two individuals/populations is of interest. A metric to quantify the difference between TS is desirable for such a comparison. The two TS represent the evolution of two different subject/population average anatomies through two paths. A method to untangle and quantify the path and inter-subject anatomy(shape) difference between the TS is presented in this paper. The proposed metric is a generalized version of Fr\\\\'echet distance designed to compare curves. The proposed method is evaluated with simulated and adult and fetal neuro templates. Results show that the metric is able to separate and quantify the path and shape differences between TS.\",\n",
              " 'Ultra-High-Definition (UHD) photo has gradually become the standard configuration in advanced imaging devices. The new standard unveils many issues in existing approaches for low-light image enhancement (LLIE), especially in dealing with the intricate issue of joint luminance enhancement and noise removal while remaining efficient. Unlike existing methods that address the problem in the spatial domain, we propose a new solution, UHDFour, that embeds Fourier transform into a cascaded network. Our approach is motivated by a few unique characteristics in the Fourier domain: 1) most luminance information concentrates on amplitudes while noise is closely related to phases, and 2) a high-resolution image and its low-resolution version share similar amplitude patterns.Through embedding Fourier into our network, the amplitude and phase of a low-light image are separately processed to avoid amplifying noise when enhancing luminance. Besides, UHDFour is scalable to UHD images by implementing amplitude and phase enhancement under the low-resolution regime and then adjusting the high-resolution scale with few computations. We also contribute the first real UHD LLIE dataset, \\\\textbf{UHD-LL}, that contains 2,150 low-noise/normal-clear 4K image pairs with diverse darkness and noise levels captured in different scenarios. With this dataset, we systematically analyze the performance of existing LLIE methods for processing UHD images and demonstrate the advantage of our solution. We believe our new framework, coupled with the dataset, would push the frontier of LLIE towards UHD. The code and dataset are available at https://li-chongyi.github.io/UHDFour.',\n",
              " 'Transparent object perception is a crucial skill for applications such as robot manipulation in household and laboratory settings. Existing methods utilize RGB-D or stereo inputs to handle a subset of perception tasks including depth and pose estimation. However, transparent object perception remains to be an open problem. In this paper, we forgo the unreliable depth map from RGB-D sensors and extend the stereo based method. Our proposed method, MVTrans, is an end-to-end multi-view architecture with multiple perception capabilities, including depth estimation, segmentation, and pose estimation. Additionally, we establish a novel procedural photo-realistic dataset generation pipeline and create a large-scale transparent object detection dataset, Syn-TODD, which is suitable for training networks with all three modalities, RGB-D, stereo and multi-view RGB. Project Site: https://ac-rad.github.io/MVTrans/',\n",
              " 'Deep learning based image enhancement models have largely improved the readability of fundus images in order to decrease the uncertainty of clinical observations and the risk of misdiagnosis. However, due to the difficulty of acquiring paired real fundus images at different qualities, most existing methods have to adopt synthetic image pairs as training data. The domain shift between the synthetic and the real images inevitably hinders the generalization of such models on clinical data. In this work, we propose an end-to-end optimized teacher-student framework to simultaneously conduct image enhancement and domain adaptation. The student network uses synthetic pairs for supervised enhancement, and regularizes the enhancement model to reduce domain-shift by enforcing teacher-student prediction consistency on the real fundus images without relying on enhanced ground-truth. Moreover, we also propose a novel multi-stage multi-attention guided enhancement network (MAGE-Net) as the backbones of our teacher and student network. Our MAGE-Net utilizes multi-stage enhancement module and retinal structure preservation module to progressively integrate the multi-scale features and simultaneously preserve the retinal structures for better fundus image quality enhancement. Comprehensive experiments on both real and synthetic datasets demonstrate that our framework outperforms the baseline approaches. Moreover, our method also benefits the downstream clinical tasks.',\n",
              " 'Distinguishing among different marine benthic habitat characteristics is of key importance in a wide set of seabed operations ranging from installations of oil rigs to laying networks of cables and monitoring the impact of humans on marine ecosystems. The Side-Scan Sonar (SSS) is a widely used imaging sensor in this regard. It produces high-resolution seafloor maps by logging the intensities of sound waves reflected back from the seafloor. In this work, we leverage these acoustic intensity maps to produce pixel-wise categorization of different seafloor types. We propose a novel architecture adapted from the Vision Transformer (ViT) in an encoder-decoder framework. Further, in doing so, the applicability of ViTs is evaluated on smaller datasets. To overcome the lack of CNN-like inductive biases, thereby making ViTs more conducive to applications in low data regimes, we propose a novel feature extraction module to replace the Multi-layer Perceptron (MLP) block within transformer layers and a novel module to extract multiscale patch embeddings. A lightweight decoder is also proposed to complement this design in order to further boost multiscale feature extraction. With the modified architecture, we achieve state-of-the-art results and also meet real-time computational requirements. We make our code available at ~\\\\url{https://github.com/hayatrajani/s3seg-vit',\n",
              " 'Multimodal learning, particularly for pedestrian detection, has recently received emphasis due to its capability to function equally well in several critical autonomous driving scenarios such as low-light, night-time, and adverse weather conditions. However, in most cases, the training distribution largely emphasizes the contribution of one specific input that makes the network biased towards one modality. Hence, the generalization of such models becomes a significant problem where the non-dominant input modality during training could be contributing more to the course of inference. Here, we introduce a novel training setup with regularizer in the multimodal architecture to resolve the problem of this disparity between the modalities. Specifically, our regularizer term helps to make the feature fusion method more robust by considering both the feature extractors equivalently important during the training to extract the multimodal distribution which is referred to as removing the imbalance problem. Furthermore, our decoupling concept of output stream helps the detection task by sharing the spatial sensitive information mutually. Extensive experiments of the proposed method on KAIST and UTokyo datasets shows improvement of the respective state-of-the-art performance.',\n",
              " \"Generative adversarial networks (GANs), trained on a large-scale image dataset, can be a good approximator of the natural image manifold. GAN-inversion, using a pre-trained generator as a deep generative prior, is a promising tool for image restoration under corruptions. However, the performance of GAN-inversion can be limited by a lack of robustness to unknown gross corruptions, i.e., the restored image might easily deviate from the ground truth. In this paper, we propose a Robust GAN-inversion (RGI) method with a provable robustness guarantee to achieve image restoration under unknown \\\\textit{gross} corruptions, where a small fraction of pixels are completely corrupted. Under mild assumptions, we show that the restored image and the identified corrupted region mask converge asymptotically to the ground truth. Moreover, we extend RGI to Relaxed-RGI (R-RGI) for generator fine-tuning to mitigate the gap between the GAN learned manifold and the true image manifold while avoiding trivial overfitting to the corrupted input image, which further improves the image restoration and corrupted region mask identification performance. The proposed RGI/R-RGI method unifies two important applications with state-of-the-art (SOTA) performance: (i) mask-free semantic inpainting, where the corruptions are unknown missing regions, the restored background can be used to restore the missing content; (ii) unsupervised pixel-wise anomaly detection, where the corruptions are unknown anomalous regions, the retrieved mask can be used as the anomalous region's segmentation mask.\",\n",
              " 'Huge challenges exist for old landslide detection because their morphology features have been partially or strongly transformed over a long time and have little difference from their surrounding. Besides, small-sample problem also restrict in-depth learning.   In this paper, an iterative classification and semantic segmentation network (ICSSN) is developed, which can greatly enhance both object-level and pixel-level classification performance by iteratively upgrading the feature extractor shared by two network. An object-level contrastive learning (OCL) strategy is employed in the object classification sub-network featuring a siamese network to realize the global features extraction, and a sub-object-level contrastive learning (SOCL) paradigm is designed in the semantic segmentation sub-network to efficiently extract salient features from boundaries of landslides. Moreover, an iterative training strategy is elaborated to fuse features in semantic space such that both object-level and pixel-level classification performance are improved.   The proposed ICSSN is evaluated on the real landslide data set, and the experimental results show that ICSSN can greatly improve the classification and segmentation accuracy of old landslide detection. For the semantic segmentation task, compared to the baseline, the F1 score increases from 0.5054 to 0.5448, the mIoU improves from 0.6405 to 0.6610, the landslide IoU improved from 0.3381 to 0.3743, and the object-level detection accuracy of old landslides is enhanced from 0.55 to 0.9. For the object classification task, the F1 score increases from 0.8846 to 0.9230, and the accuracy score is up from 0.8375 to 0.8875.',\n",
              " 'This paper presents an aligned multi-temporal and multi-resolution satellite image dataset for research in change detection. We expect our dataset to be useful to researchers who want to fuse information from multiple satellites for detecting changes on the surface of the earth that may not be fully visible in any single satellite. The dataset we present was created by augmenting the SpaceNet-7 dataset with temporally parallel stacks of Landsat and Sentinel images. The SpaceNet-7 dataset consists of time-sequenced Planet images recorded over 101 AOIs (Areas-of-Interest). In our dataset, for each of the 60 AOIs that are meant for training, we augment the Planet datacube with temporally parallel datacubes of Landsat and Sentinel images. The temporal alignments between the high-res Planet images, on the one hand, and the Landsat and Sentinel images, on the other, are approximate since the temporal resolution for the Planet images is one month -- each image being a mosaic of the best data collected over a month. Whenever we have a choice regarding which Landsat and Sentinel images to pair up with the Planet images, we have chosen those that had the least cloud cover. A particularly important feature of our dataset is that the high-res and the low-res images are spatially aligned together with our MuRA framework presented in this paper. Foundational to the alignment calculation is the modeling of inter-satellite misalignment errors with polynomials as in NASA\\'s AROP algorithm. We have named our dataset MuRA-T for the MuRA framework that is used for aligning the cross-satellite images and \"T\" for the temporal dimension in the dataset.',\n",
              " \"To address the problem of medical image recognition, computer vision techniques like convolutional neural networks (CNN) are frequently used. Recently, 3D CNN-based models dominate the field of magnetic resonance image (MRI) analytics. Due to the high similarity between MRI data and videos, we conduct extensive empirical studies on video recognition techniques for MRI classification to answer the questions: (1) can we directly use video recognition models for MRI classification, (2) which model is more appropriate for MRI, (3) are the common tricks like data augmentation in video recognition still useful for MRI classification? Our work suggests that advanced video techniques benefit MRI classification. In this paper, four datasets of Alzheimer's and Parkinson's disease recognition are utilized in experiments, together with three alternative video recognition models and data augmentation techniques that are frequently applied to video tasks. In terms of efficiency, the results reveal that the video framework performs better than 3D-CNN models by 5% - 11% with 50% - 66% less trainable parameters. This report pushes forward the potential fusion of 3D medical imaging and video understanding research.\",\n",
              " 'Lossy face image compression can degrade the image quality and the utility for the purpose of face recognition. This work investigates the effect of lossy image compression on a state-of-the-art face recognition model, and on multiple face image quality assessment models. The analysis is conducted over a range of specific image target sizes. Four compression types are considered, namely JPEG, JPEG 2000, downscaled PNG, and notably the new JPEG XL format. Frontal color images from the ColorFERET database were used in a Region Of Interest (ROI) variant and a portrait variant. We primarily conclude that JPEG XL allows for superior mean and worst case face recognition performance especially at lower target sizes, below approximately 5kB for the ROI variant, while there appears to be no critical advantage among the compression types at higher target sizes. Quality assessments from modern models correlate well overall with the compression effect on face recognition performance.',\n",
              " 'Automatic damage assessment based on UAV-derived 3D point clouds can provide fast information on the damage situation after an earthquake. However, the assessment of multiple damage grades is challenging due to the variety in damage patterns and limited transferability of existing methods to other geographic regions or data sources. We present a novel approach to automatically assess multi-class building damage from real-world multi-temporal point clouds using a machine learning model trained on virtual laser scanning (VLS) data. We (1) identify object-specific change features, (2) separate changed and unchanged building parts, (3) train a random forest machine learning model with VLS data based on object-specific change features, and (4) use the classifier to assess building damage in real-world point clouds from photogrammetry-based dense image matching (DIM). We evaluate classifiers trained on different input data with respect to their capacity to classify three damage grades (heavy, extreme, destruction) in pre- and post-event DIM point clouds of a real earthquake event. Our approach is transferable with respect to multi-source input point clouds used for training (VLS) and application (DIM) of the model. We further achieve geographic transferability of the model by training it on simulated data of geometric change which characterises relevant damage grades across different geographic regions. The model yields high multi-target classification accuracies (overall accuracy: 92.0% - 95.1%). Its performance improves only slightly when using real-world region-specific training data (< 3% higher overall accuracies) and when using real-world region-specific training data (< 2% higher overall accuracies). We consider our approach relevant for applications where timely information on the damage situation is required and sufficient real-world training data is not available.',\n",
              " 'In recent years, as the use of micromobility gained popularity, technological challenges connected to e-scooters became increasingly important. This paper focuses on road surface recognition, an important task in this area. A reliable and accurate method for road surface recognition can help improve the safety and stability of the vehicle. Here a data-driven method is proposed to recognize if an e-scooter is on a road or a sidewalk. The proposed method uses only the widely available inertial measurement unit (IMU) sensors on a smartphone device. deep neural networks (DNNs) are used to infer whether an e-scooteris driving on a road or on a sidewalk by solving a binary classification problem. A data set is collected and several different deep models as well as classical machine learning approaches for the binary classification problem are applied and compared. Experiment results on a route containing the two surfaces are presented demonstrating the DNNs ability to distinguish between them.',\n",
              " 'Histopathological tissue classification is a fundamental task in computational pathology. Deep learning-based models have achieved superior performance but centralized training with data centralization suffers from the privacy leakage problem. Federated learning (FL) can safeguard privacy by keeping training samples locally, but existing FL-based frameworks require a large number of well-annotated training samples and numerous rounds of communication which hinder their practicability in the real-world clinical scenario. In this paper, we propose a universal and lightweight federated learning framework, named Federated Deep-Broad Learning (FedDBL), to achieve superior classification performance with limited training samples and only one-round communication. By simply associating a pre-trained deep learning feature extractor, a fast and lightweight broad learning inference system and a classical federated aggregation approach, FedDBL can dramatically reduce data dependency and improve communication efficiency. Five-fold cross-validation demonstrates that FedDBL greatly outperforms the competitors with only one-round communication and limited training samples, while it even achieves comparable performance with the ones under multiple-round communications. Furthermore, due to the lightweight design and one-round communication, FedDBL reduces the communication burden from 4.6GB to only 276.5KB per client using the ResNet-50 backbone at 50-round training. Since no data or deep model sharing across different clients, the privacy issue is well-solved and the model security is guaranteed with no model inversion attack risk. Code is available at https://github.com/tianpeng-deng/FedDBL.',\n",
              " 'Safe human-robot collaboration (HRC) has recently gained a lot of interest with the emerging Industry 5.0 paradigm. Conventional robots are being replaced with more intelligent and flexible collaborative robots (cobots). Safe and efficient collaboration between cobots and humans largely relies on the cobot\\'s comprehensive semantic understanding of the dynamic surrounding of industrial environments. Despite the importance of semantic understanding for such applications, 3D semantic segmentation of collaborative robot workspaces lacks sufficient research and dedicated datasets. The performance limitation caused by insufficient datasets is called \\'data hunger\\' problem. To overcome this current limitation, this work develops a new dataset specifically designed for this use case, named \"COVERED\", which includes point-wise annotated point clouds of a robotic cell. Lastly, we also provide a benchmark of current state-of-the-art (SOTA) algorithm performance on the dataset and demonstrate a real-time semantic segmentation of a collaborative robot workspace using a multi-LiDAR system. The promising results from using the trained Deep Networks on a real-time dynamically changing situation shows that we are on the right track. Our perception pipeline achieves 20Hz throughput with a prediction point accuracy of $>$96\\\\% and $>$92\\\\% mean intersection over union (mIOU) while maintaining an 8Hz throughput.',\n",
              " 'Morphological atlases are an important tool in organismal studies, and modern high-throughput Computed Tomography (CT) facilities can produce hundreds of full-body high-resolution volumetric images of organisms. However, creating an atlas from these volumes requires accurate organ segmentation. In the last decade, machine learning approaches have achieved incredible results in image segmentation tasks, but they require large amounts of annotated data for training. In this paper, we propose a self-training framework for multi-organ segmentation in tomographic images of Medaka fish. We utilize the pseudo-labeled data from a pretrained Teacher model and adopt a Quality Classifier to refine the pseudo-labeled data. Then, we introduce a pixel-wise knowledge distillation method to prevent overfitting to the pseudo-labeled data and improve the segmentation performance. The experimental results demonstrate that our method improves mean Intersection over Union (IoU) by 5.9% on the full dataset and enables keeping the quality while using three times less markup.',\n",
              " 'This paper proposes crack segmentation augmented by super resolution (SR) with deep neural networks. In the proposed method, a SR network is jointly trained with a binary segmentation network in an end-to-end manner. This joint learning allows the SR network to be optimized for improving segmentation results. For realistic scenarios, the SR network is extended from non-blind to blind for processing a low-resolution image degraded by unknown blurs. The joint network is improved by our proposed two extra paths that further encourage the mutual optimization between SR and segmentation. Comparative experiments with SoTA segmentation methods demonstrate the superiority of our joint learning, and various ablation studies prove the effects of our contributions.',\n",
              " 'Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. While image editing with GANs builds upon latent space, DMs rely on editing the conditions such as text prompts. We present an unsupervised method to discover interpretable editing directions for the latent variables $\\\\mathbf{x}_t \\\\in \\\\mathcal{X}$ of DMs. Our method adopts Riemannian geometry between $\\\\mathcal{X}$ and the intermediate feature maps $\\\\mathcal{H}$ of the U-Nets to provide a deep understanding over the geometrical structure of $\\\\mathcal{X}$. The discovered semantic latent directions mostly yield disentangled attribute changes, and they are globally consistent across different samples. Furthermore, editing in earlier timesteps edits coarse attributes, while ones in later timesteps focus on high-frequency details. We define the curvedness of a line segment between samples to show that $\\\\mathcal{X}$ is a curved manifold. Experiments on different baselines and datasets demonstrate the effectiveness of our method even on Stable Diffusion. Our source code will be publicly available for the future researchers.',\n",
              " 'Whole-body PET/CT scan is an important tool for diagnosing various malignancies (e.g., malignant melanoma, lymphoma, or lung cancer), and accurate segmentation of tumors is a key part for subsequent treatment. In recent years, CNN-based segmentation methods have been extensively investigated. However, these methods often give inaccurate segmentation results, such as over-segmentation and under-segmentation. Therefore, to address such issues, we propose a post-processing method based on a graph convolutional neural network (GCN) to refine inaccurate segmentation parts and improve the overall segmentation accuracy. Firstly, nnUNet is used as an initial segmentation framework, and the uncertainty in the segmentation results is analyzed. Certainty and uncertainty nodes establish the nodes of a graph neural network. Each node and its 6 neighbors form an edge, and 32 nodes are randomly selected for uncertain nodes to form edges. The highly uncertain nodes are taken as the subsequent refinement targets. Secondly, the nnUNet result of the certainty nodes is used as label to form a semi-supervised graph network problem, and the uncertainty part is optimized through training the GCN network to improve the segmentation performance. This describes our proposed nnUNet-GCN segmentation framework. We perform tumor segmentation experiments on the PET/CT dataset in the MICCIA2022 autoPET challenge. Among them, 30 cases are randomly selected for testing, and the experimental results show that the false positive rate is effectively reduced with nnUNet-GCN refinement. In quantitative analysis, there is an improvement of 2.12 % on the average Dice score, 6.34 on 95 % Hausdorff Distance (HD95), and 1.72 on average symmetric surface distance (ASSD). The quantitative and qualitative evaluation results show that GCN post-processing methods can effectively improve tumor segmentation performance.',\n",
              " 'Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\\\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\\\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.',\n",
              " 'This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains. The code and pre-trained models are publicly available at https://github.com/isl-org/ZoeDepth .',\n",
              " 'Contrastive self-supervised learning methods famously produce high quality transferable representations by learning invariances to different data augmentations. Invariances established during pre-training can be interpreted as strong inductive biases. However these may or may not be helpful, depending on if they match the invariance requirements of downstream tasks or not. This has led to several attempts to learn task-specific invariances during pre-training, however, these methods are highly compute intensive and tedious to train. We introduce the notion of amortised invariance learning for contrastive self supervision. In the pre-training stage, we parameterize the feature extractor by differentiable invariance hyper-parameters that control the invariances encoded by the representation. Then, for any downstream task, both linear readout and task-specific invariance requirements can be efficiently and effectively learned by gradient-descent. We evaluate the notion of amortised invariances for contrastive learning over two different modalities: vision and audio, on two widely-used contrastive learning methods in vision: SimCLR and MoCo-v2 with popular architectures like ResNets and Vision Transformers, and SimCLR with ResNet-18 for audio. We show that our amortised features provide a reliable way to learn diverse downstream tasks with different invariance requirements, while using a single feature and avoiding task-specific pre-training. This provides an exciting perspective that opens up new horizons in the field of general purpose representation learning.',\n",
              " 'We propose a method to reconstruct global human trajectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; methods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven human motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challenging in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack. Code and video results can be found at https://vye16.github.io/slahmr.',\n",
              " 'Video-Text Retrieval (VTR) aims to search for the most relevant video related to the semantics in a given sentence, and vice versa. In general, this retrieval task is composed of four successive steps: video and textual feature representation extraction, feature embedding and matching, and objective functions. In the last, a list of samples retrieved from the dataset is ranked based on their matching similarities to the query. In recent years, significant and flourishing progress has been achieved by deep learning techniques, however, VTR is still a challenging task due to the problems like how to learn an efficient spatial-temporal video feature and how to narrow the cross-modal gap. In this survey, we review and summarize over 100 research papers related to VTR, demonstrate state-of-the-art performance on several commonly benchmarked datasets, and discuss potential challenges and directions, with the expectation to provide some insights for researchers in the field of video-text retrieval.',\n",
              " 'Disease severity regression by a convolutional neural network (CNN) for medical images requires a sufficient number of image samples labeled with severity levels. Conditional generative adversarial network (cGAN)-based data augmentation (DA) is a possible solution, but it encounters two issues. The first issue is that existing cGANs cannot deal with real-valued severity levels as their conditions, and the second is that the severity of the generated images is not fully reliable. We propose continuous DA as a solution to the two issues. Our method uses continuous severity GAN to generate images at real-valued severity levels and dataset-disjoint multi-objective optimization to deal with the second issue. Our method was evaluated for estimating ulcerative colitis (UC) severity of endoscopic images and achieved higher classification performance than conventional DA methods.',\n",
              " 'Designing realistic digital humans is extremely complex. Most data-driven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes. In this paper, we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neural-network-based generative models of 3D head and body meshes. Encouraging the latent variables of mesh variational autoencoders (VAEs) or generative adversarial networks (GANs) to follow the local eigenprojections of identity attributes, we improve latent disentanglement and properly decouple the attribute creation. Experimental results show that our local eigenprojection disentangled (LED) models not only offer improved disentanglement with respect to the state-of-the-art, but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models.',\n",
              " \"We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but does not require any updates to the diffusion network's parameters. MCM is a small module trained to modulate the diffusion network's predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only $\\\\sim$1$\\\\%$ of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.\",\n",
              " 'Deep neural networks (DNNs) are sensitive to adversarial examples, resulting in fragile and unreliable performance in the real world. Although adversarial training (AT) is currently one of the most effective methodologies to robustify DNNs, it is computationally very expensive (e.g., 5-10X costlier than standard training). To address this challenge, existing approaches focus on single-step AT, referred to as Fast AT, reducing the overhead of adversarial example generation. Unfortunately, these approaches are known to fail against stronger adversaries. To make AT computationally efficient without compromising robustness, this paper takes a different view of the efficient AT problem. Specifically, we propose to minimize redundancies at the data level by leveraging data pruning. Extensive experiments demonstrate that the data pruning based AT can achieve similar or superior robust (and clean) accuracy as its unpruned counterparts while being significantly faster. For instance, proposed strategies accelerate CIFAR-10 training up to 3.44X and CIFAR-100 training to 2.02X. Additionally, the data pruning methods can readily be reconciled with existing adversarial acceleration tricks to obtain the striking speed-ups of 5.66X and 5.12X on CIFAR-10, 3.67X and 3.07X on CIFAR-100 with TRADES and MART, respectively.',\n",
              " 'In this paper we discuss a new variational approach to the Date Fusion problem of multi-spectral satellite images from Sentinel-2 and MODIS that have been captured at different resolution level and, arguably, on different days. The crucial point of our approach that the MODIS image is cloud-free whereas the images from Sentinel-2 can be corrupted by clouds or noise.',\n",
              " 'Most of the existing audio-driven 3D facial animation methods suffered from the lack of detailed facial expression and head pose, resulting in unsatisfactory experience of human-robot interaction. In this paper, a novel pose-controllable 3D facial animation synthesis method is proposed by utilizing hierarchical audio-vertex attention. To synthesize real and detailed expression, a hierarchical decomposition strategy is proposed to encode the audio signal into both a global latent feature and a local vertex-wise control feature. Then the local and global audio features combined with vertex spatial features are used to predict the final consistent facial animation via a graph convolutional neural network by fusing the intrinsic spatial topology structure of the face model and the corresponding semantic feature of the audio. To accomplish pose-controllable animation, we introduce a novel pose attribute augmentation method by utilizing the 2D talking face technique. Experimental results indicate that the proposed method can produce more realistic facial expressions and head posture movements. Qualitative and quantitative experiments show that the proposed method achieves competitive performance against state-of-the-art methods.',\n",
              " 'In this paper, we introduce the spatial bias to learn global knowledge without self-attention in convolutional neural networks. Owing to the limited receptive field, conventional convolutional neural networks suffer from learning long-range dependencies. Non-local neural networks have struggled to learn global knowledge, but unavoidably have too heavy a network design due to the self-attention operation. Therefore, we propose a fast and lightweight spatial bias that efficiently encodes global knowledge without self-attention on convolutional neural networks. Spatial bias is stacked on the feature map and convolved together to adjust the spatial structure of the convolutional features. Therefore, we learn the global knowledge on the convolution layer directly with very few additional resources. Our method is very fast and lightweight due to the attention-free non-local method while improving the performance of neural networks considerably. Compared to non-local neural networks, the spatial bias use about 10 times fewer parameters while achieving comparable performance with 1.6 ~ 3.3 times more throughput on a very little budget. Furthermore, the spatial bias can be used with conventional non-local neural networks to further improve the performance of the backbone model. We show that the spatial bias achieves competitive performance that improves the classification accuracy by +0.79% and +1.5% on ImageNet-1K and cifar100 datasets. Additionally, we validate our method on the MS-COCO and ADE20K datasets for downstream tasks involving object detection and semantic segmentation.',\n",
              " \"Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems $\\\\unicode{x2013}$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.\",\n",
              " 'Convolutional neural networks (CNNs) have achieved superior performance but still lack clarity about the nature and properties of feature extraction. In this paper, by analyzing the sensitivity of neural networks to frequencies and scales, we find that neural networks not only have low- and medium-frequency biases but also prefer different frequency bands for different classes, and the scale of objects influences the preferred frequency bands. These observations lead to the hypothesis that neural networks must learn the ability to extract features at various scales and frequencies. To corroborate this hypothesis, we propose a network architecture based on Gaussian derivatives, which extracts features by constructing scale space and employing partial derivatives as local feature extraction operators to separate high-frequency information. This manually designed method of extracting features from different scales allows our GSSDNets to achieve comparable accuracy with vanilla networks on various datasets.',\n",
              " \"Current Deep Network (DN) visualization and interpretability methods rely heavily on data space visualizations such as scoring which dimensions of the data are responsible for their associated prediction or generating new data features or samples that best match a given DN unit or representation. In this paper, we go one step further by developing the first provably exact method for computing the geometry of a DN's mapping - including its decision boundary - over a specified region of the data space. By leveraging the theory of Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a DNs geometry without resorting to approximations such as sampling or architecture simplification. SplineCam applies to any DN architecture based on CPWL nonlinearities, including (leaky-)ReLU, absolute value, maxout, and max-pooling and can also be applied to regression DNs such as implicit neural representations. Beyond decision boundary visualization and characterization, SplineCam enables one to compare architectures, measure generalizability and sample from the decision boundary on or off the manifold. Project Website: bit.ly/splinecam.\",\n",
              " 'Visibility underwater is challenging, and degrades as the distance between the subject and camera increases, making vision tasks in the forward-looking direction more difficult. We have collected underwater forward-looking stereo-vision and visual-inertial image sets in the Mediterranean and Red Sea. To our knowledge there are no other public datasets in the underwater environment acquired with this camera-sensor orientation published with ground-truth. These datasets are critical for the development of several underwater applications, including obstacle avoidance, visual odometry, 3D tracking, Simultaneous Localization and Mapping (SLAM) and depth estimation. The stereo datasets include synchronized stereo images in dynamic underwater environments with objects of known-size. The visual-inertial datasets contain monocular images and IMU measurements, aligned with millisecond resolution timestamps and objects of known size which were placed in the scene. Both sensor configurations allow for scale estimation, with the calibrated baseline in the stereo setup and the IMU in the visual-inertial setup. Ground truth depth maps were created offline for both dataset types using photogrammetry. The ground truth is validated with multiple known measurements placed throughout the imaged environment. There are 5 stereo and 8 visual-inertial datasets in total, each containing thousands of images, with a range of different underwater visibility and ambient light conditions, natural and man-made structures and dynamic camera motions. The forward-looking orientation of the camera makes these datasets unique and ideal for testing underwater obstacle-avoidance algorithms and for navigation close to the seafloor in dynamic environments. With our datasets, we hope to encourage the advancement of autonomous functionality for underwater vehicles in dynamic and/or shallow water environments.',\n",
              " 'Malignant mesothelioma is classified into three histological subtypes, Epithelioid, Sarcomatoid, and Biphasic according to the relative proportions of epithelioid and sarcomatoid tumor cells present. Biphasic tumors display significant populations of both cell types. This subtyping is subjective and limited by current diagnostic guidelines and can differ even between expert thoracic pathologists when characterising the continuum of relative proportions of epithelioid and sarcomatoid components using a three class system. In this work, we develop a novel dual-task Graph Neural Network (GNN) architecture with ranking loss to learn a model capable of scoring regions of tissue down to cellular resolution. This allows quantitative profiling of a tumor sample according to the aggregate sarcomatoid association score of all the cells in the sample. The proposed approach uses only core-level labels and frames the prediction task as a dual multiple instance learning (MIL) problem. Tissue is represented by a cell graph with both cell-level morphological and regional features. We use an external multi-centric test set from Mesobank, on which we demonstrate the predictive performance of our model. We validate our model predictions through an analysis of the typical morphological features of cells according to their predicted score, finding that some of the morphological differences identified by our model match known differences used by pathologists. We further show that the model score is predictive of patient survival with a hazard ratio of 2.30. The code for the proposed approach, along with the dataset, is available at: https://github.com/measty/MesoGraph.',\n",
              " 'Omnidirectional image quality assessment (OIQA) aims to predict the perceptual quality of omnidirectional images that cover the whole 180$\\\\times$360$^{\\\\circ}$ viewing range of the visual environment. Here we propose a blind/no-reference OIQA method named S$^2$ that bridges the gap between low-level statistics and high-level semantics of omnidirectional images. Specifically, statistic and semantic features are extracted in separate paths from multiple local viewports and the hallucinated global omnidirectional image, respectively. A quality regression along with a weighting process is then followed that maps the extracted quality-aware features to a perceptual quality prediction. Experimental results demonstrate that the proposed S$^2$ method offers highly competitive performance against state-of-the-art methods.',\n",
              " 'Layer-wise relevance propagation (LRP) is a widely used and powerful technique to reveal insights into various artificial neural network (ANN) architectures. LRP is often used in the context of image classification. The aim is to understand, which parts of the input sample have highest relevance and hence most influence on the model prediction. Relevance can be traced back through the network to attribute a certain score to each input pixel. Relevance scores are then combined and displayed as heat maps and give humans an intuitive visual understanding of classification models. Opening the black box to understand the classification engine in great detail is essential for domain experts to gain trust in ANN models. However, there are pitfalls in terms of model-inherent artifacts included in the obtained relevance maps, that can easily be missed. But for a valid interpretation, these artifacts must not be ignored. Here, we apply and revise LRP on various ANN architectures trained as classifiers on geospatial and synthetic data. Depending on the network architecture, we show techniques to control model focus and give guidance to improve the quality of obtained relevance maps to separate facts from artifacts.',\n",
              " 'This paper describes our participation in SemEval-2023 Task 9, Intimacy Analysis of Multilingual Tweets. We fine-tune some of the most popular transformer models with the training dataset and synthetic data generated by different data augmentation techniques. During the development phase, our best results were obtained by using XLM-T. Data augmentation techniques provide a very slight improvement in the results. Our system ranked in the 27th position out of the 45 participating systems. Despite its modest results, our system shows promising results in languages such as Portuguese, English, and Dutch. All our code is available in the repository \\\\url{https://github.com/isegura/hulat_intimacy}.',\n",
              " \"Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases. Anonymous codes are available at \\\\url{https://anonymous.4open.science/r/table2text-88ED}.\",\n",
              " \"Knowledge graphs (KGs) have received increasing attention due to its wide applications on natural language processing. However, its use case on temporal question answering (QA) has not been well-explored. Most of existing methods are developed based on pre-trained language models, which might not be capable to learn \\\\emph{temporal-specific} presentations of entities in terms of temporal KGQA task. To alleviate this problem, we propose a novel \\\\textbf{T}ime-aware \\\\textbf{M}ultiway \\\\textbf{A}daptive (\\\\textbf{TMA}) fusion network. Inspired by the step-by-step reasoning behavior of humans. For each given question, TMA first extracts the relevant concepts from the KG, and then feeds them into a multiway adaptive module to produce a \\\\emph{temporal-specific} representation of the question. This representation can be incorporated with the pre-trained KG embedding to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex questions are absolutely improved by 24\\\\% and 10\\\\% compared to the best-performing baseline. Furthermore, we also show that TMA employing an adaptive fusion mechanism can provide interpretability by analyzing the proportion of information in question representations.\",\n",
              " \"Transformer-based pre-trained models have achieved great improvements in semantic matching. However, existing models still suffer from insufficient ability to capture subtle differences. The modification, addition and deletion of words in sentence pairs may make it difficult for the model to predict their relationship. To alleviate this problem, we propose a novel Dual Path Modeling Framework to enhance the model's ability to perceive subtle differences in sentence pairs by separately modeling affinity and difference semantics. Based on dual-path modeling framework we design the Dual Path Modeling Network (DPM-Net) to recognize semantic relations. And we conduct extensive experiments on 10 well-studied semantic matching and robustness test datasets, and the experimental results show that our proposed method achieves consistent improvements over baselines.\",\n",
              " \"Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and inability to use external knowledge.This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in consolidated external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of mission-critical scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.\",\n",
              " \"Recently, much exertion has been paid to design graph self-supervised methods to obtain generalized pre-trained models, and adapt pre-trained models onto downstream tasks through fine-tuning. However, there exists an inherent gap between pretext and downstream graph tasks, which insufficiently exerts the ability of pre-trained models and even leads to negative transfer. Meanwhile, prompt tuning has seen emerging success in natural language processing by aligning pre-training and fine-tuning with consistent training objectives. In this paper, we identify the challenges for graph prompt tuning: The first is the lack of a strong and universal pre-training task across sundry pre-training methods in graph domain. The second challenge lies in the difficulty of designing a consistent training objective for both pre-training and downstream tasks. To overcome above obstacles, we propose a novel framework named SGL-PT which follows the learning strategy ``Pre-train, Prompt, and Predict''. Specifically, we raise a strong and universal pre-training task coined as SGL that acquires the complementary merits of generative and contrastive self-supervised graph learning. And aiming for graph classification task, we unify pre-training and fine-tuning by designing a novel verbalizer-free prompting function, which reformulates the downstream task in a similar format as pretext task. Empirical results show that our method surpasses other baselines under unsupervised setting, and our prompt tuning method can greatly facilitate models on biological datasets over fine-tuning methods.\",\n",
              " 'Distilled self-supervised models have shown competitive performance and efficiency in recent years. However, there is a lack of experience in jointly distilling multiple self-supervised speech models. In our work, we performed Ensemble Knowledge Distillation (EKD) on various self-supervised speech models such as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation techniques, layerwise-average and layerwise-concatenation, to the representations of different teacher models and found that the former was more effective. On top of that, we proposed a multiple prediction head method for student models to predict different layer outputs of multiple teacher models simultaneously. The experimental results show that our method improves the performance of the distilled models on four downstream speech processing tasks, Phoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic Speech Recognition in the hidden-set track of the SUPERB benchmark.',\n",
              " 'Unsupervised extractive summarization aims to extract salient sentences from a document as the summary without labeled data. Recent literatures mostly research how to leverage sentence similarity to rank sentences in the order of salience. However, sentence similarity estimation using pre-trained language models mostly takes little account of document-level information and has a weak correlation with sentence salience ranking. In this paper, we proposed two novel strategies to improve sentence similarity estimation for unsupervised extractive summarization. We use contrastive learning to optimize a document-level objective that sentences from the same document are more similar than those from different documents. Moreover, we use mutual learning to enhance the relationship between sentence similarity estimation and sentence salience ranking, where an extra signal amplifier is used to refine the pivotal information. Experimental results demonstrate the effectiveness of our strategies.',\n",
              " \"Effective figure captions are crucial for clear comprehension of scientific figures, yet poor caption writing remains a common issue in scientific articles. Our study of arXiv cs.CL papers found that 53.88% of captions were rated as unhelpful or worse by domain experts, showing the need for better caption generation. Previous efforts in figure caption generation treated it as a vision task, aimed at creating a model to understand visual content and complex contextual information. Our findings, however, demonstrate that over 75% of figure captions' tokens align with corresponding figure-mentioning paragraphs, indicating great potential for language technology to solve this task. In this paper, we present a novel approach for generating figure captions in scientific documents using text summarization techniques. Our approach extracts sentences referencing the target figure, then summarizes them into a concise caption. In the experiments on real-world arXiv papers (81.2% were published at academic conferences), our method, using only text data, outperformed previous approaches in both automatic and human evaluations. We further conducted data-driven investigations into the two core challenges: (i) low-quality author-written captions and (ii) the absence of a standard for good captions. We found that our models could generate improved captions for figures with original captions rated as unhelpful, and the model trained on captions with more than 30 tokens produced higher-quality captions. We also found that good captions often include the high-level takeaway of the figure. Our work proves the effectiveness of text summarization in generating figure captions for scholarly articles, outperforming prior vision-based approaches. Our findings have practical implications for future figure captioning systems, improving scientific communication clarity.\",\n",
              " 'Leveraging contextual knowledge has become standard practice in automated claim verification, yet the impact of temporal reasoning has been largely overlooked. Our study demonstrates that time positively influences the claim verification process of evidence-based fact-checking. The temporal aspects and relations between claims and evidence are first established through grounding on shared timelines, which are constructed using publication dates and time expressions extracted from their text. Temporal information is then provided to RNN-based and Transformer-based classifiers before or after claim and evidence encoding. Our time-aware fact-checking models surpass base models by up to 9% Micro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also outperform prior methods that explicitly model temporal relations between evidence. Our findings show that the presence of temporal information and the manner in which timelines are constructed greatly influence how fact-checking models determine the relevance and supporting or refuting character of evidence documents.',\n",
              " \"Pre-trained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the topological gap between tabular data and text and the lack of domain-specific knowledge make it difficult for PLMs to produce faithful text, especially in real-world applications with limited resources. In this paper, we mitigate the above challenges by introducing a novel augmentation method: Prompt-based Adapter (PA), which targets table-to-text generation under few-shot conditions. The core insight design of the PA is to inject prompt templates for augmenting domain-specific knowledge and table-related representations into the model for bridging the structural gap between tabular data and descriptions through adapters. Such prompt-based knowledge augmentation method brings at least two benefits: (1) enables us to fully use the large amounts of unlabelled domain-specific knowledge, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (2) allows us to design different types of tasks supporting the generative challenge. Extensive experiments and analyses are conducted on three open-domain few-shot NLG datasets: Humans, Books, and Songs. Compared to previous state-of-the-art approaches, our model achieves superior performance in terms of both fluency and accuracy as judged by human and automatic evaluations.\",\n",
              " 'This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model.',\n",
              " 'Large Language Models (LLMs) have yielded fast and dramatic progress in NLP, and now offer strong few- and zero-shot capabilities on new tasks, reducing the need for annotation. This is especially exciting for the medical domain, in which supervision is often scant and expensive. At the same time, model predictions are rarely so accurate that they can be trusted blindly. Clinicians therefore tend to favor \"interpretable\" classifiers over opaque LLMs. For example, risk prediction tools are often linear models defined over manually crafted predictors that must be laboriously extracted from EHRs. We propose CHiLL (Crafting High-Level Latents), which uses LLMs to permit natural language specification of high-level features for linear models via zero-shot feature extraction using expert-composed queries. This approach has the promise to empower physicians to use their domain expertise to craft features which are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR (as often done now). We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate our approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using \"Bag-of-Words\" features. We verify that learned feature weights align well with clinical expectations.',\n",
              " 'The recent progress in text-based audio retrieval was largely propelled by the release of suitable datasets. Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solution to create large-scale datasets. We study the recently proposed SoundDesc benchmark dataset, which was automatically sourced from the BBC Sound Effects web page. In our analysis, we find that SoundDesc contains several duplicates that cause leakage of training data to the evaluation data. This data leakage ultimately leads to overly optimistic retrieval performance estimates in previous benchmarks. We propose new training, validation, and testing splits for the dataset that we make available online. To avoid weak contamination of the test data, we pool audio files that share similar recording setups. In our experiments, we find that the new splits serve as a more challenging benchmark.',\n",
              " 'In this paper, we describe VivesDebate-Speech, a corpus of spoken argumentation created to leverage audio features for argument mining tasks. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities, and one of the most complete publicly available resources in this topic. Moreover, we have performed a set of first-of-their-kind experiments which show an improvement when integrating audio features into the argument mining pipeline. The provided results can be used as a baseline for future research.',\n",
              " \"Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems $\\\\unicode{x2013}$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.\",\n",
              " 'Despite recent advancements in Machine Learning, many tasks still involve working in low-data regimes which can make solving natural language problems difficult. Recently, a number of text augmentation techniques have emerged in the field of Natural Language Processing (NLP) which can enrich the training data with new examples, though they are not without their caveats. For instance, simple rule-based heuristic methods are effective, but lack variation in semantic content and syntactic structure with respect to the original text. On the other hand, more complex deep learning approaches can cause extreme shifts in the intrinsic meaning of the text and introduce unwanted noise into the training data. To more reliably control the quality of the augmented examples, we introduce a state-of-the-art approach for Self-Controlled Text Augmentation (STA). Our approach tightly controls the generation process by introducing a self-checking procedure to ensure that generated examples retain the semantic content of the original text. Experimental results on multiple benchmarking datasets demonstrate that STA substantially outperforms existing state-of-the-art techniques, whilst qualitative analysis reveals that the generated examples are both lexically diverse and semantically reliable.',\n",
              " 'Many measures of societal bias in language models have been proposed in recent years. A popular approach is to use a set of word filling prompts to evaluate the behavior of the language models. In this work, we analyze the validity of two such measures -- StereoSet and CrowS-Pairs. We show that these measures produce unexpected and illogical results when appropriate control group samples are constructed. Based on this, we believe that they are problematic and using them in the future should be reconsidered. We propose a way forward with an improved testing protocol. Finally, we also introduce a new gender bias dataset for Slovak.',\n",
              " 'Decision-makers in the humanitarian sector rely on timely and exact information during crisis events. Knowing how many civilians were injured during an earthquake is vital to allocate aids properly. Information about such victim counts is often only available within full-text event descriptions from newspapers and other reports. Extracting numbers from text is challenging: numbers have different formats and may require numeric reasoning. This renders purely string matching-based approaches insufficient. As a consequence, fine-grained counts of injured, displaced, or abused victims beyond fatalities are often not extracted and remain unseen. We cast victim count extraction as a question answering (QA) task with a regression or classification objective. We compare regex, dependency parsing, semantic role labeling-based approaches, and advanced text-to-text models. Beyond model accuracy, we analyze extraction reliability and robustness which are key for this sensitive task. In particular, we discuss model calibration and investigate few-shot and out-of-distribution performance. Ultimately, we make a comprehensive recommendation on which model to select for different desiderata and data domains. Our work is among the first to apply numeracy-focused large language models in a real-world use case with a positive impact.',\n",
              " 'Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input.',\n",
              " 'With language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. Most research on evaluating and mitigating fairness harms has been concentrated on English, while multilingual models and non-English languages have received comparatively little attention. In this paper, we survey different aspects of fairness in languages beyond English and multilingual contexts. This paper presents a survey of fairness in multilingual and non-English contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for English. We contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. Thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.',\n",
              " 'Recent advancements in interpretability research made transformer language models more transparent. This progress led to a better understanding of their inner workings for toy and naturally occurring models. However, how these models internally process sentiment changes has yet to be sufficiently answered. In this work, we introduce a new interpretability tool called PCP ablation, where we replace modules with low-rank matrices based on the principal components of their activations, reducing model parameters and their behavior to essentials. We demonstrate PCP ablations on MLP and attention layers in backdoored toy, backdoored large, and naturally occurring models. We determine MLPs as most important for the backdoor mechanism and use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements via PCP ablation.',\n",
              " \"Data multiplexing is a recently proposed method for improving a model's inference efficiency by processing multiple instances simultaneously using an ordered representation mixture. Prior work on data multiplexing only used task-specific Transformers without any pre-training, which limited their accuracy and generality. In this paper, we develop pre-trained multiplexed language models (MUX-PLMs) that can be widely finetuned on any downstream task. Our approach includes a three-stage training procedure and novel multiplexing and demultiplexing modules for improving throughput and downstream task accuracy. We demonstrate our method on BERT and ELECTRA pre-training objectives, with our MUX-BERT and MUX-ELECTRA models achieving 2x/5x inference speedup with a 2-4 \\\\% drop in absolute performance on GLUE and 1-2 \\\\% drop on token-level tasks.\",\n",
              " \"Multilingual generative language models (LMs) are increasingly fluent in a large variety of languages. Trained on the concatenation of corpora in multiple languages, they enable powerful transfer from high-resource languages to low-resource ones. However, it is still unknown what cultural biases are induced in the predictions of these models. In this work, we focus on one language property highly influenced by culture: formality. We analyze the formality distributions of XGLM and BLOOM's predictions, two popular generative multilingual language models, in 5 languages. We classify 1,200 generations per language as formal, informal, or incohesive and measure the impact of the prompt formality on the predictions. Overall, we observe a diversity of behaviors across the models and languages. For instance, XGLM generates informal text in Arabic and Bengali when conditioned with informal prompts, much more than BLOOM. In addition, even though both models are highly biased toward the formal style when prompted neutrally, we find that the models generate a significant amount of informal predictions even when prompted with formal text. We release with this work 6,000 annotated samples, paving the way for future work on the formality of generative multilingual LMs.\",\n",
              " 'Temporal concept drift refers to the problem of data changing over time. In NLP, that would entail that language (e.g. new expressions, meaning shifts) and factual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing on the latter, we benchmark $11$ pretrained masked language models (MLMs) on a series of tests designed to evaluate the effect of temporal concept drift, as it is crucial that widely used language models remain up-to-date with the ever-evolving factual updates of the real world. Specifically, we provide a holistic framework that (1) dynamically creates temporal test sets of any time granularity (e.g. month, quarter, year) of factual data from Wikidata, (2) constructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to ensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways (single-token probing, multi-token generation, MLM scoring). In contrast to prior work, our framework aims to unveil how robust an MLM is over time and thus to provide a signal in case it has become outdated, by leveraging multiple views of evaluation.',\n",
              " 'Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1.',\n",
              " 'Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example in a black-box language model. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where state-of-the-art results are achieved on arithmetic reasoning (+2.7\\\\%), commonsense reasoning (+3.4\\\\%), symbolic reasoning (+3.2\\\\%), and non-reasoning tasks (+2.5\\\\%). Our code will be available at https://github.com/shizhediao/automate-cot.',\n",
              " 'Dictionaries are one of the oldest and most used linguistic resources. Building them is a complex task that, to the best of our knowledge, has yet to be explored with generative Large Language Models (LLMs). We introduce the \"Spanish Built Factual Freectianary\" (Spanish-BFF) as the first Spanish IA-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We also define future steps we aim to follow to improve this initial commitment to the field, such as more additional languages.',\n",
              " 'When humans read a text, their eye movements are influenced by the structural complexity of the input sentences. This cognitive phenomenon holds across languages and recent studies indicate that multilingual language models utilize structural similarities between languages to facilitate cross-lingual transfer. We use sentence-level eye-tracking patterns as a cognitive indicator for structural complexity and show that the multilingual model XLM-RoBERTa can successfully predict varied patterns for 13 typologically diverse languages, despite being fine-tuned only on English data. We quantify the sensitivity of the model to structural complexity and distinguish a range of complexity characteristics. Our results indicate that the model develops a meaningful bias towards sentence length but also integrates cross-lingual differences. We conduct a control experiment with randomized word order and find that the model seems to additionally capture more complex structural information.',\n",
              " \"In this paper, we propose Tutoring bot, a generative chatbot trained on a large scale of tutor-student conversations for English-language learning. To mimic a human tutor's behavior in language education, the tutor bot leverages diverse educational instructions and grounds to each instruction as additional input context for the tutor response generation. As a single instruction generally involves multiple dialogue turns to give the student sufficient speaking practice, the tutor bot is required to monitor and capture when the current instruction should be kept or switched to the next instruction. For that, the tutor bot is learned to not only generate responses but also infer its teaching action and progress on the current conversation simultaneously by a multi-task learning scheme. Our Tutoring bot is deployed under a non-commercial use license at https://tutoringai.com.\",\n",
              " 'Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and reading. CARE facilitates data collection for inline commentaries in a commonplace collaborative reading environment, and provides a framework for enhancing reading with NLP-based assistance, such as text classification, generation or question answering. The extensible behavioral logging allows unique insights into the reading and commenting behavior, and flexible configuration makes the platform easy to deploy in new scenarios. To evaluate CARE in action, we apply the platform in a user study dedicated to scholarly peer review. CARE facilitates the data collection and study of inline commentary in NLP, extrinsic evaluation of NLP assistance, and application prototyping. We invite the community to explore and build upon the open source implementation of CARE.',\n",
              " 'We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation.',\n",
              " 'Emotion-cause pair extraction (ECPE) task aims to extract all the pairs of emotions and their causes from an unannotated emotion text. The previous works usually extract the emotion-cause pairs from two perspectives of emotion and cause. However, emotion extraction is more crucial to the ECPE task than cause extraction. Motivated by this analysis, we propose an end-to-end emotion-cause extraction approach oriented toward emotion prediction (EPO-ECPE), aiming to fully exploit the potential of emotion prediction to enhance emotion-cause pair extraction. Considering the strong dependence between emotion prediction and emotion-cause pair extraction, we propose a synchronization mechanism to share their improvement in the training process. That is, the improvement of emotion prediction can facilitate the emotion-cause pair extraction, and then the results of emotion-cause pair extraction can also be used to improve the accuracy of emotion prediction simultaneously. For the emotion-cause pair extraction, we divide it into genuine pair supervision and fake pair supervision, where the genuine pair supervision learns from the pairs with more possibility to be emotion-cause pairs. In contrast, fake pair supervision learns from other pairs. In this way, the emotion-cause pairs can be extracted directly from the genuine pair, thereby reducing the difficulty of extraction. Experimental results show that our approach outperforms the 13 compared systems and achieves new state-of-the-art performance.',\n",
              " \"Advances in computational methods and big data availability have recently translated into breakthroughs in AI applications. With successes in bottom-up challenges partially overshadowing shortcomings, the 'human-like' performance of Large Language Models has raised the question of how linguistic performance is achieved by algorithms. Given systematic shortcomings in generalization across many AI systems, in this work we ask whether linguistic performance is indeed guided by language knowledge in Large Language Models. To this end, we prompt GPT-3 with a grammaticality judgement task and comprehension questions on less frequent constructions that are thus unlikely to form part of Large Language Models' training data. These included grammatical 'illusions', semantic anomalies, complex nested hierarchies and self-embeddings. GPT-3 failed for every prompt but one, often offering answers that show a critical lack of understanding even of high-frequency words used in these less frequent grammatical constructions. The present work sheds light on the boundaries of the alleged AI human-like linguistic competence and argues that, far from human-like, the next-word prediction abilities of LLMs may face issues of robustness, when pushed beyond training data.\",\n",
              " 'Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown that off-loading memory from trainable weights to a retrieval database can significantly improve language modeling and match the performance of non-retrieval models that are an order of magnitude larger in size. It has been suggested that at least some of this performance gain is due to non-trivial generalization based on both model weights and retrieval. In this paper, we try to better understand the relative contributions of these two components. We find that the performance gains from retrieval largely originate from overlapping tokens between the database and the test data, suggesting less non-trivial generalization than previously assumed. More generally, our results point to the challenges of evaluating the generalization of retrieval-augmented language models such as RETRO, as even limited token overlap may significantly decrease test-time loss. We release our code and model at https://github.com/TobiasNorlund/retro',\n",
              " 'Pre-trained Transformer models such as BERT have shown great success in a wide range of applications, but at the cost of substantial increases in model complexity. Quantization-aware training (QAT) is a promising method to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation due to unstable convergence, especially when the downstream dataset is not abundant. This work proposes a proactive knowledge distillation method called Teacher Intervention (TI) for fast converging QAT of ultra-low precision pre-trained Transformers. TI intervenes layer-wise signal propagation with the intact signal from the teacher to remove the interference of propagated quantization errors, smoothing loss surface of QAT and expediting the convergence. Furthermore, we propose a gradual intervention mechanism to stabilize the recovery of subsections of Transformer layers from quantization. The proposed schemes enable fast convergence of QAT and improve the model accuracy regardless of the diverse characteristics of downstream fine-tuning tasks. We demonstrate that TI consistently achieves superior accuracy with significantly lower fine-tuning iterations on well-known Transformers of natural language processing as well as computer vision compared to the state-of-the-art QAT methods.',\n",
              " \"Knowledge-aware question answering (KAQA) requires the model to answer questions over a knowledge base, which is essential for both open-domain QA and domain-specific QA, especially when language models alone cannot provide all the knowledge needed. Despite the promising result of recent KAQA systems which tend to integrate linguistic knowledge from pre-trained language models (PLM) and factual knowledge from knowledge graphs (KG) to answer complex questions, a bottleneck exists in effectively fusing the representations from PLMs and KGs because of (i) the semantic and distributional gaps between them, and (ii) the difficulties in joint reasoning over the provided knowledge from both modalities. To address the above two problems, we propose a Fine-grained Two-stage training framework (FiTs) to boost the KAQA system performance: The first stage aims at aligning representations from the PLM and the KG, thus bridging the modality gaps between them, named knowledge adaptive post-training. The second stage, called knowledge-aware fine-tuning, aims to improve the model's joint reasoning ability based on the aligned representations. In detail, we fine-tune the post-trained model via two auxiliary self-supervised tasks in addition to the QA supervision. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.\",\n",
              " 'Visual Question Answering (VQA) is a challenging task of natural language processing (NLP) and computer vision (CV), attracting significant attention from researchers. English is a resource-rich language that has witnessed various developments in datasets and models for visual question answering. Visual question answering in other languages also would be developed for resources and models. In addition, there is no multilingual dataset targeting the visual content of a particular country with its own objects and cultural characteristics. To address the weakness, we provide the research community with a benchmark dataset named EVJVQA, including 33,000+ pairs of question-answer over three languages: Vietnamese, English, and Japanese, on approximately 5,000 images taken from Vietnam for evaluating multilingual VQA systems or models. EVJVQA is used as a benchmark dataset for the challenge of multilingual visual question answering at the 9th Workshop on Vietnamese Language and Speech Processing (VLSP 2022). This task attracted 62 participant teams from various universities and organizations. In this article, we present details of the organization of the challenge, an overview of the methods employed by shared-task participants, and the results. The highest performances are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained vision model and mT5 for the pre-trained language model, a powerful pre-trained language model based on the transformer architecture. EVJVQA is a challenging dataset that motivates NLP and CV researchers to further explore the multilingual models or systems for visual question answering systems.',\n",
              " 'To protect user privacy and meet legal regulations, federated learning (FL) is attracting significant attention. Training neural machine translation (NMT) models with traditional FL algorithm (e.g., FedAvg) typically relies on multi-round model-based interactions. However, it is impractical and inefficient for machine translation tasks due to the vast communication overheads and heavy synchronization. In this paper, we propose a novel federated nearest neighbor (FedNN) machine translation framework that, instead of multi-round model-based interactions, leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems. The whole approach equips the public NMT model trained on large-scale accessible data with a $k$-nearest-neighbor ($$kNN) classifier and integrates the external datastore constructed by private text data in all clients to form the final FL model. A two-phase datastore encryption strategy is introduced to achieve privacy-preserving during this process. Extensive experiments show that FedNN significantly reduces computational and communication costs compared with FedAvg, while maintaining promising performance in different FL settings.',\n",
              " '$k$NN-MT is a straightforward yet powerful approach for fast domain adaptation, which directly plugs pre-trained neural machine translation (NMT) models with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, $k$NN-MT is burdened with massive storage requirements and high computational complexity since it conducts nearest neighbor searches over the entire reference corpus. In this paper, we propose a simple and scalable nearest neighbor machine translation framework to drastically promote the decoding and storage efficiency of $k$NN-based models while maintaining the translation performance. To this end, we dynamically construct an extremely small datastore for each input via sentence-level retrieval to avoid searching the entire datastore in vanilla $k$NN-MT, based on which we further introduce a distance-aware adapter to adaptively incorporate the $k$NN retrieval results into the pre-trained NMT models. Experiments on machine translation in two general settings, static domain adaptation and online learning, demonstrate that our proposed approach not only achieves almost 90% speed as the NMT model without performance degradation, but also significantly reduces the storage requirements of $k$NN-MT.',\n",
              " \"Parsing spoken dialogue presents challenges that parsing text does not, including a lack of clear sentence boundaries. We know from previous work that prosody helps in parsing single sentences (Tran et al. 2018), but we want to show the effect of prosody on parsing speech that isn't segmented into sentences. In experiments on the English Switchboard corpus, we find prosody helps our model both with parsing and with accurately identifying sentence boundaries. However, we find that the best-performing parser is not necessarily the parser that produces the best sentence segmentation performance. We suggest that the best parses instead come from modelling sentence boundaries jointly with other constituent boundaries.\",\n",
              " 'The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierarchical attention networks (HAN) to learn the relationships among words and sentences in three different levels and (2) knowledge encoding (KE) to incorporate external knowledge for real-world entities into the process of political stance prediction. Also, to take into account the subtle and important difference between opposite political stances, we build two independent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by ourselves and learn to fuse the different political knowledge. Through extensive evaluations on three real-world datasets, we demonstrate the superiority of DASH in terms of (1) accuracy, (2) efficiency, and (3) effectiveness.',\n",
              " 'This paper describes SPINDLE - an open source Python module implementing an efficient and accurate parser for written Dutch that transforms raw text input to programs for meaning composition, expressed as {\\\\lambda} terms. The parser integrates a number of breakthrough advances made in recent years. Its output consists of hi-res derivations of a multimodal type-logical grammar, capturing two orthogonal axes of syntax, namely deep function-argument structures and dependency relations. These are produced by three interdependent systems: a static type-checker asserting the well-formedness of grammatical analyses, a state-of-the-art, structurally-aware supertagger based on heterogeneous graph convolutions, and a massively parallel proof search component based on Sinkhorn iterations. Packed in the software are also handy utilities and extras for proof visualization and inference, intended to facilitate end-user utilization.',\n",
              " \"Multi-document grounded dialogue systems (DGDS) belong to a class of conversational agents that answer users' requests by finding supporting knowledge from a collection of documents. Most previous studies aim to improve the knowledge retrieval model or propose more effective ways to incorporate external knowledge into a parametric generation model. These methods, however, focus on retrieving knowledge from mono-granularity language units (e.g. passages, sentences, or spans in documents), which is not enough to effectively and efficiently capture precise knowledge in long documents. This paper proposes Re3G, which aims to optimize both coarse-grained knowledge retrieval and fine-grained knowledge extraction in a unified framework. Specifically, the former efficiently finds relevant passages in a retrieval-and-reranking process, whereas the latter effectively extracts finer-grain spans within those passages to incorporate into a parametric answer generation model (BART, T5). Experiments on DialDoc Shared Task demonstrate the effectiveness of our method.\",\n",
              " 'Empathetic dialogue is a human-like behavior that requires the perception of both affective factors (e.g., emotion status) and cognitive factors (e.g., cause of the emotion). Besides concerning emotion status in early work, the latest approaches study emotion causes in empathetic dialogue. These approaches focus on understanding and duplicating emotion causes in the context to show empathy for the speaker. However, instead of only repeating the contextual causes, the real empathic response often demonstrate a logical and emotion-centered transition from the causes in the context to those in the responses. In this work, we propose an emotion cause transition graph to explicitly model the natural transition of emotion causes between two adjacent turns in empathetic dialogue. With this graph, the concept words of the emotion causes in the next turn can be predicted and used by a specifically designed concept-aware decoder to generate the empathic response. Automatic and human experimental results on the benchmark dataset demonstrate that our method produces more empathetic, coherent, informative, and specific responses than existing models.',\n",
              " 'We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting. Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed filtering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following. So far, these attacks assumed that the adversary is directly prompting the LLM.   In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented specific demonstrations of the proposed attacks within synthetic applications. In summary, our work calls for an urgent evaluation of current mitigation techniques and an investigation of whether new techniques are needed to defend LLMs against these threats.',\n",
              " 'A large amount of feedback was collected over the years. Many feedback analysis models have been developed focusing on the English language. Recognizing the concept of feedback is challenging and crucial in languages which do not have applicable corpus and tools employed in Natural Language Processing (i.e., vocabulary corpus, sentence structure rules, etc). However, in this paper, we study a feedback classification in Mongolian language using two different word embeddings for deep learning. We compare the results of proposed approaches. We use feedback data in Cyrillic collected from 2012-2018. The result indicates that word embeddings using their own dataset improve the deep learning based proposed model with the best accuracy of 80.1% and 82.7% for two classification tasks.',\n",
              " 'ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance when facing unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT does not show consistent advantages on adversarial and OOD classification tasks, while performing well on translation tasks. This suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.',\n",
              " 'Despite the dramatic success in image generation, Generative Adversarial Networks (GANs) still face great challenges in synthesizing sequences of discrete elements, in particular human language. The difficulty in generator training arises from the limited representation capacity and uninformative learning signals obtained from the discriminator. In this work, we (1) first empirically show that the mixture-of-experts approach is able to enhance the representation capacity of the generator for language GANs and (2) harness the Feature Statistics Alignment (FSA) paradigm to render fine-grained learning signals to advance the generator training. Specifically, FSA forces the mean statistics of the distribution of fake data to approach that of real samples as close as possible in the finite-dimensional feature space. Empirical study on synthetic and real benchmarks shows the superior performance in quantitative evaluation and demonstrates the effectiveness of our approach to adversarial text generation.',\n",
              " 'We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when trained on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.',\n",
              " 'The COVID-19 pandemic has caused substantial damage to global health. Even though three years have passed, the world continues to struggle with the virus. Concerns are growing about the impact of COVID-19 on the mental health of infected individuals, who are more likely to experience depression, which can have long-lasting consequences for both the affected individuals and the world. Detection and intervention at an early stage can reduce the risk of depression in COVID-19 patients. In this paper, we investigated the relationship between COVID-19 infection and depression through social media analysis. Firstly, we managed a dataset of COVID-19 patients that contains information about their social media activity both before and after infection. Secondly,We conducted an extensive analysis of this dataset to investigate the characteristic of COVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep neural network for early prediction of depression risk. This model considers daily mood swings as a psychiatric signal and incorporates textual and emotional characteristics via knowledge distillation. Experimental results demonstrate that our proposed framework outperforms baselines in detecting depression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has the potential to enable public health organizations to initiate prompt intervention with high-risk patients',\n",
              " 'Named Entity Recognition (NER) models capable of Continual Learning (CL) are realistically valuable in areas where entity types continuously increase (e.g., personal assistants). Meanwhile the learning paradigm of NER advances to new patterns such as the span-based methods. However, its potential to CL has not been fully explored. In this paper, we propose SpanKL1, a simple yet effective Span-based model with Knowledge distillation (KD) to preserve memories and multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence labeling approaches, the inherently independent modeling in span and entity level with the designed coherent optimization on SpanKL promotes its learning at each incremental step and mitigates the forgetting. Experiments on synthetic CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly outperforms previous SoTA in many aspects, and obtains the smallest gap from CL to the upper bound revealing its high practiced value.',\n",
              " \"The increasing reliability of automatic speech recognition has proliferated its everyday use. However, for research purposes, it is often unclear which model one should choose for a task, particularly if there is a requirement for speed as well as accuracy. In this paper, we systematically evaluate six speech recognizers using metrics including word error rate, latency, and the number of updates to already recognized words on English test data, as well as propose and compare two methods for streaming audio into recognizers for incremental recognition. We further propose Revokes per Second as a new metric for evaluating incremental recognition and demonstrate that it provides insights into overall model performance. We find that, generally, local recognizers are faster and require fewer updates than cloud-based recognizers. Finally, we find Meta's Wav2Vec model to be the fastest, and find Mozilla's DeepSpeech model to be the most stable in its predictions.\",\n",
              " 'Sentiment transfer aims at revising the input text to satisfy a given sentiment polarity while retaining the original semantic content. The nucleus of sentiment transfer lies in precisely separating the sentiment information from the content information. Existing explicit approaches generally identify and mask sentiment tokens simply based on prior linguistic knowledge and manually-defined rules, leading to low generality and undesirable transfer performance. In this paper, we view the positions to be masked as the learnable parameters, and further propose a novel AM-ST model to learn adaptive task-relevant masks based on the attention mechanism. Moreover, a sentiment-aware masked language model is further proposed to fill in the blanks in the masked positions by incorporating both context and sentiment polarity to capture the multi-grained semantics comprehensively. AM-ST is thoroughly evaluated on two popular datasets, and the experimental results demonstrate the superiority of our proposal.',\n",
              " \"The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-cot.\",\n",
              " 'Neural networks drive the success of natural language processing. A fundamental property of natural languages is their compositional structure, allowing us to describe new meanings systematically. However, neural networks notoriously struggle with systematic generalization and do not necessarily benefit from a compositional structure in emergent communication simulations. Here, we test how neural networks compare to humans in learning and generalizing a new language. We do this by closely replicating an artificial language learning study (conducted originally with human participants) and evaluating the memorization and generalization capabilities of deep neural networks with respect to the degree of structure in the input language. Our results show striking similarities between humans and deep neural networks: More structured linguistic input leads to more systematic generalization and better convergence between humans and neural network agents and between different neural agents. We then replicate this structure bias found in humans and our recurrent neural networks with a Transformer-based large language model (GPT-3), showing a similar benefit for structured linguistic input regarding generalization systematicity and memorization errors. These findings show that the underlying structure of languages is crucial for systematic generalization. Due to the correlation between community size and linguistic structure in natural languages, our findings underscore the challenge of automated processing of low-resource languages. Nevertheless, the similarity between humans and machines opens new avenues for language evolution research.',\n",
              " 'Using online information discovery as a case study, in this position paper we discuss the need to design, develop, and deploy (conversational) agents that can -- non-intrusively -- guide children in their quest for online resources rather than simply finding resources for them. We argue that agents should \"let children learn\" and should be built to take on a teacher-facilitator function, allowing children to develop their technical and critical thinking abilities as they interact with varied technology in a broad range of use cases.',\n",
              " 'Deep neural network based speech enhancement technique focuses on learning a noisy-to-clean transformation supervised by paired training data. However, the task-specific evaluation metric (e.g., PESQ) is usually non-differentiable and can not be directly constructed in the training criteria. This mismatch between the training objective and evaluation metric likely results in sub-optimal performance. To alleviate it, we propose a metric-oriented speech enhancement method (MOSE), which leverages the recent advances in the diffusion probabilistic model and integrates a metric-oriented training strategy into its reverse process. Specifically, we design an actor-critic based framework that considers the evaluation metric as a posterior reward, thus guiding the reverse process to the metric-increasing direction. The experimental results demonstrate that MOSE obviously benefits from metric-oriented training and surpasses the generative baselines in terms of all evaluation metrics.',\n",
              " 'Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a \"memorize-then-abstract\" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales.',\n",
              " 'Sentence Simplification aims to rephrase complex sentences into simpler sentences while retaining original meaning. Large Language models (LLMs) have demonstrated the ability to perform a variety of natural language processing tasks. However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system. In this work, we empirically analyze the zero-/few-shot learning ability of LLMs by evaluating them on a number of benchmark test sets. Experimental results show LLMs outperform state-of-the-art sentence simplification methods, and are judged to be on a par with human annotators.',\n",
              " 'Large language models have demonstrated an emergent capability in answering knowledge intensive questions. With recent progress on web-scale visual and language pre-training, do these models also understand how to answer visual information seeking questions? To answer this question, we present InfoSeek, a Visual Question Answering dataset that focuses on asking information-seeking questions, where the information can not be answered by common sense knowledge. We perform a multi-stage human annotation to collect a natural distribution of high-quality visual information seeking question-answer pairs. We also construct a large-scale, automatically collected dataset by combining existing visual entity recognition datasets and Wikidata, which provides over one million examples for model fine-tuning and validation. Based on InfoSeek, we analyzed various pre-trained Visual QA systems to gain insights into the characteristics of different pre-trained models. Our analysis shows that it is challenging for the state-of-the-art multi-modal pre-trained models to answer visual information seeking questions, but this capability is improved through fine-tuning on the automated InfoSeek dataset. We hope our analysis paves the way to understand and develop the next generation of multi-modal pre-training.',\n",
              " 'The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.',\n",
              " 'The widespread availability of internet access and handheld devices confers to social media a power similar to the one newspapers used to have. People seek affordable information on social media and can reach it within seconds. Yet this convenience comes with dangers; any user may freely post whatever they please and the content can stay online for a long period, regardless of its truthfulness. A need to detect untruthful information, also known as fake news, arises. In this paper, we present an end-to-end solution that accurately detects fake news and immunizes network nodes that spread them in real-time. To detect fake news, we propose two new stack deep learning architectures that utilize convolutional and bidirectional LSTM layers. To mitigate the spread of fake news, we propose a real-time network-aware strategy that (1) constructs a minimum-cost weighted directed spanning tree for a detected node, and (2) immunizes nodes in that tree by scoring their harmfulness using a novel ranking function. We demonstrate the effectiveness of our solution on five real-world datasets.',\n",
              " 'Current captioning datasets, focus on object-centric captions, describing the visible objects in the image, often ending up stating the obvious (for humans), e.g. \"people eating food in a park\". Although these datasets are useful to evaluate the ability of Vision & Language models to recognize the visual content, they lack in expressing trivial abstract concepts, e.g. \"people having a picnic\". Such concepts are licensed by human\\'s personal experience and contribute to forming common sense assumptions. We present the High-Level Dataset; a dataset extending 14997 images of the COCO dataset with 134973 human-annotated (high-level) abstract captions collected along three axes: scenes, actions and rationales. We describe and release such dataset and we show how it can be used to assess models\\' multimodal grounding of abstract concepts and enrich models\\' visio-lingusitic representations. Moreover, we describe potential tasks enabled by this dataset involving high- and low-level concepts interactions.',\n",
              " 'In this paper, we summarize the current state of the field of NLP & Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP & Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.',\n",
              " 'The multi-sentential long sequence textual data unfolds several interesting research directions pertaining to natural language processing and generation. Though we observe several high-quality long-sequence datasets for English and other monolingual languages, there is no significant effort in building such resources for code-mixed languages such as Hinglish (code-mixing of Hindi-English). In this paper, we propose a novel task of identifying multi-sentential code-mixed text (MCT) from multilingual articles. As a use case, we leverage multilingual articles from two different data sources and build a first-of-its-kind multi-sentential code-mixed Hinglish dataset i.e., MUTANT. We propose a token-level language-aware pipeline and extend the existing metrics measuring the degree of code-mixing to a multi-sentential framework and automatically identify MCT in the multilingual articles. The MUTANT dataset comprises 67k articles with 85k identified Hinglish MCTs. To facilitate future research, we make the publicly available.',\n",
              " 'Natural language provides a powerful modality to program robots to perform temporal tasks. Linear temporal logic (LTL) provides unambiguous semantics for formal descriptions of temporal tasks. However, existing approaches cannot accurately and robustly translate English sentences to their equivalent LTL formulas in unseen environments. To address this problem, we propose Lang2LTL, a novel modular system that leverages pretrained large language models to first extract referring expressions from a natural language command, then ground the expressions to real-world landmarks and objects, and finally translate the command into an LTL task specification for the robot. It enables any robotic system to interpret natural language navigation commands without additional training, provided that it tracks its position and has a semantic map with landmarks labeled with free-form text. We demonstrate the state-of-the-art ability to generalize to multi-scale navigation domains such as OpenStreetMap (OSM) and CleanUp World (a simulated household environment). Lang2LTL achieves an average accuracy of 88.4% in translating challenging LTL formulas in 22 unseen OSM environments as evaluated on a new corpus of over 10,000 commands, 22 times better than the previous SoTA. Without modification, the best performing Lang2LTL model on the OSM dataset can translate commands in CleanUp World with 82.8% accuracy. As a part of our proposed comprehensive evaluation procedures, we collected a new labeled dataset of English commands representing 2,125 unique LTL formulas, the largest ever dataset of natural language commands to LTL specifications for robotic tasks with the most diverse LTL formulas, 40 times more than previous largest dataset. Finally, we integrated Lang2LTL with a planner to command a quadruped mobile robot to perform multi-step navigational tasks in an analog real-world environment created in the lab.',\n",
              " 'We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when trained on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.',\n",
              " \"The increasing reliability of automatic speech recognition has proliferated its everyday use. However, for research purposes, it is often unclear which model one should choose for a task, particularly if there is a requirement for speed as well as accuracy. In this paper, we systematically evaluate six speech recognizers using metrics including word error rate, latency, and the number of updates to already recognized words on English test data, as well as propose and compare two methods for streaming audio into recognizers for incremental recognition. We further propose Revokes per Second as a new metric for evaluating incremental recognition and demonstrate that it provides insights into overall model performance. We find that, generally, local recognizers are faster and require fewer updates than cloud-based recognizers. Finally, we find Meta's Wav2Vec model to be the fastest, and find Mozilla's DeepSpeech model to be the most stable in its predictions.\",\n",
              " 'Deep neural network based speech enhancement technique focuses on learning a noisy-to-clean transformation supervised by paired training data. However, the task-specific evaluation metric (e.g., PESQ) is usually non-differentiable and can not be directly constructed in the training criteria. This mismatch between the training objective and evaluation metric likely results in sub-optimal performance. To alleviate it, we propose a metric-oriented speech enhancement method (MOSE), which leverages the recent advances in the diffusion probabilistic model and integrates a metric-oriented training strategy into its reverse process. Specifically, we design an actor-critic based framework that considers the evaluation metric as a posterior reward, thus guiding the reverse process to the metric-increasing direction. The experimental results demonstrate that MOSE obviously benefits from metric-oriented training and surpasses the generative baselines in terms of all evaluation metrics.',\n",
              " 'Deep neural network based speech enhancement approaches aim to learn a noisy-to-clean transformation using a supervised learning paradigm. However, such a trained-well transformation is vulnerable to unseen noises that are not included in training set. In this work, we focus on the unsupervised noise adaptation problem in speech enhancement, where the ground truth of target domain data is completely unavailable. Specifically, we propose a generative adversarial network based method to efficiently learn a converse clean-to-noisy transformation using a few minutes of unpaired target domain data. Then this transformation is utilized to generate sufficient simulated data for domain adaptation of the enhancement model. Experimental results show that our method effectively mitigates the domain mismatch between training and test sets, and surpasses the best baseline by a large margin.',\n",
              " 'Monaural speech enhancement has been widely studied using real networks in the time-frequency (TF) domain. However, the input and the target are naturally complex-valued in the TF domain, a fully complex network is highly desirable for effectively learning the feature representation and modelling the sequence in the complex domain. Moreover, phase, an important factor for perceptual quality of speech, has been proved learnable together with magnitude from noisy speech using complex masking or complex spectral mapping. Many recent studies focus on either complex masking or complex spectral mapping, ignoring their performance boundaries. To address above issues, we propose a fully complex dual-path dual-decoder conformer network (D2Former) using joint complex masking and complex spectral mapping for monaural speech enhancement. In D2Former, we extend the conformer network into the complex domain and form a dual-path complex TF self-attention architecture for effectively modelling the complex-valued TF sequence. We further boost the TF feature representation in the encoder and the decoders using a dual-path learning structure by exploiting complex dilated convolutions on time dependency and complex feedforward sequential memory networks (CFSMN) for frequency recurrence. In addition, we improve the performance boundaries of complex masking and complex spectral mapping by combining the strengths of the two training targets into a joint-learning framework. As a consequence, D2Former takes fully advantages of the complex-valued operations, the dual-path processing, and the joint-training targets. Compared to the previous models, D2Former achieves state-of-the-art results on the VoiceBank+Demand benchmark with the smallest model size of 0.87M parameters.',\n",
              " 'Transformer based models have provided significant performance improvements in monaural speech separation. However, there is still a performance gap compared to a recent proposed upper bound. The major limitation of the current dual-path Transformer models is the inefficient modelling of long-range elemental interactions and local feature patterns. In this work, we achieve the upper bound by proposing a gated single-head transformer architecture with convolution-augmented joint self-attentions, named \\\\textit{MossFormer} (\\\\textit{Mo}naural \\\\textit{s}peech \\\\textit{s}eparation Trans\\\\textit{Former}). To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence. The joint attention enables MossFormer model full-sequence elemental interaction directly. In addition, we employ a powerful attentive gating mechanism with simplified single-head self-attentions. Besides the attentive long-range modelling, we also augment MossFormer with convolutions for the position-wise local pattern modelling. As a consequence, MossFormer significantly outperforms the previous models and achieves the state-of-the-art results on WSJ0-2/3mix and WHAM!/WHAMR! benchmarks. Our model achieves the SI-SDRi upper bound of 21.2 dB on WSJ0-3mix and only 0.3 dB below the upper bound of 23.1 dB on WSJ0-2mix.',\n",
              " 'In this work, we propose a frequency bin-wise method to estimate the single-channel speech presence probability (SPP) with multiple deep neural networks (DNNs) in the short-time Fourier transform domain. Since all frequency bins are typically considered simultaneously as input features for conventional DNN-based SPP estimators, high model complexity is inevitable. To reduce the model complexity and the requirements on the training data, we take a single frequency bin and some of its neighboring frequency bins into account to train separate gate recurrent units. In addition, the noisy speech and the a posteriori probability SPP representation are used to train our model. The experiments were performed on the Deep Noise Suppression challenge dataset. The experimental results show that the speech detection accuracy can be improved when we employ the frequency bin-wise model. Finally, we also demonstrate that our proposed method outperforms most of the state-of-the-art SPP estimation methods in terms of speech detection accuracy and model complexity.',\n",
              " 'In this study, we present an approach to train a single speech enhancement network that can perform both personalized and non-personalized speech enhancement. This is achieved by incorporating a frame-wise conditioning input that specifies the type of enhancement output. To improve the quality of the enhanced output and mitigate oversuppression, we experiment with re-weighting frames by the presence or absence of speech activity and applying augmentations to speaker embeddings. By training under a multi-task learning setting, we empirically show that the proposed unified model obtains promising results on both personalized and non-personalized speech enhancement benchmarks and reaches similar performance to models that are trained specialized for either task. The strong performance of the proposed method demonstrates that the unified model is a more economical alternative compared to keeping separate task-specific models during inference.',\n",
              " 'Speech utterances recorded under differing conditions exhibit varying degrees of confidence in their embedding estimates, i.e., uncertainty, even if they are extracted using the same neural network. This paper aims to incorporate the uncertainty estimate produced in the xi-vector network front-end with a probabilistic linear discriminant analysis (PLDA) back-end scoring for speaker verification. To achieve this we derive a posterior covariance matrix, which measures the uncertainty, from the frame-wise precisions to the embedding space. We propose a log-likelihood ratio function for the PLDA scoring with the uncertainty propagation. We also propose to replace the length normalization pre-processing technique with a length scaling technique for the application of uncertainty propagation in the back-end. Experimental results on the VoxCeleb-1, SITW test sets as well as a domain-mismatched CNCeleb1-E set show the effectiveness of the proposed techniques with 14.5%-41.3% EER reductions and 4.6%-25.3% minDCF reductions.',\n",
              " 'In this work, we propose a frequency bin-wise method to estimate the single-channel speech presence probability (SPP) with multiple deep neural networks (DNNs) in the short-time Fourier transform domain. Since all frequency bins are typically considered simultaneously as input features for conventional DNN-based SPP estimators, high model complexity is inevitable. To reduce the model complexity and the requirements on the training data, we take a single frequency bin and some of its neighboring frequency bins into account to train separate gate recurrent units. In addition, the noisy speech and the a posteriori probability SPP representation are used to train our model. The experiments were performed on the Deep Noise Suppression challenge dataset. The experimental results show that the speech detection accuracy can be improved when we employ the frequency bin-wise model. Finally, we also demonstrate that our proposed method outperforms most of the state-of-the-art SPP estimation methods in terms of speech detection accuracy and model complexity.',\n",
              " 'In this study, we present an approach to train a single speech enhancement network that can perform both personalized and non-personalized speech enhancement. This is achieved by incorporating a frame-wise conditioning input that specifies the type of enhancement output. To improve the quality of the enhanced output and mitigate oversuppression, we experiment with re-weighting frames by the presence or absence of speech activity and applying augmentations to speaker embeddings. By training under a multi-task learning setting, we empirically show that the proposed unified model obtains promising results on both personalized and non-personalized speech enhancement benchmarks and reaches similar performance to models that are trained specialized for either task. The strong performance of the proposed method demonstrates that the unified model is a more economical alternative compared to keeping separate task-specific models during inference.',\n",
              " 'Speech utterances recorded under differing conditions exhibit varying degrees of confidence in their embedding estimates, i.e., uncertainty, even if they are extracted using the same neural network. This paper aims to incorporate the uncertainty estimate produced in the xi-vector network front-end with a probabilistic linear discriminant analysis (PLDA) back-end scoring for speaker verification. To achieve this we derive a posterior covariance matrix, which measures the uncertainty, from the frame-wise precisions to the embedding space. We propose a log-likelihood ratio function for the PLDA scoring with the uncertainty propagation. We also propose to replace the length normalization pre-processing technique with a length scaling technique for the application of uncertainty propagation in the back-end. Experimental results on the VoxCeleb-1, SITW test sets as well as a domain-mismatched CNCeleb1-E set show the effectiveness of the proposed techniques with 14.5%-41.3% EER reductions and 4.6%-25.3% minDCF reductions.',\n",
              " 'We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when trained on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.',\n",
              " \"The increasing reliability of automatic speech recognition has proliferated its everyday use. However, for research purposes, it is often unclear which model one should choose for a task, particularly if there is a requirement for speed as well as accuracy. In this paper, we systematically evaluate six speech recognizers using metrics including word error rate, latency, and the number of updates to already recognized words on English test data, as well as propose and compare two methods for streaming audio into recognizers for incremental recognition. We further propose Revokes per Second as a new metric for evaluating incremental recognition and demonstrate that it provides insights into overall model performance. We find that, generally, local recognizers are faster and require fewer updates than cloud-based recognizers. Finally, we find Meta's Wav2Vec model to be the fastest, and find Mozilla's DeepSpeech model to be the most stable in its predictions.\",\n",
              " 'Deep neural network based speech enhancement technique focuses on learning a noisy-to-clean transformation supervised by paired training data. However, the task-specific evaluation metric (e.g., PESQ) is usually non-differentiable and can not be directly constructed in the training criteria. This mismatch between the training objective and evaluation metric likely results in sub-optimal performance. To alleviate it, we propose a metric-oriented speech enhancement method (MOSE), which leverages the recent advances in the diffusion probabilistic model and integrates a metric-oriented training strategy into its reverse process. Specifically, we design an actor-critic based framework that considers the evaluation metric as a posterior reward, thus guiding the reverse process to the metric-increasing direction. The experimental results demonstrate that MOSE obviously benefits from metric-oriented training and surpasses the generative baselines in terms of all evaluation metrics.',\n",
              " 'Deep neural network based speech enhancement approaches aim to learn a noisy-to-clean transformation using a supervised learning paradigm. However, such a trained-well transformation is vulnerable to unseen noises that are not included in training set. In this work, we focus on the unsupervised noise adaptation problem in speech enhancement, where the ground truth of target domain data is completely unavailable. Specifically, we propose a generative adversarial network based method to efficiently learn a converse clean-to-noisy transformation using a few minutes of unpaired target domain data. Then this transformation is utilized to generate sufficient simulated data for domain adaptation of the enhancement model. Experimental results show that our method effectively mitigates the domain mismatch between training and test sets, and surpasses the best baseline by a large margin.',\n",
              " 'Monaural speech enhancement has been widely studied using real networks in the time-frequency (TF) domain. However, the input and the target are naturally complex-valued in the TF domain, a fully complex network is highly desirable for effectively learning the feature representation and modelling the sequence in the complex domain. Moreover, phase, an important factor for perceptual quality of speech, has been proved learnable together with magnitude from noisy speech using complex masking or complex spectral mapping. Many recent studies focus on either complex masking or complex spectral mapping, ignoring their performance boundaries. To address above issues, we propose a fully complex dual-path dual-decoder conformer network (D2Former) using joint complex masking and complex spectral mapping for monaural speech enhancement. In D2Former, we extend the conformer network into the complex domain and form a dual-path complex TF self-attention architecture for effectively modelling the complex-valued TF sequence. We further boost the TF feature representation in the encoder and the decoders using a dual-path learning structure by exploiting complex dilated convolutions on time dependency and complex feedforward sequential memory networks (CFSMN) for frequency recurrence. In addition, we improve the performance boundaries of complex masking and complex spectral mapping by combining the strengths of the two training targets into a joint-learning framework. As a consequence, D2Former takes fully advantages of the complex-valued operations, the dual-path processing, and the joint-training targets. Compared to the previous models, D2Former achieves state-of-the-art results on the VoiceBank+Demand benchmark with the smallest model size of 0.87M parameters.',\n",
              " 'Transformer based models have provided significant performance improvements in monaural speech separation. However, there is still a performance gap compared to a recent proposed upper bound. The major limitation of the current dual-path Transformer models is the inefficient modelling of long-range elemental interactions and local feature patterns. In this work, we achieve the upper bound by proposing a gated single-head transformer architecture with convolution-augmented joint self-attentions, named \\\\textit{MossFormer} (\\\\textit{Mo}naural \\\\textit{s}peech \\\\textit{s}eparation Trans\\\\textit{Former}). To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence. The joint attention enables MossFormer model full-sequence elemental interaction directly. In addition, we employ a powerful attentive gating mechanism with simplified single-head self-attentions. Besides the attentive long-range modelling, we also augment MossFormer with convolutions for the position-wise local pattern modelling. As a consequence, MossFormer significantly outperforms the previous models and achieves the state-of-the-art results on WSJ0-2/3mix and WHAM!/WHAMR! benchmarks. Our model achieves the SI-SDRi upper bound of 21.2 dB on WSJ0-3mix and only 0.3 dB below the upper bound of 23.1 dB on WSJ0-2mix.',\n",
              " 'Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1.',\n",
              " 'We study multi-task learning for two orthogonal speech technology tasks: speech and speaker recognition. We use wav2vec2 as a base architecture with two task-specific output heads. We experiment with different methods to mix speaker and speech information in the output embedding sequence, and propose a simple dynamic approach to balance the speech and speaker recognition loss functions. Our multi-task learning networks can produce a shared speaker and speech embedding, which are evaluated on the LibriSpeech and VoxCeleb test sets, and achieve a performance comparable to separate single-task models. Code is available at https://github.com/nikvaessen/2022-repo-mt-w2v2.',\n",
              " 'Conventional methods for speaker diarization involve windowing an audio file into short segments to extract speaker embeddings, followed by an unsupervised clustering of the embeddings. This multi-step approach generates speaker assignments for each segment. In this paper, we propose a novel Supervised HierArchical gRaph Clustering algorithm (SHARC) for speaker diarization where we introduce a hierarchical structure using Graph Neural Network (GNN) to perform supervised clustering. The supervision allows the model to update the representations and directly improve the clustering performance, thus enabling a single-step approach for diarization. In the proposed work, the input segment embeddings are treated as nodes of a graph with the edge weights corresponding to the similarity scores between the nodes. We also propose an approach to jointly update the embedding extractor and the GNN model to perform end-to-end speaker diarization (E2E-SHARC). During inference, the hierarchical clustering is performed using node densities and edge existence probabilities to merge the segments until convergence. In the diarization experiments, we illustrate that the proposed E2E-SHARC approach achieves 53% and 44% relative improvements over the baseline systems on benchmark datasets like AMI and Voxconverse, respectively.',\n",
              " 'Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.',\n",
              " 'The recent progress in text-based audio retrieval was largely propelled by the release of suitable datasets. Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solution to create large-scale datasets. We study the recently proposed SoundDesc benchmark dataset, which was automatically sourced from the BBC Sound Effects web page. In our analysis, we find that SoundDesc contains several duplicates that cause leakage of training data to the evaluation data. This data leakage ultimately leads to overly optimistic retrieval performance estimates in previous benchmarks. We propose new training, validation, and testing splits for the dataset that we make available online. To avoid weak contamination of the test data, we pool audio files that share similar recording setups. In our experiments, we find that the new splits serve as a more challenging benchmark.',\n",
              " 'Distilled self-supervised models have shown competitive performance and efficiency in recent years. However, there is a lack of experience in jointly distilling multiple self-supervised speech models. In our work, we performed Ensemble Knowledge Distillation (EKD) on various self-supervised speech models such as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation techniques, layerwise-average and layerwise-concatenation, to the representations of different teacher models and found that the former was more effective. On top of that, we proposed a multiple prediction head method for student models to predict different layer outputs of multiple teacher models simultaneously. The experimental results show that our method improves the performance of the distilled models on four downstream speech processing tasks, Phoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic Speech Recognition in the hidden-set track of the SUPERB benchmark.',\n",
              " 'Previous pitch-controllable text-to-speech (TTS) models rely on directly modeling fundamental frequency, leading to low variance in synthesized speech. To address this issue, we propose PITS, an end-to-end pitch-controllable TTS model that utilizes variational inference to model pitch. Based on VITS, PITS incorporates the Yingram encoder, the Yingram decoder, and adversarial training of pitch-shifted synthesis to achieve pitch-controllability. Experiments demonstrate that PITS generates high-quality speech that is indistinguishable from ground truth speech and has high pitch-controllability without quality degradation. Code and audio samples will be available at https://github.com/anonymous-pits/pits.',\n",
              " 'This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model.',\n",
              " 'Distilled self-supervised models have shown competitive performance and efficiency in recent years. However, there is a lack of experience in jointly distilling multiple self-supervised speech models. In our work, we performed Ensemble Knowledge Distillation (EKD) on various self-supervised speech models such as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation techniques, layerwise-average and layerwise-concatenation, to the representations of different teacher models and found that the former was more effective. On top of that, we proposed a multiple prediction head method for student models to predict different layer outputs of multiple teacher models simultaneously. The experimental results show that our method improves the performance of the distilled models on four downstream speech processing tasks, Phoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic Speech Recognition in the hidden-set track of the SUPERB benchmark.',\n",
              " 'Previous pitch-controllable text-to-speech (TTS) models rely on directly modeling fundamental frequency, leading to low variance in synthesized speech. To address this issue, we propose PITS, an end-to-end pitch-controllable TTS model that utilizes variational inference to model pitch. Based on VITS, PITS incorporates the Yingram encoder, the Yingram decoder, and adversarial training of pitch-shifted synthesis to achieve pitch-controllability. Experiments demonstrate that PITS generates high-quality speech that is indistinguishable from ground truth speech and has high pitch-controllability without quality degradation. Code and audio samples will be available at https://github.com/anonymous-pits/pits.',\n",
              " 'This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model.',\n",
              " 'Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1.',\n",
              " 'We study multi-task learning for two orthogonal speech technology tasks: speech and speaker recognition. We use wav2vec2 as a base architecture with two task-specific output heads. We experiment with different methods to mix speaker and speech information in the output embedding sequence, and propose a simple dynamic approach to balance the speech and speaker recognition loss functions. Our multi-task learning networks can produce a shared speaker and speech embedding, which are evaluated on the LibriSpeech and VoxCeleb test sets, and achieve a performance comparable to separate single-task models. Code is available at https://github.com/nikvaessen/2022-repo-mt-w2v2.',\n",
              " 'Conventional methods for speaker diarization involve windowing an audio file into short segments to extract speaker embeddings, followed by an unsupervised clustering of the embeddings. This multi-step approach generates speaker assignments for each segment. In this paper, we propose a novel Supervised HierArchical gRaph Clustering algorithm (SHARC) for speaker diarization where we introduce a hierarchical structure using Graph Neural Network (GNN) to perform supervised clustering. The supervision allows the model to update the representations and directly improve the clustering performance, thus enabling a single-step approach for diarization. In the proposed work, the input segment embeddings are treated as nodes of a graph with the edge weights corresponding to the similarity scores between the nodes. We also propose an approach to jointly update the embedding extractor and the GNN model to perform end-to-end speaker diarization (E2E-SHARC). During inference, the hierarchical clustering is performed using node densities and edge existence probabilities to merge the segments until convergence. In the diarization experiments, we illustrate that the proposed E2E-SHARC approach achieves 53% and 44% relative improvements over the baseline systems on benchmark datasets like AMI and Voxconverse, respectively.',\n",
              " 'Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.',\n",
              " 'The recent progress in text-based audio retrieval was largely propelled by the release of suitable datasets. Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solution to create large-scale datasets. We study the recently proposed SoundDesc benchmark dataset, which was automatically sourced from the BBC Sound Effects web page. In our analysis, we find that SoundDesc contains several duplicates that cause leakage of training data to the evaluation data. This data leakage ultimately leads to overly optimistic retrieval performance estimates in previous benchmarks. We propose new training, validation, and testing splits for the dataset that we make available online. To avoid weak contamination of the test data, we pool audio files that share similar recording setups. In our experiments, we find that the new splits serve as a more challenging benchmark.',\n",
              " 'In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.',\n",
              " 'Existing deep learning based speech enhancement (SE) methods either use blind end-to-end training or explicitly incorporate speaker embedding or phonetic information into the SE network to enhance speech quality. In this paper, we perceive speech and noises as different types of sound events and propose an event-based query method for SE. Specifically, representative speech embeddings that can discriminate speech with noises are first pre-trained with the sound event detection (SED) task. The embeddings are then clustered into fixed golden speech queries to assist the SE network to enhance the speech from noisy audio. The golden speech queries can be obtained offline and generalizable to different SE datasets and networks. Therefore, little extra complexity is introduced and no enrollment is needed for each speaker. Experimental results show that the proposed method yields significant gains compared with baselines and the golden queries are well generalized to different datasets.',\n",
              " 'Stuttering is a neuro-developmental speech impairment characterized by uncontrolled utterances (interjections) and core behaviors (blocks, repetitions, and prolongations), and is caused by the failure of speech sensorimotors. Due to its complex nature, stuttering detection (SD) is a difficult task. If detected at an early stage, it could facilitate speech therapists to observe and rectify the speech patterns of persons who stutter (PWS). The stuttered speech of PWS is usually available in limited amounts and is highly imbalanced. To this end, we address the class imbalance problem in the SD domain via a multibranching (MB) scheme and by weighting the contribution of classes in the overall loss function, resulting in a huge improvement in stuttering classes on the SEP-28k dataset over the baseline (StutterNet). To tackle data scarcity, we investigate the effectiveness of data augmentation on top of a multi-branched training scheme. The augmented training outperforms the MB StutterNet (clean) by a relative margin of 4.18% in macro F1-score (F1). In addition, we propose a multi-contextual (MC) StutterNet, which exploits different contexts of the stuttered speech, resulting in an overall improvement of 4.48% in F 1 over the single context based MB StutterNet. Finally, we have shown that applying data augmentation in the cross-corpora scenario can improve the overall SD performance by a relative margin of 13.23% in F1 over the clean training.',\n",
              " 'Visual speech (i.e., lip motion) is highly related to auditory speech due to the co-occurrence and synchronization in speech production. This paper investigates this correlation and proposes a cross-modal speech co-learning paradigm. The primary motivation of our cross-modal co-learning method is modeling one modality aided by exploiting knowledge from another modality. Specifically, two cross-modal boosters are introduced based on an audio-visual pseudo-siamese structure to learn the modality-transformed correlation. Inside each booster, a max-feature-map embedded Transformer variant is proposed for modality alignment and enhanced feature generation. The network is co-learned both from scratch and with pretrained models. Experimental results on the LRSLip3, GridLip, LomGridLip, and VoxLip datasets demonstrate that our proposed method achieves 60% and 20% average relative performance improvement over independently trained audio-only/visual-only and baseline fusion systems, respectively.',\n",
              " 'End-to-end automatic speech recognition (ASR) usually suffers from performance degradation when applied to a new domain due to domain shift. Unsupervised domain adaptation (UDA) aims to improve the performance on the unlabeled target domain by transferring knowledge from the source to the target domain. To improve transferability, existing UDA approaches mainly focus on matching the distributions of the source and target domains globally and/or locally, while ignoring the model discriminability. In this paper, we propose a novel UDA approach for ASR via inter-domain MAtching and intra-domain DIscrimination (MADI), which improves the model transferability by fine-grained inter-domain matching and discriminability by intra-domain contrastive discrimination simultaneously. Evaluations on the Libri-Adapt dataset demonstrate the effectiveness of our approach. MADI reduces the relative word error rate (WER) on cross-device and cross-environment ASR by 17.7% and 22.8%, respectively.',\n",
              " 'We previously proposed contextual spelling correction (CSC) to correct the output of end-to-end (E2E) automatic speech recognition (ASR) models with contextual information such as name, place, etc. Although CSC has achieved reasonable improvement in the biasing problem, there are still two drawbacks for further accuracy improvement. First, due to information limitation in text only hypothesis or weak performance of ASR model on rare domains, the CSC model may fail to correct phrases with similar pronunciation or anti-context cases where all biasing phrases are not present in the utterance. Second, there is a discrepancy between the training and inference of CSC. The bias list in training is randomly selected but in inference there may be more similarity between ground truth phrase and other phrases. To solve above limitations, in this paper we propose an improved non-autoregressive (NAR) spelling correction model for contextual biasing in E2E neural transducer-based ASR systems to improve the previous CSC model from two perspectives: Firstly, we incorporate acoustics information with an external attention as well as text hypotheses into CSC to better distinguish target phrase from dissimilar or irrelevant phrases. Secondly, we design a semantic aware data augmentation schema in training phrase to reduce the mismatch between training and inference to further boost the biasing accuracy. Experiments show that the improved method outperforms the baseline ASR+Biasing system by as much as 20.3% relative name recall gain and achieves stable improvement compared to the previous CSC method over different bias list name coverage ratio.',\n",
              " 'Orcinus orca (killer whales) exhibit complex calls. They last about a second. In a call, an orca typically uses multiple frequencies simultaneously, varies the frequencies, and varies their volumes. Behavior data is hard to obtain because orcas live under water and travel quickly. Sound data is relatively easy to capture. As a science goal, we would like to know whether orca vocalizations constitute a semantic language. We do this by studying whether machine learning can predict behavior from vocalizations. Such prediction would also help scientific research and safety applications because one would like to predict behavior while only having to capture sound. A significant challenge in this process is lack of labeled data. We work with recent recordings of McMurdo Sound orcas [Wellard et al. 2020] where each recording is labeled with the behaviors observed during the recording. This yields a dataset where sound segments - continuous vocalizations that can be thought of as call sequences or more general structures - within the recordings are labeled with superfluous behaviors. Despite that, with a careful combination of recent machine learning techniques, we achieve 96.4% classification accuracy. This suggests that orcas do use a semantic language. It is also promising for research and applications.',\n",
              " 'Speaker diarization is a task to label an audio or video recording with the identity of the speaker at each given time stamp. In this work, we propose a novel machine learning framework to conduct real-time multi-speaker diarization and recognition without prior registration and pretraining in a fully online and reinforcement learning setting. Our framework combines embedding extraction, clustering, and resegmentation into the same problem as an online decision-making problem. We discuss practical considerations and advanced techniques such as the offline reinforcement learning, semi-supervision, and domain adaptation to address the challenges of limited training data and out-of-distribution environments. Our approach considers speaker diarization as a fully online learning problem of the speaker recognition task, where the agent receives no pretraining from any training set before deployment, and learns to detect speaker identity on the fly through reward feedbacks. The paradigm of the reinforcement learning approach to speaker diarization presents an adaptive, lightweight, and generalizable system that is useful for multi-user teleconferences, where many people might come and go without extensive pre-registration ahead of time. Lastly, we provide a desktop application that uses our proposed approach as a proof of concept. To the best of our knowledge, this is the first approach to apply a reinforcement learning approach to the speaker diarization task.',\n",
              " 'Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temporal receptive field that processes the raw pixels depicting the lips or faces. At the higher level, there is an encoder that attends to the embeddings produced by the front-end over a large temporal receptive field. Previous work has focused on improving the visual front-end of the model to extract more useful features for speech recognition. Surprisingly, our work shows that complex visual front-ends are not necessary. Instead of allocating resources to a sophisticated visual front-end, we find that a linear visual front-end paired with a larger Conformer encoder results in lower latency, more efficient memory usage, and improved WER performance. We achieve a new state-of-the-art of $12.8\\\\%$ WER for visual speech recognition on the TED LRS3 dataset, which rivals the performance of audio-only models from just four years ago.',\n",
              " \"Speech enhancement (SE) is proved effective in reducing noise from noisy speech signals for downstream automatic speech recognition (ASR), where multi-task learning strategy is employed to jointly optimize these two tasks. However, the enhanced speech learned by SE objective may not always yield good ASR results. From the optimization view, there sometimes exists interference between the gradients of SE and ASR tasks, which could hinder the multi-task learning and finally lead to sub-optimal ASR performance. In this paper, we propose a simple yet effective approach called gradient remedy (GR) to solve interference between task gradients in noise-robust speech recognition, from perspectives of both angle and magnitude. Specifically, we first project the SE task's gradient onto a dynamic surface that is at acute angle to ASR gradient, in order to remove the conflict between them and assist in ASR optimization. Furthermore, we adaptively rescale the magnitude of two gradients to prevent the dominant ASR task from being misled by SE gradient. Experimental results show that the proposed approach well resolves the gradient interference and achieves relative word error rate (WER) reductions of 9.3% and 11.1% over multi-task learning baseline, on RATS and CHiME-4 datasets, respectively. Our code is available at GitHub.\",\n",
              " 'A study is presented in which a contrastive learning approach is used to extract low-dimensional representations of the acoustic environment from single-channel, reverberant speech signals. Convolution of room impulse responses (RIRs) with anechoic source signals is leveraged as a data augmentation technique that offers considerable flexibility in the design of the upstream task. We evaluate the embeddings across three different downstream tasks, which include the regression of acoustic parameters reverberation time RT60 and clarity index C50, and the classification into small and large rooms. We demonstrate that the learned representations generalize well to unseen data and achieve similar performance compared to a fully supervised baseline.',\n",
              " 'Word-piece models (WPMs) are commonly used subword units in state-of-the-art end-to-end automatic speech recognition (ASR) systems. For multilingual ASR, due to the differences in written scripts across languages, multilingual WPMs bring the challenges of having overly large output layers and scaling to more languages. In this work, we propose a universal monolingual output layer (UML) to address such problems. Instead of one output node for only one WPM, UML re-associates each output node with multiple WPMs, one for each language, and results in a smaller monolingual output layer shared across languages. Consequently, the UML enables to switch in the interpretation of each output node depending on the language of the input speech. Experimental results on an 11-language voice search task demonstrated the feasibility of using UML for high-quality and high-efficiency multilingual streaming ASR.',\n",
              " \"Recent studies in neural network-based monaural speech separation (SS) have achieved a remarkable success thanks to increasing ability of long sequence modeling. However, they would degrade significantly when put under realistic noisy conditions, as the background noise could be mistaken for speaker's speech and thus interfere with the separated sources. To alleviate this problem, we propose a novel network to unify speech enhancement and separation with gradient modulation to improve noise-robustness. Specifically, we first build a unified network by combining speech enhancement (SE) and separation modules, with multi-task learning for optimization, where SE is supervised by parallel clean mixture to reduce noise for downstream speech separation. Furthermore, in order to avoid suppressing valid speaker information when reducing noise, we propose a gradient modulation (GM) strategy to harmonize the SE and SS tasks from optimization view. Experimental results show that our approach achieves the state-of-the-art on large-scale Libri2Mix- and Libri3Mix-noisy datasets, with SI-SNRi results of 16.0 dB and 15.8 dB respectively. Our code is available at GitHub.\",\n",
              " \"Speech enhancement (SE) is proved effective in reducing noise from noisy speech signals for downstream automatic speech recognition (ASR), where multi-task learning strategy is employed to jointly optimize these two tasks. However, the enhanced speech learned by SE objective may not always yield good ASR results. From the optimization view, there sometimes exists interference between the gradients of SE and ASR tasks, which could hinder the multi-task learning and finally lead to sub-optimal ASR performance. In this paper, we propose a simple yet effective approach called gradient remedy (GR) to solve interference between task gradients in noise-robust speech recognition, from perspectives of both angle and magnitude. Specifically, we first project the SE task's gradient onto a dynamic surface that is at acute angle to ASR gradient, in order to remove the conflict between them and assist in ASR optimization. Furthermore, we adaptively rescale the magnitude of two gradients to prevent the dominant ASR task from being misled by SE gradient. Experimental results show that the proposed approach well resolves the gradient interference and achieves relative word error rate (WER) reductions of 9.3% and 11.1% over multi-task learning baseline, on RATS and CHiME-4 datasets, respectively. Our code is available at GitHub.\",\n",
              " 'A study is presented in which a contrastive learning approach is used to extract low-dimensional representations of the acoustic environment from single-channel, reverberant speech signals. Convolution of room impulse responses (RIRs) with anechoic source signals is leveraged as a data augmentation technique that offers considerable flexibility in the design of the upstream task. We evaluate the embeddings across three different downstream tasks, which include the regression of acoustic parameters reverberation time RT60 and clarity index C50, and the classification into small and large rooms. We demonstrate that the learned representations generalize well to unseen data and achieve similar performance compared to a fully supervised baseline.',\n",
              " 'Word-piece models (WPMs) are commonly used subword units in state-of-the-art end-to-end automatic speech recognition (ASR) systems. For multilingual ASR, due to the differences in written scripts across languages, multilingual WPMs bring the challenges of having overly large output layers and scaling to more languages. In this work, we propose a universal monolingual output layer (UML) to address such problems. Instead of one output node for only one WPM, UML re-associates each output node with multiple WPMs, one for each language, and results in a smaller monolingual output layer shared across languages. Consequently, the UML enables to switch in the interpretation of each output node depending on the language of the input speech. Experimental results on an 11-language voice search task demonstrated the feasibility of using UML for high-quality and high-efficiency multilingual streaming ASR.',\n",
              " \"Recent studies in neural network-based monaural speech separation (SS) have achieved a remarkable success thanks to increasing ability of long sequence modeling. However, they would degrade significantly when put under realistic noisy conditions, as the background noise could be mistaken for speaker's speech and thus interfere with the separated sources. To alleviate this problem, we propose a novel network to unify speech enhancement and separation with gradient modulation to improve noise-robustness. Specifically, we first build a unified network by combining speech enhancement (SE) and separation modules, with multi-task learning for optimization, where SE is supervised by parallel clean mixture to reduce noise for downstream speech separation. Furthermore, in order to avoid suppressing valid speaker information when reducing noise, we propose a gradient modulation (GM) strategy to harmonize the SE and SS tasks from optimization view. Experimental results show that our approach achieves the state-of-the-art on large-scale Libri2Mix- and Libri3Mix-noisy datasets, with SI-SNRi results of 16.0 dB and 15.8 dB respectively. Our code is available at GitHub.\",\n",
              " 'Existing deep learning based speech enhancement (SE) methods either use blind end-to-end training or explicitly incorporate speaker embedding or phonetic information into the SE network to enhance speech quality. In this paper, we perceive speech and noises as different types of sound events and propose an event-based query method for SE. Specifically, representative speech embeddings that can discriminate speech with noises are first pre-trained with the sound event detection (SED) task. The embeddings are then clustered into fixed golden speech queries to assist the SE network to enhance the speech from noisy audio. The golden speech queries can be obtained offline and generalizable to different SE datasets and networks. Therefore, little extra complexity is introduced and no enrollment is needed for each speaker. Experimental results show that the proposed method yields significant gains compared with baselines and the golden queries are well generalized to different datasets.',\n",
              " 'Stuttering is a neuro-developmental speech impairment characterized by uncontrolled utterances (interjections) and core behaviors (blocks, repetitions, and prolongations), and is caused by the failure of speech sensorimotors. Due to its complex nature, stuttering detection (SD) is a difficult task. If detected at an early stage, it could facilitate speech therapists to observe and rectify the speech patterns of persons who stutter (PWS). The stuttered speech of PWS is usually available in limited amounts and is highly imbalanced. To this end, we address the class imbalance problem in the SD domain via a multibranching (MB) scheme and by weighting the contribution of classes in the overall loss function, resulting in a huge improvement in stuttering classes on the SEP-28k dataset over the baseline (StutterNet). To tackle data scarcity, we investigate the effectiveness of data augmentation on top of a multi-branched training scheme. The augmented training outperforms the MB StutterNet (clean) by a relative margin of 4.18% in macro F1-score (F1). In addition, we propose a multi-contextual (MC) StutterNet, which exploits different contexts of the stuttered speech, resulting in an overall improvement of 4.48% in F 1 over the single context based MB StutterNet. Finally, we have shown that applying data augmentation in the cross-corpora scenario can improve the overall SD performance by a relative margin of 13.23% in F1 over the clean training.',\n",
              " 'Visual speech (i.e., lip motion) is highly related to auditory speech due to the co-occurrence and synchronization in speech production. This paper investigates this correlation and proposes a cross-modal speech co-learning paradigm. The primary motivation of our cross-modal co-learning method is modeling one modality aided by exploiting knowledge from another modality. Specifically, two cross-modal boosters are introduced based on an audio-visual pseudo-siamese structure to learn the modality-transformed correlation. Inside each booster, a max-feature-map embedded Transformer variant is proposed for modality alignment and enhanced feature generation. The network is co-learned both from scratch and with pretrained models. Experimental results on the LRSLip3, GridLip, LomGridLip, and VoxLip datasets demonstrate that our proposed method achieves 60% and 20% average relative performance improvement over independently trained audio-only/visual-only and baseline fusion systems, respectively.',\n",
              " 'End-to-end automatic speech recognition (ASR) usually suffers from performance degradation when applied to a new domain due to domain shift. Unsupervised domain adaptation (UDA) aims to improve the performance on the unlabeled target domain by transferring knowledge from the source to the target domain. To improve transferability, existing UDA approaches mainly focus on matching the distributions of the source and target domains globally and/or locally, while ignoring the model discriminability. In this paper, we propose a novel UDA approach for ASR via inter-domain MAtching and intra-domain DIscrimination (MADI), which improves the model transferability by fine-grained inter-domain matching and discriminability by intra-domain contrastive discrimination simultaneously. Evaluations on the Libri-Adapt dataset demonstrate the effectiveness of our approach. MADI reduces the relative word error rate (WER) on cross-device and cross-environment ASR by 17.7% and 22.8%, respectively.',\n",
              " 'We previously proposed contextual spelling correction (CSC) to correct the output of end-to-end (E2E) automatic speech recognition (ASR) models with contextual information such as name, place, etc. Although CSC has achieved reasonable improvement in the biasing problem, there are still two drawbacks for further accuracy improvement. First, due to information limitation in text only hypothesis or weak performance of ASR model on rare domains, the CSC model may fail to correct phrases with similar pronunciation or anti-context cases where all biasing phrases are not present in the utterance. Second, there is a discrepancy between the training and inference of CSC. The bias list in training is randomly selected but in inference there may be more similarity between ground truth phrase and other phrases. To solve above limitations, in this paper we propose an improved non-autoregressive (NAR) spelling correction model for contextual biasing in E2E neural transducer-based ASR systems to improve the previous CSC model from two perspectives: Firstly, we incorporate acoustics information with an external attention as well as text hypotheses into CSC to better distinguish target phrase from dissimilar or irrelevant phrases. Secondly, we design a semantic aware data augmentation schema in training phrase to reduce the mismatch between training and inference to further boost the biasing accuracy. Experiments show that the improved method outperforms the baseline ASR+Biasing system by as much as 20.3% relative name recall gain and achieves stable improvement compared to the previous CSC method over different bias list name coverage ratio.',\n",
              " 'Orcinus orca (killer whales) exhibit complex calls. They last about a second. In a call, an orca typically uses multiple frequencies simultaneously, varies the frequencies, and varies their volumes. Behavior data is hard to obtain because orcas live under water and travel quickly. Sound data is relatively easy to capture. As a science goal, we would like to know whether orca vocalizations constitute a semantic language. We do this by studying whether machine learning can predict behavior from vocalizations. Such prediction would also help scientific research and safety applications because one would like to predict behavior while only having to capture sound. A significant challenge in this process is lack of labeled data. We work with recent recordings of McMurdo Sound orcas [Wellard et al. 2020] where each recording is labeled with the behaviors observed during the recording. This yields a dataset where sound segments - continuous vocalizations that can be thought of as call sequences or more general structures - within the recordings are labeled with superfluous behaviors. Despite that, with a careful combination of recent machine learning techniques, we achieve 96.4% classification accuracy. This suggests that orcas do use a semantic language. It is also promising for research and applications.',\n",
              " 'Speaker diarization is a task to label an audio or video recording with the identity of the speaker at each given time stamp. In this work, we propose a novel machine learning framework to conduct real-time multi-speaker diarization and recognition without prior registration and pretraining in a fully online and reinforcement learning setting. Our framework combines embedding extraction, clustering, and resegmentation into the same problem as an online decision-making problem. We discuss practical considerations and advanced techniques such as the offline reinforcement learning, semi-supervision, and domain adaptation to address the challenges of limited training data and out-of-distribution environments. Our approach considers speaker diarization as a fully online learning problem of the speaker recognition task, where the agent receives no pretraining from any training set before deployment, and learns to detect speaker identity on the fly through reward feedbacks. The paradigm of the reinforcement learning approach to speaker diarization presents an adaptive, lightweight, and generalizable system that is useful for multi-user teleconferences, where many people might come and go without extensive pre-registration ahead of time. Lastly, we provide a desktop application that uses our proposed approach as a proof of concept. To the best of our knowledge, this is the first approach to apply a reinforcement learning approach to the speaker diarization task.',\n",
              " 'Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temporal receptive field that processes the raw pixels depicting the lips or faces. At the higher level, there is an encoder that attends to the embeddings produced by the front-end over a large temporal receptive field. Previous work has focused on improving the visual front-end of the model to extract more useful features for speech recognition. Surprisingly, our work shows that complex visual front-ends are not necessary. Instead of allocating resources to a sophisticated visual front-end, we find that a linear visual front-end paired with a larger Conformer encoder results in lower latency, more efficient memory usage, and improved WER performance. We achieve a new state-of-the-art of $12.8\\\\%$ WER for visual speech recognition on the TED LRS3 dataset, which rivals the performance of audio-only models from just four years ago.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trendflow.lrt.clustering import ClusterPipeline, Configuration,BaselineConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534,
          "referenced_widgets": [
            "8ae83559b11b4e2d9fe58aa6abc5a96e",
            "6db2186b6a83489ab5b476a0bd3ae2aa",
            "05878862954541ddb77ab7edb4769de3",
            "2e8b23b2999f4abbbaaaccecf94df051",
            "954b09471cca45ab9eaa0dab9c68cecb",
            "5c8d2d3899c54a57a56b1757dcb17d2c",
            "6fc159fb437d47ed8a894c2e0b795a43",
            "bb2e34f148ad439f908db7f436019881",
            "8ae0725ccdf646fdaa1aedcd91a69bd2",
            "af7636b7d4b645b7957fb06f66650618",
            "65669b9abcba45f1a0c5af707083d570",
            "427d921f616d48c68b84b48630124248",
            "7aa7e087aaf24babb528053a52e192fe",
            "3a423c98c1ff4fb7a15b235466891973",
            "25e276cc46ee46198fc9365bb669f555",
            "d3adb812a122499a8558c635f1470c22",
            "290e1d4708f047319e201b798ee48885",
            "fa7aecf3211f4db4b2d271e3c6d9e830",
            "221686fea867482cb861de64385ed116",
            "7472c0bc72564089ab3e090362c81254",
            "d148c3f128b64e02a2ad8fd0d8b2f2ad",
            "20e7b14bb67144ea847e14923070d624",
            "6e01c27cbaa24825a54b45125e00390b",
            "b9f1b0ab81d44cb9bab76bb5323d9273",
            "df6866b93b9e4bc79fc460d36774f823",
            "2aa68c08af4d4f3a82db39f8fc7a1686",
            "0041eb297cfb4f6e828a996be211b972",
            "9775f0fe8ebe45988073e6acf28adf80",
            "23bfcb1ca1b5448ea505d8a912f21f4a",
            "1098809972f4468e99abc0d771ea0e52",
            "85119b164c95411a8eae81a68c91368a",
            "b9ee2363791b47e2a9bd947aec05cc6b",
            "628a7c2718a7443e83edbe6764daca1e",
            "f76d1eb1ee3144449d573a48f37d3710",
            "d151d926d2f54699a606a30ac056113c",
            "12f5471e921247a28f302884e6246ab0",
            "721146ab982647c79c28a81bd4f75fdb",
            "cfc2dd3861e94e998a3323a6e25f3ba0",
            "ba2dcc52c87f42dc9cea3db0b4fea8bd",
            "684c9dc166184257932407ddb0e8a26c",
            "cb9d6a6bbf314577b5c8379148a2b145",
            "7948445092d849bc8d7a829d883350df",
            "dc4fb855adca467ea825eebc2ff3ce08",
            "d5b466e610844ea5834868223a88e4e3",
            "8f703c56820d441d89a48cf35ec8db68",
            "8505f58481f44b7a9a6afa0b9b70cf5a",
            "7bfa69fc752f42c9a6a72392064ca9e6",
            "7410aa9c44844766a3aa22b696967480",
            "46a844384240456b8b91a27f9ad831cb",
            "4f3cb85b626d4210b9b738265f7988ff",
            "4da96f47c43d4b369e9101151da9ee90",
            "176191747b2346c0a538f61cfb6fe51d",
            "9b091be617d94383a7f2d3c4634268fe",
            "836d83ae97284e4f91742951a6a75989",
            "076570802c8c48609a561a2faf9b9005",
            "8ebfc2038f0d48c4b632454221d2be0a",
            "a65ce49a42324fc09fc9b175721464ff",
            "be9a63ee2c55416291299e2b03118fa9",
            "db0e45988f2e48f7b3a59fdec3ad363e",
            "47935e21ff624e94afd1af095931e472",
            "1ffa7e95d96446d893b0d541694b9629",
            "b56e5f2036b24f65a6fad456a6300e67",
            "597af8d2fe924cf290d59c4f7752ae22",
            "543ad99e70e44c69a0576f6fa29849b1",
            "7cc5921396f94d63892b5f0e32f98447",
            "f2c4f6e031af49b1a3c4c32b96c6fae7",
            "ab32ca5ebd204e438672cb9e7e0d4485",
            "79056073a7fb4fe1957c13b49c7cf131",
            "741e1a3133db4b1b8964455b49463f8e",
            "13e86a17c5b6494b9a68bc7abc995770",
            "a112a6c5294f4f57b404dc35a0c2a831",
            "390917ee25204728aa3305a392d21689",
            "2adf7572db9648b98106d504d5570507",
            "79a6cea97682478fa31d638681679212",
            "477941937d294f1a8da25a5495e4736d",
            "3df91721a25a4ae4a4cf66e881c8df9f",
            "966c40e2fb1c4a5da068720ee9e61be7",
            "a7b50b28062c465f80b10f5ef4d9f9bc",
            "fdb0f63e5a404d248aa191a081b089e5",
            "5a943c13502b4078af9c3bee4d78ea8c",
            "62209b0e1a7649668c2e4a8cc3d81a0e",
            "e90ac26826954c57a90b3947f3974f05",
            "ebbcda309d374d23933207c22eb24d64",
            "f5820023ca1b405cbbe338c89d657396",
            "3f5e350e53fc45378d3e79611adb66a6",
            "0c321aaa2ef9452aadecea8551316968",
            "38b04a6da66d4989915ea791108117e7",
            "5dd7eb4878a6420586b4c5de5dddf2a4",
            "42d509907d394f0293fa2e11b4afcd8a",
            "e4a9bad6a3d84a0aa82523f0092fb903",
            "0c638477821b4971ae2f3bb1e3bef43b",
            "4731078865384c4b809e0aa1a1d5aa6f",
            "ac06166affca477dbc183cf2449f7e8c",
            "a36d495595974c63af1d4aee6c5d1d74",
            "3d1ff1d4ff8742fc8815e7690d9f3179",
            "6d0aa7f08ec04861933d6e93ab04f79e",
            "dffcb7e870144c29a2de4860f85f67d9",
            "35aaa1637818468c81b7775f221cc267",
            "543774c7710342b7b2a165d764d28e7f",
            "0b95e39f47f34f3498ea301c69350285",
            "d54e9f4e53784237a2299a8a2bf61e37",
            "54b9a22613c5493380a7964b52be7899",
            "88d281dde0624e5680061fceae2dd931",
            "03385f540b3643848dc6d6aa9601b21e",
            "a8610bedc97842dea1764afaa1ed33b8",
            "00726846fbda49a596341894a0e5a5e8",
            "6ac96dec71104e49a191c38d31b28c7b",
            "5341a643726046218510ef6a96a185fa",
            "b842632ebb1a4a84a0fb58af01b5392f",
            "6e2bcc65628e461e87d9ac4a1fab680d",
            "83fdc910aff749e881a3d4dd8d215273",
            "592b6291fbb1464ea1c1cebb34fe3ae5",
            "14101fa6beaa4e1fadd05e160db47c09",
            "d1e19a8e0a8145debe512aefe5a3b404",
            "05e78e3776a9446a8e75382cbead308a",
            "b8309279ce7542499b9d0aba749a37e9",
            "473d1e9252e94ac6aca022ff4aa51b56",
            "4a90cfba72064dc791ac54028c8940c5",
            "c3add2f864f54ce2893dd5131073959b",
            "3306b42d03d04e0abd8627c5cb74ac3e",
            "918379d2943749f7aca904453e31fb05",
            "ca80c7f809624476bcd482dc07d294f7",
            "62dbeee55eee47e3b3340622a9811df9",
            "ee2c803db17546308b128b1d004404e1",
            "86db3a836e214fcbb315c9582051e934",
            "082fb62422a843c19bee6cd0e3aafec2",
            "4ff121b30bd647f1a7b0f7c53e250c02",
            "05ba914bec2a4adc96a4b4c58ae672b5",
            "c767ebaa57c34eb7b7f714a518db91f0",
            "95bee6c62a7c4ef4b07caa9799b36d4f",
            "46b957bcb16d479e9a7c5cb9b1fbb3f7",
            "48ea82547ceb4acab2c91fdcfc2db363",
            "7e939bc23b744f8a9f1730870d0716d5",
            "f38db89804d84aa8a1e3c0532cb47068",
            "5ebfa2b84a4e4a31a0888d94fef5deee",
            "69ac8f580561422b91670de79cd03a4d",
            "a1cd7fc336944d549a7cf70a06b64203",
            "f83c525c1277487b8fb8c60e6f5eac52",
            "ea9efaebe0e4498cbfa78c9dea0d0293",
            "7b28c2d608ea47268c1dbaa16e52ae30",
            "9bfbe2c767144efabacd4dd1eefe2e14",
            "bc8aa6729eb34d57806d40c6cdfc6ad4",
            "48b12856a1204004a2c82cbe7187e74f",
            "fe01942c9a0b44db93d9ece417efbd26",
            "27a2a8a8f1aa4b6bb8df0c7a4510cd8f",
            "df5c130b98174099ab89d030c6fe48f2",
            "c28ca8700c564cd694e71f4f86da8a4c",
            "f8e86f396c2845a2b2806f9c8b09b1be",
            "ccdb7f2a4cba4bc4a3e2b87429194d6d",
            "19e70173fe674d6187315d64f58c8883",
            "41b58d0c6a914168a7c67d1ba51b1ddb",
            "4c10a9d2060a41c6b5289aae6a6f297f",
            "32fb99b839a946d48f627aaafede39af",
            "c0a421556c454d22adf4bd82c174dfa3",
            "c87a103d23cc47158c689b3e7fe75ec9",
            "ee9a98f6e1504ecfb0e942f5d3c856ee",
            "274007015e9f4894a93813c55ca93c97",
            "8e4870c4bab940caaa299a9feaffeea2",
            "1a84477a0a944827913ac7901041d1f1",
            "2cfb9d9d0df3408e9de65c6cff594968",
            "26f4013d8e874336973dc8205c324fd7",
            "55f79d2b47604365a12b77fdba061aa4",
            "a804ce499c3a4a9d8a0092e750ab4d70",
            "4471bc04619e4a83b57eb0be223141b0",
            "e661b6176921470fb57076213bf5c318"
          ]
        },
        "id": "X23EPBWRaGCF",
        "outputId": "c71cf0b4-8234-4889-c459-d478232372d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ae83559b11b4e2d9fe58aa6abc5a96e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "427d921f616d48c68b84b48630124248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e01c27cbaa24825a54b45125e00390b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f76d1eb1ee3144449d573a48f37d3710"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f703c56820d441d89a48cf35ec8db68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ebfc2038f0d48c4b632454221d2be0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab32ca5ebd204e438672cb9e7e0d4485"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7b50b28062c465f80b10f5ef4d9f9bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42d509907d394f0293fa2e11b4afcd8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b95e39f47f34f3498ea301c69350285"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83fdc910aff749e881a3d4dd8d215273"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca80c7f809624476bcd482dc07d294f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e939bc23b744f8a9f1730870d0716d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe01942c9a0b44db93d9ece417efbd26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c87a103d23cc47158c689b3e7fe75ec9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = BaselineConfig()\n",
        "\n",
        "config1 = Configuration('all-mpnet-base-v2','pca','kmeans-euclidean','keyphrase-transformer')\n",
        "\n",
        "config2 = Configuration('all-mpnet-base-v2', 'none', 'gmm', 'keyphrase-transformer')\n",
        "\n",
        "config3 = Configuration('all-mpnet-base-v2', 'pca', 'gmm', 'keyphrase-transformer')\n",
        "\n",
        "configs = [\n",
        "    baseline,\n",
        "    config1,\n",
        "    config2,\n",
        "    config3\n",
        "    \n",
        "]"
      ],
      "metadata": {
        "id": "z_OZS8mku907"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def compute_center(texts,cluster_pipe,n_components):\n",
        "    center = cluster_pipe.__1_generate_word_embeddings__(texts)\n",
        "    # center = cluster_pipe.__2_dimenstion_reduction__(center, n_components)\n",
        "    if cluster_pipe.dimension_reduction is not None:\n",
        "      pca = PCA(n_components,svd_solver='randomized')\n",
        "      center = pca.fit_transform(center)\n",
        "    center = np.mean(center, axis = 0)\n",
        "    return center"
      ],
      "metadata": {
        "id": "AiWtyyn74gvF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsupervised_learning.clustering import GaussianMixture, Silhouette, compute_clustering_accuracy\n",
        "import numpy as np\n",
        "\n",
        "accuracies = []\n",
        "for config in configs:\n",
        "  print(config)\n",
        "  cluster_pipe = ClusterPipeline(config)\n",
        "\n",
        "  x = cluster_pipe.__1_generate_word_embeddings__(texts)\n",
        "  x = cluster_pipe.__2_dimenstion_reduction__(x)\n",
        "  print(x.shape)\n",
        " \n",
        "\n",
        "  cv_center = np.mean(x[:92],axis=0)\n",
        "  nlp_center = np.mean(x[92:159],axis = 0)\n",
        "  audio_center = np.mean(x[159:218], axis=0)\n",
        "\n",
        "  gold_centers = np.array([cv_center,nlp_center,audio_center])\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  best_k_algo = Silhouette(GaussianMixture,2,10)\n",
        "  best_k = best_k_algo.get_best_k(x)\n",
        "\n",
        "  labels, cluster_centers = cluster_pipe.clustering(x, k=best_k)\n",
        "\n",
        "  accuracy = compute_clustering_accuracy(\n",
        "      gold_labels = list(all_df['label']),\n",
        "      gold_centers = gold_centers,\n",
        "      clustered_labels = labels,\n",
        "      cluster_centers = cluster_centers\n",
        "      )\n",
        "\n",
        "  accuracies.append(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsEZ_MWbv9qP",
        "outputId": "956a0168-61a1-4ae0-c5b9-5d05fb2f0f3d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<trendflow.lrt.clustering.config.BaselineConfig object at 0x7f830257c340>\n",
            ">>> start generating word embeddings...\n",
            ">>> successfully generated word embeddings...\n",
            "(218, 768)\n",
            "<trendflow.lrt.clustering.config.Configuration object at 0x7f830257c3a0>\n",
            ">>> start generating word embeddings...\n",
            ">>> successfully generated word embeddings...\n",
            ">>> start dimension reduction...\n",
            ">>> The reduced dimension is 111.\n",
            ">>> finished dimension reduction...\n",
            "(218, 111)\n",
            "<trendflow.lrt.clustering.config.Configuration object at 0x7f830257c280>\n",
            ">>> start generating word embeddings...\n",
            ">>> successfully generated word embeddings...\n",
            "(218, 768)\n",
            "<trendflow.lrt.clustering.config.Configuration object at 0x7f830257c370>\n",
            ">>> start generating word embeddings...\n",
            ">>> successfully generated word embeddings...\n",
            ">>> start dimension reduction...\n",
            ">>> The reduced dimension is 111.\n",
            ">>> finished dimension reduction...\n",
            "(218, 111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PIwAjdBDPuR",
        "outputId": "b9e041a9-6136-484f-8728-6d36d0c70148"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8853211009174312, 0.8853211009174312, 0.8944954128440367, 0.8899082568807339]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import MultipleLocator\n",
        "\n",
        "plt.figure(figsize=(10,10),dpi=600)\n",
        "\n",
        "# 数据\n",
        "labels = ['Baseline: kmeans', 'kmeans+pca', 'gmm', 'gmm+pca']\n",
        "values = accuracies\n",
        "\n",
        "# 绘制柱状图\n",
        "plt.bar(labels, values)\n",
        "\n",
        "# 设置标题和坐标轴标签\n",
        "plt.title('Clustering Experiment Result')\n",
        "plt.xlabel('Configurations')\n",
        "plt.ylabel('Accuracies')\n",
        "\n",
        "\n",
        "y_major_locator=MultipleLocator(0.002) \n",
        "\n",
        "ax=plt.gca() #ax为两条坐标轴的实例\n",
        "ax.yaxis.set_major_locator(y_major_locator) #把y轴的主刻度设置为10的倍数\n",
        "\n",
        "plt.ylim(0.88,0.9) # 设置y轴范围\n",
        "\n",
        "# 显示图形\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AruIK9NHAZ6j",
        "outputId": "1ea58fbc-cf26-46b0-9fee-f71347bc534a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 6000x6000 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAFD0AABOlCAYAAACnTsTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAFxGAABcRgEUlENBAAEAAElEQVR4nOzdd7hsZ1U/8O9KQgihBELvoUrvJaGGKghIU6oIKCJF/QGKSlGaKIKCBQEBBQQxkSZFioIJEEB6MfQuHUJJSG/r98eeSAg3d/bMmX3m3HM/n+eZB7iz9rvWnDOz93uGM99T3R0AAAAAAAAAAAAAAAAAAAAAAAAAgFXbY90DAAAAAAAAAAAAAAAAAAAAAAAAAADbk9BDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiH0EAAAAAAAAAAAAAAAAAAAAAAAAACYhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGASQg8BAAAAAAAAAAAAAAAAAAAAAAAAgEkIPQQAAAAAAAAAWJGqOryqes7t4HXPCWwPzjkAMK2qeumIa+2D1j0nAMCOVNVXRuxlDlj3nAAAAAAAAOwe9lr3AAAAAAAAAAAAZ1VVeyW5VJIDZreLJzl3kn1nt72SnJTkxCTHJflOkm8l+WaSz3T3jzZ7ZgAAAAAAAAAAAAAA4GcJPQQAAAAAAAAA1q6qLpHklkkOTHJQkuskOccG1vtmkiOTvDfJu5L8d3efsPFJAQAAAAAAAAAAAACARQg9BAAAAAAAAADWoqouleSXkvxyhqDDWuHyl5jdbj/73ydW1X8meW2S13f3D1fYCwCYUFV9Jcll1z3HEm7V3YevewgAgGVU1cFJDpto+VOTnJTk5CRHJ/luku8k+XKSzyT5ZJIP+gMWAAAAAAAAANuH0EMAAAAAAAAAYFNV1Y2T/F6SuyfZc5Pa7pPkLrPbSVX16iQv6O4jNqk/AAAAAIO9ZrdzJ7lAkgN2UHNKVX04yb8neVV3f3bzxgMAAAAAAABg1fZY9wAAAAAAAAAAwO6hqm5YVe9M8t9JfimbF3h4VudMcv8k766qD1fVL65pDgAAAAB27BxJDkzytCSfqar3VdW9qmpd7ycBAAAAAAAAsAFCDwEAAAAAAACASVXVharqRRnCDm+x7nnO4npJXl9VH6mqg9c8C7uxqjq8qnrO7eB1zwkAALCd+Flsl3JgkkOTfKyqbr3uYYCfVlUHjzifHr7uOQEAAAAAAFgfoYcAAAAAAAAAwGSq6g5JPp3kIdnav6dw3SSHVdUhVXWJdQ8DAAAAwA5dI8k7qur5VbXPuocBAAAAAAAAYJyt/GECAAAAAAAAAGAXVVV7VtWfJHlzkgute54F3DvJvdY9BAAAAAA79bAk766qC657EAAAAAAAAADm22vdAwAAAAAAAAAA20tV7ZPk1UnutIFlTktyZJL3JPlkkq8k+XKSHyU5bnbbO8l5kpw7ySWTXHF2u36SmyY53wb6AwAAALC13SDJYVV1cHf/YN3DAAAAAAAAAHD2hB4CAAAAAAAAACtTVedJ8oYkt1ri8JOTvDXJIUne1N0/nlN/wuz2vQyhiO850xx7JrlekrsmuV+Syy0xDwAAAAA798UkRyx4zD5JLpDk/EkuluQyG+h/zSSHVNUdu/u0DawDAAAAAAAAwISEHgIAAAAAAAAAK1FV+yb5jyQHLXjoj5P8fZK/7O5vr2KW2YfcPzi7PbGqDkryqCS/lGSPVfQAWLfuPnjdM8BW0t217hmA7aW7H5TkQWseA2CrO2J2vlxaVV00yY2T/HySByQ574JL3C7JHyT5043MAQAAAAAAAMB0/BI/AAAAAAAAALBhVVVJXpHFAw9fkeQK3f3YVQUe7kh3v6+7753kKklekuT0qXoBAAAAMF53f6e739Ddj0xyqSSPSXLsgss8saout/rpAAAAAAAAAFgFoYcAAAAAAAAAwCo8I8ndF6g/KskduvsB3f29iWb6Gd39+e7+tSQ3SvL+zeoLAAAAwHzdfUx3PyfJdZN8aIFDz5XkidNMBQAAAAAAAMBGCT0EAAAAAAAAADakqu6e5PcXOOQTSW7Y3W+baKS5uvvDSQ5K8sgkJ65rDgAAAAB+Vnd/Icktsljw4a9U1cUnGgkAAAAAAACADRB6CAAAAAAAAAAsraoulOQFCxzykSS36O6vTDPReD14XpIbJfn0uucBAAAA4Ce6+4Qkv5jkGyMP2TvJvaebCAAAAAAAAIBlCT0EAAAAAAAAADbiuUkuMrL2yCS37+6jJ5xnYd39P0lukOR1654FAAAAgJ/o7m8ledwCh9xzqlkAAAAAAAAAWJ7QQwAAAAAAAABgKVV1myT3Hll+dJK7dff3Jxxpad19fJJfSvLX654FAAAAgJ/yyiSfG1l7YFWda8phAAAAAAAAAFic0EMAAAAAAAAAYFl/ukDtr3f3FyebZAW6+/TuflSSV6x7FgAAAAAG3X1akheOLN8ryfUnHAcAAAAAAACAJey17gEAAAAAAAAAgF1PVd0tyY1Glr+mu18z4Tgr1d1HrXsGVqeq9khy2SQ/N/vPiyTZP8k5Z7dTkhw/u52Q5NgkX0/y1SRf7e7vrWHsbaeqLpHkSkkun+TSSc6b5NxJ9s7wdT8uyVFJvpzki0mO7O5T1zMtAOx6qupCGfY7l8+w5zlfkn2TnCvJiRmutT/IcK39cpJPdPeJ65kWtjd7X5jMYQvUXjPJEVMNMlZVVZIrJrlCkssluUSG88G+SfbM8F7EcUm+meGc8Lnu/vx6pl1cVe2fYf9xxSQXS3LhDI/tnLOSE/LT77l8J7P3W5J83bkPAAAAAAAAdi9CDwEAAAAAAACAZTxhZN3xSR4z5SBwVlX1c0nunuTWSW6cIfRn2bWOT/LpJP99xq27v7CKObezqtovyd2S3D7JzZJcZsEljq+qDyR5S5JDu/urq51wc1XVnhmeizfLEAhx5SSXSnKe2W2vDEEXR2cIu/iP7n7SeqbdHLNA0oOS3DLJNZJcLcmFkuyXnwR0HZvkGxnCPz6a5D1J3tPdp6xj5kVU1cWS3C7JgUmukiH0aL8MoUenJjlmdvtSkk8m+XiSt3b3d9YyMGwzVXX+JD+f5HpJrp4hiGe/DHuCvTOcX854DX46yfuTvK27v72OeZdRVedKcqckv5DkphmuLYs4uao+nOQ/MlxrP73iETfVLFTqOhmuK1fJcL09I2zuPBnCl47PcK39VpKPdPfD1jLsJqmqcyY5OMP19uoZvi77Z3gd7JPhNfDDDHuPD2R4HfxHdx890TxXzvD9uc7sdtEMr8v9kpyWYS/09SSfn83y9u7+2BSzrJq970/b7vs81upjSX6U5Pwjai875SA7U1WXSfLLSW6V5CZJLrDg8UdlCGx8fZLXTXVeXsbsfHenJHfMsP+43AaWO62qvpbhGnTGey4f6e6TNjwoAAAAAAAAsCVVd697BgAAAAAAAABgF1JV10vy4ZHlz+ru359ynq2kqg7PEOywM7fq7sNX0Osrmf8h/st191c22muMqjogQ2DFzny1uw+YqP9eSe6X5LeS3HCKHmfyjSSvTnJohhDEUb+AU1UPSvKSCeca653dffAUC1fVTTMEnd4pQ7jQqhyW5Bnd/R8rXPNsVdXBs547M/frWFW3TPKwDIEQ+y0wwqjv0Safcw7Oar4mV0ry6CT3THKRJUb5QZLXJHnOVgvoqqp9ktw7yUMzhJss6vQMgR8vTvLy7j55J70emSFEZWeeu4rv/VYw8pqX7q7pp5lOVT06ybNHlJ6Y5Mbd/YmJR/o/VfX8DOezeb6X5Drd/c2JR/oZs4DZe2SY8xZZ/A+Dd5L3JXleklft7DW4TlV1zSS/l+GxnmeFS384yZ8neU13n77CdXdoVXvHqrpukkcmuUsWu66M2pdW1UuTPHBO2YO7+6UL9D67XgdkNV+TA5M8KsN+bNHnyAlJXpXkb7p77M98O5vlAkl+M8P18TpLLPGpJC9I8vdb8TVp7/sz62zZfd7u8LPYFEY+N5LkZd39oEmHmamqj2bc+eSfu/tXJh7n/8z2IffK8J7EQUlWtS89OckhGc4Ja/v5p6qun+T/ZXiMqzzfndVJSd6a4f2WN3b3sRtZbN3vVe3IyPPRyl5TU7x/OPbns02wkvc7AAAAAAAA2Dx7rHsAAAAAAAAAAGCX85sj605O8lcTzgFJkqq6c5JPJnlZpg88TJJLZviw/3uTfLWqLrcJPbe0qjqoqt6V5IgMIUyrDkG4VZK3VdV7qupaK1575arqDlX1iSSHJ7lPFgs83Jaq6tJVdWiSzyR5eJYLwkmS/ZP8RpIjq+oVVbXsOitTVXtU1YOTfCHJS7Nc4GEy/D7fgRlCD79cVQ+rqrMLS7lhhkChnd0OWHIO1qS7n5PkTSNK90lyaFWde+KRkiRV9UsZF3jYSX51mcDDqjq4qnrO7fA5M342yb8muXUWDzxMhnCimyR5RZLPV9V9l1hjMlV11ap6Y5KPJ/nVrDbwMEmun+Hr9z+z0N4trapuVFXvTvKRJL+e5a8r20ZVXbuq3pkhvPPeWe45cq4Mz68PVtWLquqCS85ynqr64wxBU3+W5QIPk+RqSf4myeeq6g5LrrFy9r4/bTvv89iSvj+ybtXXybNVVQ9I8rkkr8ywl1hlEPfeGc7Ln6yql1bVhVe49lxVdanZ6/tDSR6QaQMPM1v/rhm+lt+tqt+auB8AAAAAAACwiYQeAgAAAAAAAACjVdU+ScYGwByyTOgNjFVV+1TVPyZ5Y5Irr2mMS2c3DrSrqvNX1QuTvCfJzTeh5U2SfLiqnlpVW+53n6rqwlX16iRvSXLNdc+zVVTVIzIEk94rq/udtT2S3D/Jp6vqjitac2FVdUCSdyX5xwyBqKtyiSTPT3KYYNXdzoOSfH1E3VWSPG/aUZLZ8+/FI8v/orvfOuU8ZzU7774myauSXGGFS18mySur6j+r6mIrXHdhs/3On2YIO7xzVhuktCNXS3J4Vf39bO+/pVTVuavqBRmC/W627nm2gqras6qeliGQ6harWjbJQzKEz11/wXlunOH5+pSsbp982SRvqapn7SQQeHL2vj9rO+/z2LLGhh7uO+kUSarqKrNQ5n9Kcvmp2yV5YJLPVNU9J+41NKy6d5IjM7y+1+FcSa64pt4AAAAAAADABLbkLz8AAAAAAAAAAFvWrZKcd2Tty6cchN1bVe2f5LAkD173LLurqrp2kg8n+Y1MH8B0Znsl+aMkb66q829i352qqmsm+WCSTQmg2BVU1Tmr6uVJ/i7jrx2L2j/JG6vqNyda/2zNQng+nuSmE7a5ZZL3V9WNJuzBFtLd388QMH3aiPJfrapfnWqWqjpHkkMyLrTs/UmeMNUsO1JVV8wQfHePCdvcNsnHq2ozws1+xixY9T1JHpfkHJvc/qFJjqiqS21y37NVVZdOckSS34zfgU6SVNV5MgSAPzHDHmnVLpbknVV1p5HzPCrD92iq8K/fS/LydQQA2vv+tO2+z2NLG/v6O33SIarukyFs9pZT9tmB/ZO8uqr+bMoQ2Kp6fMbvAwEAAAAAAABG8Qs/AAAAAAAAAMAiRoVdJPlOhkA6WLmq2jfJm5IcuO5ZdldVdY8MQVNTBdqM8fNJ3jELwFyrqjooyXuTXHbds2wVsxCmtyf5lU1ot2eS51fVAzahV5Jk1usNSc63Ce0unOSwqrr1JvRiC+juI5L88cjy51XVz000yjOSjAncPDrJfbr7lInm+BlVdbUM590rbEK7iyR5W1X9wib0+j9VdbMMYUrX28y+Z3H9JO+ehS+uVVVdKckHklxnzaNsGVW1X5J3JrnjxK3OneRVVXX9OfP8cZLnZJrwxTO7f5K/nLjHT7H3/WnbfZ/HlnfBkXU/mmqAqnpGkn/JcH5clz9M8uIpQmBnAbZPX/W6AAAAAAAAAEIPAQAAAAAAAIBFjA17eUN3nzbpJOzO/jrJQeseYndVVfdJcmiSc617lgxBUO+YBa+sRVVdJUMI59pm2Gqqau8kr0tys81sm+RFVXWdyRtV/VKSl2b6UKcz2zfJa2bPN3YPz8gQKDXPuZMcWlX7rLJ5Vd0pyaNHlv96d39llf13pqoukuTfMwSCbpZzJfm3qrrNZjSrqlsmeWvGBztN6YAk76yqS6xrgKq6aIavx8XWNcNWc6Zr7WaFYp4ryetn34sdzfPHSZ6ySbMkyaOq6l6b0cje96dt930eu4S1hh5W1d8m+YMp1l7CryX5+1UuWFW3yCYHywIAAAAAAAC7j838pUsAAAAAAAAAYBdWVZdJcrmR5YdNOQu7r1nY0EMWPOz7Sf4zyQeSfGF2+1GS45Icn2TvJOdLst/sdvEk10pyzdl/XjHJnhufftdXVXdL8oos/vU4MsP34ENJPpfka0l+nOTEDAEyF0xyhSQ3TnL7JLfM+D/oep0kL6+qe3R3LzjXhswCZ/49yf4jyk9L8tUkX8rw/DsxQ5Ddfkl+LsmlpplyLV6a5LYj6k7N8PX4SpKjk5yc5EJJLpLhenP+BfueM8krq+ra3X3KgseOUlU3yfAaWOYPDp+U5ItJvpzh+X96kvMmuWySK2V+mNL5k7xJ4M/uobtPr6pfSfKxzA96u3aS5yR5+Cp6V9UlM7yOa0T587r7NavoO0ZV7ZnktRmC+OY5LcNr7otJjslPXnOXy/Ca23vB9ufIED56s+4+csFjR6uqGyd5c4ZrxCK+lORtST6Y5FMZrrVHJzkhyT4ZziGXT3KDDOfo22X81+AySV5XVbfo7pMWnGtDZt/z12WYfZ7O8Li/lOQHGfZ5+2TY510xw/d+zPN6V/D8JLcaUXdMks8n+XqSYzM8ry6a5BIZ9zo6s0smeXaS+5/5H2dhwGMCD0/P8L05Yy90aoZr/kWTXDnDdXwRz6+qw7v7uwseN5q97w69NNt0n8fWV1V7ZXjtjPGdCfr/RZLfWvCwU5O8N8P7ZB/J8Lr4Zob3I07NEGB98Qx7k5smuUuSqy+w/kOq6lPd/ZwF5/oZs1DTf8xiP+ucmuR9GR7f5zK833LG4zs+w37sjPdbzpfhZ+er5Sfvt1wj/oAAAAAAAAAA7DaEHgIAAAAAAAAAY11/gdrDpxqC3d6fLVD7n0mekeTw7j59J3WnZvgw/rfP9G+vP+O/VNX5k9wxyS8muUMWD+hIknT3SzOEhPyMqjo8Q9jJztyquw9fpvcqVNU1k7w840Nfjk/y90le0N2f20ndcbPb/2YISnhGVV0qye9lCPAaE8h0tySPTfLMkbOtyrOz8xCmY5O8Osm/JnlPdx9zdoVVdaEkd0ryq1kuUG9LqKqHJbnvTkp+mOSQJK9J8r7uPv5s1tkzQ+jHLyZ5WIYwkDGumiGIZMOhHzuYaf8kh2axUKbjMjzef8nwHDjxbNbeK8mBSX4pyQNy9kGaV8hwXmM30N3fmQUf/kfmnxceVlXv6O5Xb6Tn7LX3ygzBVPN8PMnvbqTfEh6d4dxwdk5P8qYM19u3d/ePd1RUVfskOTjJryT55YwP/9svyWur6rrdfdzIY0abBU6+LuMDD0/JcG3+u+7+yE7qjp/dvpnkiCR/VVUXTPLbSR6TIQxynhtlOLc+YuRsq/IHSQ7ayf0nJ3lDhnPtO7v7qLMrrKrzZtjL3S9DWOguqarumeTXdlLy1SQvy/Bc+sTZ7YOr6ooZwrUelSHYcoz7VdULuvvdszWuluQlO6k/KcNe6JAk7zq7vdAsSPoOSR6UYT80xv5JnpTkkSPrF2Lv+7N25X3erv6zGP/nBhkfkPfRVTauqgdnsX3P15P8VZKXdvf3d1J3zOz22Qx7mMdV1Q2S/FGG18gYz6qq93f3exeYb0cekvGhkl9L8hcZHt/Z/pw78/3Z7Qz/dcZ/mZ0PDsrwWO+S5Cqjp2VtuvuAHf17VR2c+X8I553dffBqJwIAAAAAAGBXscv+ciwAAAAAAAAAsOmuN7Luq939rUknYbdUVTdKcsMRpcckuXt33767/2tO4OFc3f2j7v6X7r5vkgsn+YUMAVS7jVlI0OszPmDin5NcobsfMyf0ZYe6++vd/agMgUQfHnnYU6rqyov22oBrJfmNs7nvlCR/nuSy3f3g7n7LvCCI7j6qu1/W3bfJEPizK7pczj6E5rgkj09yme5+RHe/4+yCcJKku0/r7nd19+9lCL44dIE5nlBV51qgfqwXJrnUyNpO8rwkB3T3Q2aPd4eBh0nS3ad29xGz5/1lkzw9w/NoRx6RIXyM3UB3vyPJn44sf3FVXW6DLZ+c5BYj6o5Lcu+dPa8ncJkkT93J/e9Ico3uvmt3v+7sAg+TpLtP7O63dvevJLliklctMMeVkvz1AvWjzEJ/Xp3k4iMPeVuSq3b3r88JPNyh7v5+dz85Q4jY20Ye9rBZmM1muVCGULsd6SQvyrDf+OXufs3OAg+TpLt/3N2v6u67J7nVimfdLOfLcD3akR8keWiSK3b3k7r7YzvbB3f3F7r7ORmeA3+e4Ws6xp8m//ec/efseH/YSf4hw/fnV7r7TTvbC3X3sd396u6+c5K7ZgjoHOOhVXXpkbWj2fvu0Hbf57FruPUCtWNfS3NV1fWSvGBk+UlJnpjkSt39l3MCD3eouz/U3XfNEAL73RGH7JnkH6pqkXD2HfmtkXUvzvD4/mZE4OFOzc4HR3T373f3VZNcOUNY5IbWBQAAAAAAALYmoYcAAAAAAAAAwFjXHVn3qUmnYHd23xE1JyY5uLv/bYoBZsFkb+nun09yjQwf9j9pil5bzDMzBJ3Mc0KS+87Cbb690abd/ZkkN8u4MKp9MoTMbZYLnM2/fzHJDbv7D7v7B8ss3N1fXX6stbpMhu/DWX0wQxDZn3X3sYsuOgsCuk+GQKYxLpjkAYv22ZmqukOSe44s/0GSn+/uR84L4NqRWfDTE5MclGRHz4XKEFDF7uPJSd49om6/JIdU1TmWaVJVt84QWjXGI7r7s8v02YDLJdlR0NXpSf4wye26+9OLLtrdX+vueyX5tYy/pv/6BOF/v5fkwBF1pyV5dHffobu/uNGm3f2NDIHOY4IcK8kLZmF3m+HcSfbewb9/L8ltuvuh3f31ZRbeha+1F0iy/w7+/Z0ZQjBf1N2nLrJgdx/f3X+Y5JEjD7lZVV0ryaOSXGcH9/8wyR1nob/fWGSW2TxvSHJwku+MKN8rycMX7TGCve/P2rb7PHYNs/3Nb44s/2p3f22FfV+SHV+PzuorSW7U3U9fRTB0d785yfWTjNnfXCXJY5ftVVXXzrifMZ7Z3b/R3ZO8D9Ldn+/uR2cIm390hq8pAAAAAAAAsE0IPQQAAAAAAAAAxrrSyDqhh0zlViNqHt/dH518kiTd/cnZh/0XDljalVTVzTMuXOLEDIFTh6yy/yws4r5J3jii/DZVdctV9l/QR5Ic2N0fX+MMW80bk9y8u7+y0YVmgUx/N7L81zba7wxVtVeS54ws/06SW3T3f260b3d/OEPw0ec2uha7tu4+LcN58Psjym+U5M8W7VFVF0nyioz7vdJ/6u5/WrTHRE5L8qDu/vPu7o0s1N0vSfKLGa5nYzynqlbye7hVdcUM4ZbznJ4hYO2vVtH3DN19enc/KuMC1H4u6w0c+2qSG3f3YWucYat5bYY92Hc3skh3Pz/J00aWPz3JU3bw799OclB3v22Ds3w+yc9nXBDpg6qqNtLvzOx9F7LL7/PYpTwoQ/jmGK9YYd/fT3KtEXVfzHB9+sQKe2cW7nvL7DgM/ax+t6rOv2SrMe+3fDjJE5ZcfyHd/ePu/qtV73kAAAAAAACA9RJ6CAAAAAAAAACMdamRdcKZWLmqOkeSq88pOy7JizZhnN3NnyeZFyRzepJf6e73TDHALPDrgRkX9PCkKWYY4ctJ7tjdR62p/1Z0WJJf6u4xgUVj/UGS/x1Rd6OqGhuKMs99klxlRN0JSX6xuz+5or5nhJzcMYnn1W6uu7+R4Tw4JtjvMVX1C2PXnoWFvTzJxUeUfzbJI8auvQl+v7tfvqrFuvs/kjx4ZPl1MgSTrcLTkuwzou73uvtVK+q5I49O8qERdU9cZcjcAn6Y5Pbd/eU19N6q3pPkPt19yorWe1qG1/k8d05y7rP820lJ7tDdY46faxYi/cwRpRdPcpNV9Jyx9x1nu+zz2AVU1WUzhK2OtZJw5qq6UIbn5TxHZfhZcEPhs2enu7+X5N5J5p3rz5/kd5Zsc90RNc/t7lOXXB8AAAAAAABA6CEAAAAAAAAAMF9V7Z9k35Hl35pyFnZbl02y15yad3T3sZsxzO6iqu6U5KARpX/U3a+Zcpbu/mGGoIfT55TeqqquNOUsO3BKhsChSUIudlHfSPLL3X3yKhft7uOS/O6I0soQFrgKjx1Z9/vd/YEV9fw/3f2lJL+26nW3o6rqLXL7tykeX3f/e5Jnj/lSJHlZVV1y5NJ/kOT2I+pOTHLv2etwK3hdd4/5eiykuw9J8tyR5b+30X5VdY0k9xpR+uLufs5G++3M7Jx9ryTz9lNXSHLbKWc5G7/e3QLWf+LYJL+6wsDDzNb60yUPf9wsqHCVnp3kxyPq7rSKZva+o22nfR5bXFWdN8kbk1x45CH/tsJrxR8kOe+cmk5y9+7+/Ip67rhJ9/uTPH5E6UOqapnPCV1hRM3rl1gXAAAAAAAA4P8IPQQAAAAAAAAAxhgbmpMk355sCnZnFxhR87+TT7H7GRPm9Pkkz5p6kOT/gh5eO6L016ee5Sz+doqwu13cI7v7+xOt/bqMu9bcbKONqupmSa41ovS9Sf5uo/3OTne/McmhU63PLuVxSd4/ou5CSV5ZVXvurKiqbpLkaSN7P2aCQLNlHZ3kEROu/7iM21dcp6putcFev5v5v8/7wwzBS5Pr7i8necGI0s2+1r62u1+3yT23ukfNgnFX7V8zvMYWcViSv1r1IN39o9k889x0RS3tfcfZFvs8tr6qulqGffY1Rx5yUsYFZ47pfZ4kDxtR+k/dfcQqeo7wN0m+Oafm0hkXZn1W895zOXYWxgoAAAAAAACwNKGHAAAAAAAAAMAY+y9Q+53JpmB3ds4RNcdNPsVupKqumOTgEaW/392nTDzOmf35iJq7Tz7FTxyT5Kmb2G9X8K7ufv1Ui3f3aUkOGVF6oxW0u9/Iuj/o7l5Bv515QpLNfK2xBc3Ot/fJuECyWyR50tndWVUXSPIvSfYasdZruvv5o4bcHM/q7smCtrv72Ozka3cWv7psn6o6X5J7jSh9anf/YNk+S/irJCfPqblTVZ1jE2ZJktOzSaGPu5AvJPnHKRbu7hOTvG3Bw5444XVwTNjlDatqQ78Xb+872nba57FFVdX5q+r3k3wwyTUWOPTpKwyDvU+S88ypOT7J41fUb67uPjnJc0aULnNOmPeei/dbAAAAAAAAgA0TeggAAAAAAAAAjHGuBWp9EJopjAl3uuTkU+xeHjyi5uPd/W9TD3Jm3f2hJO+dU3blqrrcZsyT5MXdPeb5uTt5xib0eO2ImitU1d7LNpiFJ/3SiNL3dvcRy/YZq7u/mOQ1U/dh6+vuryT59ZHlT6iqW5/NfS9JcpkRa3wlyUNG9tsMxyT5603o8/IkY4KT7l5VY8KZd+TeSfadU3NUkr9bcv2ldPc3krx6Ttl5ktxsE8ZJkn/r7i9sUq9dxd9NHLb7jgVqP9Ld8/ZmG3F4huDLnTlXkgM22Mfed5xtsc9j66mqi1bV3arqBUm+niHwc9418sxeleRPVjjSmHPCi7r7myvsOcbfJzlpTs3PL7HuvJ9pL+w1BwAAAAAAAGzUmL/OCwAAAAAAAACwSOjhvA9fwzKOGlFzu6rao7vnhaIwzl1H1Bw6+RQ79vYkN5lTc+sk/7AJs2xGj13JN5K8bRP6fHxEzZ5JrpjkU0v2uF6SC4+oe8GS6y/jBUnus4n92KK6+zVV9bwkj5hTukeSV1TVdbr7u2f8Y1X9Tsad509Jcp/u/tHSw67ey7v72KmbdPdpVfWiJH82p3S/JLdK8tYl2oz5Hry2u09ZYu2NenuS+82puXWSwzZhFtfan3Z8htDSKX1kgdrnTjZFku4+rqo+k+Rqc0p/LuOCSs+Ove9822mfx2rdrKpeuuAx+yQ5/+x28YwLYj4770rywFWFwVbVhZMcNKJ0088J3f3jqnp/klvspOyyVXWFWWj6WPPec9kjye2S/PsCawIAAAAAAAD8FKGHAAAAAAAAAMAYi4QenjjZFOzOjsoQvHSOndRcNMlDs7kBZNtSVV06ydVHlP7r1LOcjXeNqLnu5FMkn+5uQSs/7Y2bETza3cdU1VeTXHZO6SWyfBjOrUfUnJLkDUuuv4x3J/lOhvMdPCZDCNZ15tRdPMnLq+oO3d1Vdb0kzxrZ4/Hd/f4NzDiFl29yrz9NUnPqFg49rKpzzo6bZ3e/1h6dIfCNn3hPdx89cY/PLlD7lsmm+InPZn7o4SWXXdzed7TttM9jta4wu63Di5M8srtPXuGad8j8a///JvnvFfZcxLuy89DDZDgnLBJ6+K0RNU+oqrf4QxMAAAAAAADAsvZY9wAAAAAAAAAAwC5hzwVqffiZlevuU5J8YETps6rqllPPsxu47YiaT3b3IiEKq/S+JD2nZjOCX96xCT12NZsZTPXJETUbCQe8yYiad29C8NT/mQWMvHmz+rG1dfdJSe6d5NgR5bdP8gdVdd4khybZe8Qxb0nyl8tPOIlvZ9x+YCW6+xtJPjyidEx44VndJMm+c2qOSXL4Emtv2Owa/505ZZtxrX33ioOstoP3Tt2gu3+c5AcjSr/S3d+eep4kXxlRc5ENrG/vO8522uex6/t2kgd1929McJ0Yc054Y3fPe11O5T0jahY9JxwxouagJM+pqnmBkAAAAAAAAAA7JPQQAAAAAAAAABjjxAVqzznZFOzu/mtEzXmSvLWqHltVnovLu8GImv+ZfIqz0d3HJ5kXNHfFTRhlTDDE7uajm9jrhyNq9t/A+tccUbOO58CYkBN2E939uSQPH1n+tCRvyrjz4zeTPHCNYT5n57A1zDQm4PZaVbXXguuOudZ+qrtPW3DdVfrWnPsvUVXnmngG19qf9b5N6vP9ETWTBzDOHDWi5oIbWN/ed5zttM9j1/X9JE9NcsXuftlEPbb0OSHzr8/J4ueEMe+3JMnvJHl9VV1uwfUBAAAAAAAAhB4CAAAAAAAAAKOcsECtoDmm8qokY4KO9knyzCRfqqo/qqrLTDvWtnTdETWfnnyKnZsXxHPhqtp74hmOnHj9Xc1JSb68if2OGVGz1DWpqvZNMibI4/3LrL9BH1hDz11Cd9cWud1tkx/3K5K8ZETpXkluMaLu9CT37+7vbWiwaWxW2NuiPc+Z5EoLrrsdrrVJcqmJZ3Ct/Vmb9TWZF7KXJJ+cfIrBmFn22cD62+H1OPXed9vs89hlvT/J3ZJcvLuf1N3HTdFkFub7cyNK13lOWPn1ubu/lORDI8vvkuSzVfWKqjq4qmqRXgAAAAAAAMDuS+ghAAAAAAAAADDGIqGHGwmbgLPV3f+T5N8WOOQSSZ6a5CtV9b6qenxVXa+q/M7MfNcYUfOpyafYuXlBD5Xk4hP27ySfn3D9XdE3untMMOmq/HhEzbLXpMtleA7N89kl19+Iz2cIpYMz+62s7rz81O4+fEVrrdrH19DzEyPrrr7gutvhWpsM+60preM8u9X9YJP6nDSi5oeTTzEYM8tGAvC2w+tx6r3vdtrnsWu6YZJfTXLZiftcNcmeI+rWeU6Y6vr8tAVqz5Hk/kkOS/L1qvq7qrpzVZ13ib4AAAAAAADAbmKvdQ8AAAAAAAAAAOwSjl2g9oJJvj3VIOz2npTkTkn2XuCYSnLg7Pb0JD+oqvckeU+S9yb5UHcvEuy5rVXVfknGBBW8qmpMJtxaTRm48P3uPnnC9XdFmxXCdIYxr9tlf0duTGjQaUm+suT6S+vuE6vq60kus9m92bq6+/iquleSDyY51waWOjyLBd5sts+toedXk5yY+eFal1pw3UuPqHlWVT1rwXU329ThRt+aeP1dzUmbuG89dUTNZoUejpllqWu+ve9o22mfx65pjyT3SHLHqvr97n7uRH3GXJ+T5Htb/Jyw8Pmgu99QVe9LctCCh14iySNmt1Or6sMZ3m95T5L3dbdrOQAAAAAAAJBk+D9+AQAAAAAAAADmWeQDyhebbAp2e939Pxk+SL8R+ye5S5JnJHlXkqOr6gNV9eyquntVXWCjc+7iFg1t2so2Evw1z3cnXHtXdeK6B1ihMaGHR3X3aZNPsmPfWVNftrDu/mSS39nAEkcluX93n76ikVbtpO7e9GDt2dfjayNKx5w3kiRVde4k5192pi1mymvt8d193ITr74o2K2RwrK02zzLsfcfZTvs8dm3nSvK3VfV3VTXF52G2yzlh2fPBL2djf8hkryQ3TvKYJK9J8s2q+nJVvaKqHlZVV9nA2gAAAAAAAMAuTughAAAAAAAAADDGN5OMDcAResikuvsfkjxzhUueI8kNkzw6yWuTfK+q3ldVj6uqK62wz65iO72Gpwx+OXbCtXdV6woAnML5R9QcNfUQO/G9NfZmC+vuFyc5ZJlDkzywu7+54pFWaZ3P+zG9F7l+utaO41r7s05a9wBnsdXmWYbX4zjbaZ/H6r2su2vMLUMo3v5JDkhykyQPT/KiLB4q/4gkz1vlg5jZLueEpc4H3f2NJHdPcswKZzkgyf2TPD/Jp6vqq1X1/Kq6bVXttcI+AAAAAAAAwBYn9BAAAAAAAAAAmKu7T03y7ZHll51yFkiS7v6DJI9KcvIEy++Z5MAkf5rkc7MAxF+rqn0m6LUVnXvdA6zQlL8ftR1Cfjh7Y17vx00+xdk7fo292foemuQLCx7zl9395imGWaHvr7H3mJDT8yywnmvtOK61bAavR9hE3X1ad/+wu7/a3e/r7hd090OTXCLJHZN8cIHlfrOqnrriEbfLOaGWPbC7/zvJzZN8fnXj/JTLJHlYkv9M8vWq+vOqutxEvQAAAAAAAIAtxC82AAAAAAAAAABjfXVk3dUmnQJmuvuvk9wwyXsnbnVgkn9I8uWqekxV7T1xv3XbXcIdN+r0dQ/ApM45omaK0NWxBIFxtrr7x0nevcAhRyZ5/ETjrNKJW7z3ItdP19pxXGvZDF6PsAXMwhDfmuTGSX4940O+n1hVv7jCUZwTknT3J5JcN8mzk5w6YauLJvn9JJ+vqpdX1QET9gIAAAAAAADWTOghAAAAAAAAADDWx0fWXX3SKeBMuvsT3X3TJHfN9OGHF0vyl0k+VVUHT9xrncaEvQFJ76a92eKq6h5JHrzAIVdMctWJxlmlrR40ukhIkmstbB1ej7CF9OAfk9wqyY9GHFJJ/qmqLr2iEZwTZrr7uO7+3SQ/l+RFSU6YsN2eSX4lyWeq6o+raq8JewEAAAAAAABrIvQQAAAAAAAAABjrIyPrrlpVPiTOpuruN8zCD6+b5DlJvjZhuysk+a+q+qMJe6zTKeseALaAMQFn67zWLRKuxm6kqi6b5B8WPGyfJIdW1bknGGmV1vk7r3uOqDltgfVca2Hr8HqELai7P5DkFzNuX75fFt//nB3nhLPo7i9190OTXCrJbyc5IsnpE7U7Z5KnJHlnVV10oh4AAAAAAADAmvjrZwAAAAAAAADAWGNDD8+Z5MAk75xwFtih7v5Yko8leUxVXSvJ7ZPcPMlNklxoha0qyVOrap/ufsIK190KThhZd7vufvukk8D6nDiiZt/Jp9iavdmiqmqvJIckOf8Sh18lyXOTPHiVM63Y3mvsPSbkdMx54wxjr7W/0d0vXmBdYHH2vrBFdfe7q+pRSZ4/ovx2VfWQFVw3x54TztHdp26w1y6lu3+QYb/43Kq6UIb3W26V5KYZ9pK1wnY3yfDHJm7S3UevcF0AAAAAAABgjdb5V28BAAAAAAAAgF3L/yQ5fmTtraYcBMbo7k909190912TXCTJ1ZM8PMkrk3x1RW0eX1UPWtFaW8XY1/k+k04B6zUmWOPCk0+xNXuzdT09Q/D0sh5UVfdf1TATOPcW771I6KFrLWwdXo+whXX3C5L828jyP6uqC2ywpXPCCN19VHe/srt/o7uvluE9l3skeU6SDyZZRSDk1ZK8ehbsDQAAAAAAAGwDQg8BAAAAAAAAgFG6++Qk7xhZfscpZ2FL2tIfQu/Bp7r7Bd19/+4+IMmlkjwgyUuSfGsDy7+gqi69ijm3iO+MrDvfpFPAeo05J1yoqvacfJIdu+ia+rJFVdXPJ3nsCpZ6QVVdaQXrTOEiW7z3DxZYz7UWtg6vR9j6HpbkRyPqLpTkKRvs5ZywhFkI4uu6+zHdfaMk+yW5bYZQ7g8l6SWXvm2SR69ozK1iS79/BgAAAAAAAFMSeggAAAAAAAAALOLfR9bdqKouP+kkbDXnWvcAi+rub3T3K7r717r7EklumOTPk3xzwaXOmeSPVj7g+vzvyLpLTjoFrNeY0MM9kxww8Rw/o6r2yRDaCkmSqrp4kn9KUitY7jxJDqmqvVew1qrtX1XnWFPvMUGjowOUu/uYJEePKHWthenZ+8IW193fSfKEkeUP32CAs3PCCnT38d39ju5+YnffMMklkjw8yXuWWO4Pquq8q51wrXa5988AAAAAAABgVYQeAgAAAAAAAACLePMCtfeZbAqS5LQRNftMPsVPXGATe02iuz/U3X+Y5DJJ7p3kUwsc/uCq2hahB939oyTHjCi97MSjwDp9OUmPqPu5qQfZgSvF7/4xU1V7JHlFkouMKP9oku+PqLtekmdtZK6J7JHkcpvdtKr2zRDUM8/o0MOZr46oca2Fidn7wi7j7zPuZ/S9kjxlA33GXJ8T54SFdPe3u/sF3X2zJNdM8sqM+3krSS6Y5BFLtt5q750l2+D9MwAAAAAAAFiWX3wEAAAAAAAAAEbr7q8ledfI8t+sqr2mnGc3d9KImvNOPsVPXGoTe02qu0/r7n9Ncu0kfzXysL2S3GGyoTbf/4youfbkU8CadPdxSb4yovRGE4+yVXqydT0hya1H1B2d5J5JHjRy3d+pql9cdqgJrStotEbUfWnBdV1rYevweoQtrrtPS/IHI8vvXVXXWLLVp5OcOqLOOWFJ3X1kd98/yW0zLnQ2Se68ZLut9t5Zso3ePwMAAAAAAIBFCT0EAAAAAAAAABb1wpF1l0ly3ykH2c2dOKJmMz+4fbVN7LUpuvvU7n50kueNPOR2U86zyT4wouY6VXWOySeB9RkTgHTzyaf4WTddQ0+2oKq6eZInjSx/aHd/ubvflOQ5I495SVVdernpJnPDNfQcGzR65ILrjrnWXqqqLr7gusDi7H1hFzDbx4x5ve6R5ClL9jgx434OEES+Qd39X0numHEhkwdW1TLvcY1572zfqtpzibWXte3ePwMAAAAAAICxhB4CAAAAAAAAAIt6dZLvj6x9XFXtNeUwu7EfjKjZf/IpfuJ6m9hrs/1+km+MqDto6kE20ftH1Jwn6wl8g83y3hE1N6uq800+yUxV7ZHkFzarH1tXVV0wySuTjAloeWF3/+uZ/vcfJvnwiOP2T/LKTQ6BmWcdoZ9jru9Hdfe3F1x3zLU2GcKAgGnZ+8KuY2zg892r6tpL9hhzTrhFVe275PrMdPd7k/zdiNK9ktxgiRY/zrhQxQsssfbCZvvqa21GLwAAAAAAANiKhB4CAAAAAAAAAAvp7pOSvGBk+VWT/M6E4+zOvjOi5uqTT5GkqvbONg4A6e7jMgRLzXORDbQ5bUTNZv6uz39mXDDAPaYeBNbov0bU7J3kF6ce5ExunuSim9iPreulSS41ou7IJI868z9098lJ7p0hBGaemyV58mKjTepmVXXezWpWVZVxoYNjQlLP6kNJjhpR51oL07P33b1ttZ/F2InufmuSD44orYwPSDyrt4yo2SfCyFflhSPrFn7Ppbs7yXdHlG7K+2dJbpxk0/aya+B8CgAAAAAAwE75P4sAAAAAAAAAgGX8RZIfjqx9clVdesphdlPfGFFzrcmnGNwmyXk2qde6vGNEzT5VtezX4eQRNXsvufbCuvsHSd49ovT+VbXv1PPAmnw44wLJHjb1IGvqxRZVVY9OcucRpccnuXd3n3DWO7r7i0keOrLl46vq1guMOKW9s7kBQwcludiIujEhqT+lu09L8sYRpXewl4Zp2fvu9rbUz2KM8pSRdXerqusssf5/ZthHzTN2L8VOdPenknx7ROmFl2yxld4/u+sm9VkX51MAAAAAAAB2SughAAAAAAAAALCw7v5RkmeOLD9vkkOr6hzTTbQaVXVgVd1v3XOM9NkRNdedfIrBQzapzzp9c2TdfkuuPyZQ4XxLrr2sQ0fUnD/Jb048B6xFd5+e5NUjSm9aVTebep6qunySe07dh62tqm6Q5Bkjy//fLERmh7r7kCT/MGKdPZK8oqqWDZpZtV/fxF5j9zhvX3L9MdfaPZP83pLrA+PZ++6+tuLPYuxEd/97hoDyeSrJk5ZY/4QkbxhRetuqut6i67NDY95zWfb9li3x/tnsfdFfnbrPmjmfAgAAAAAAsFNCDwEAAAAAAACAZf1Nkq+MrD0oyV9ON8rGVdU9kvxXkouse5aRzjbE6EwuV1XXmXKIqrpKkrtN2WMXM+YD3jty1Iiaiy659rJekeToEXVPqKr9px4G1uRfRtb9eVXVpJMkf5pkywcIM52qOl+SQ5LsPaL80O5+8Yi638m4PcXFk/zTJjzPx7htVV196iZVddEk9xlR+unu/uSSbf4jyedH1D2sqq60ZA9gHHvf3ddW/FmM+Z46su5uVbVMoN3fjaipJH+xxNosZ9n3W8bsde9SVXstuf5YD05ysYl7rJvzKQAAAAAAADsl9BAAAAAAAAAAWEp3H5/k15L0yEN+u6qeNOFIS6mqParqj5O8Ksm51j3PAj6a5OQRdfebeI5nZff4HZRLj6g5JckxS67/3RE1l1ty7aV093FJ/nFE6QWTPH/icWAtuvtdSY4cUXqTJI+cao6qukuSe0+1PruMFya5woi6LyV56JgFZ/u5eyc5YUT5HZI8dsy6E6skT9mEPo/LuL3h2HDUn9HdneS5I0r3TvKyqtpz2V7Aztn77ta23M9izNfdb8jwvsgYC78X1d1HjFz/VlX1iEXX52eMec/l+0uu/f4RNRdKcvsl159rFl7+x1Otv4UcleT0OTX7V9V+mzEMAAAAAAAAW8/u8AvnAAAAAAAAAMBEuvuwLBb48OStFHxYVZdK8l8ZgnN2qd+j6O4Tk3xgROkDquo8U8xQVQ9Icucp1t6Cxnz4/QvdfdqS639pRM11l1x7I56Z5NgRdfeqqsdMPQysyV+MrHtmVd1w1c2r6vIZF8LENlZVv5FxwZenJLl3d48O4e3uI5M8amT5n1TVjceuPaF7VtXtplq8qq6VZEyA0alJXrrBdi9K8rURdQclec4GewE7Z++7e9qqP4sx31NH1t21qpb5Ho4NqfvLqrrpEuuTpKquk+TCI0o/s2SL92XYI8/z8CXXH+M5SS454fpbQnefkuTrI0qvM/EoAAAAAAAAbFG71C/rAwAAAAAAAABb0u8n+fgC9U+uqkOrat+pBpqnqvaoqocl+USSW65rjhV484iaiyVZedBkVV0/yd+vet2d9HtoVd18s/qdpfd+SX5lROn7N9DmsyNqDqyq826gx8K6+9tJ/nxk+V/MQrnWpqrOW1WPr6rLrHMOtp1XJvnciLpzJXlDVV1tVY2r6pIZzvUXWtWa7Hqq6upJ/npk+R9294cW7dHdL0zyryNKz5HkkNm1cd1eVFX7r3rRqtonycsyPNZ5Du3uMYGFZ6u7T0jy+JHlv11VT9tIv42qqr2r6hFThLzCutn77ra25M9ijPL6jH8/6smLLt7db0ryjhGl+yR547qvjVV12ar6syWOu3ZV/dZsD7QOvzWi5oQk/7PM4rO91mEjSu9cVXdapsfOVNXDk/zaqtfdwsacU8f8YREAAAAAAAC2IaGHAAAAAAAAAMCGdPdxSe6S5NsLHHavJB+rqltPM9XZq6qbJflgkucnucBm91+xV42se1RVHbiqplV13SRvyxDwtVlukuRdVXVYVd25qjbz917+IckFR9SNCaE8Ox9Ncsqcmn2SPHgDPZb1F0k+PaKukrywqv6kqvaceKafblx1oap6YpIvJ3l6krWFqrL9dPcpSX53ZPnFMpyrbrvRvlV1vSRHJPm5ja7FrmsWEv2vGXfNfXOS52yg3UMznEfnOSDJizfQZ1Uum+Rfq2rvVS1YVZXhun+dEeWnZ3w42jz/nOS/RtY+sar+sao2cx92Rrja7yT5QpK/y7i9EeyK7H13P1v5ZzF2ors7ydgw4F+c7a8X9VsZAvfmuUCSw6rqHkv02JCqumpV/X2GoPZlnqcXSPK3Sb5UVY+tqk17r6yq7pLk10eUHjYLL1zW2PfP/raqVhY4X1UPTvLcVa23i/jAiJoHbfZeFgAAAAAAgK1B6CEAAAAAAAAAsGHd/bUkd824D4Kf4UpJ3lFV/1xVV5tmsp+oqjtU1TuTvDvJMh9033K6+wtJ3jmidK8kb62qG260Z1XdL8PXcF1BNwcneWOSz80+jH/xqRpV1T5V9U9J7jmi/Kgkb1i21+zD+x8aUfonVXXzZfsso7tPTHK/JCePPOQJSY6oqmtON9Wgqm5SVS9O8rUMYRcCmJhEd78pyb+NLL9gkrdV1XOXCcyoqvNU1dOSvC9DuNzPjJNxYUxsD3+TZMw+6ZtJHjgL/1lKdx+d5D6ZH/yUJL9UVQ9bttcK3SbJa6pqn40uNAtUflGGa94Y/9jd/7PRvsn/hTY9MMkPRh7y4CQfmYV5T6qqrl1Vz0ny9SR/neTSU/eEdbL33f1s5Z/FGOW1SY4cWfvkRRfv7s9kfAD6uTPsS/6xqi68aK9FVNXeVXWPqnprkk9mCK/eaBD0xZM8M8nXq+pFVXWTjc65M1V17ySHjCz/xw22+9ckPx5Rd7kM71VuKPiwqs5RVc/MMPfu9rmtd42ouUSSf1jFHh4AAAAAAIBdy+72f54BAAAAAAAAABPp7g8kuVOSYxc89H5Jjqyq11fV3arqnKuaqaquUFV/VFWfSfKWJLdY1dpbyLNH1u2X5O1V9VtVteeiTarq6lX1hiT/nOGD/Gf1+UXX3KArZPgw/teq6m1V9ZtVdclVLFyDu2UIvnjAyMP+prtP2mDrfxtRc94kh1XVC6vqplW11wZ7jtLdH0vy/xY45MAkH6uql1XVykJGq2qPqjqoqp5eVV9I8p4kv57EB+XZDA/JECw3xh5JHpnkK7PX6613dn2rqr1mQUbPSfLVJE/M2YeWPD/JBxaYm11UVd03wzluntOT3L+7j9poz9l+7nEjy5+zGSFfI9w5yXuq6grLLlBVF03yHxn39U6SH2V4na5Md389Q/DhaSMPuUqSd8/20Ldc5SyzoMMnVNXHk3wsyaOSnG+VPWArs/fdLf3biJq1/CzGzs2Cg586svwuVXX9JXo8P8mhCxzy4CRfrKqnVdWlFu13dqrqXFV156r6+yTfSvKaJD+fpFbVY2bfDD/7vKeqPl9Vf1JVN66qlfSpqivO/sDEIbNe83wuyes30rO7j0ny4pHl10ry3qq6/TK9quoOGfZPjz2bks1+/2yzHZ5hrzzPfTOEeD+8qi4y6UQAAAAAAABsGf6PdgAAAAAAAABgZbr7sKq6XYaAwfMvcGgl+cXZ7eiqemuS985un+zuE+YuMAT5XTbJ9ZPccna7xkIPYNf0xiT/nSFoY57zJfnbJL85+5D8m7r7K2dXPPtw/m0zfBD5djn7D9J/P8mvJXn3+LFXZs8kt5/dUlWfTHJEhufOJ5J8prtPnLdIVZ07ybUzBHfePclVF5jhq0n+YrGxd+hfkjw983+nZ88kvzG7nTwL9fxmhsDR45P0nOM/093PWHS47n5BVV0m48Ow9kjyq0l+dRaa9Pok70jy0e7+8ZgFZkGWV01yvSQ3nd0uuOjssArd/f2quk+S/0wyNqD33PnJ6/Wkqvp8kq8k+XGG1+p5klwmyZUzLvDjS0n+MMO5nLOoqnnnv3W43M6utWenqq6Y5O9Hlv9Jdx++aI+deHaSWyf5hTl1+yQ5tKpu0N3Hr7D/2fnvDHuZq+3gvutlCNF+dpK/7u7vjlmwqs6X5GFJnpDFQv0e1t3fWaB+lO5+U1U9IuO/98lsDz07v7w+Q3jjh7v7B2MOnoXMXDXJdZLcJMnNklxikblhO7L33e1s6Z/FmOvVST6VHe8RzurJSe6yRI8HJrlokoNH1p83Q0Dy46rqsAzv3bw7yZHdfcq8g2fvcV0+w2O6UYZr9I2TnGvhyTfmihn2SU9I8r2qOiJDAOuHM7xf9715C1TVHhkey20zfO1vn8U+y/So7j510cF34BkZgmPH7PmulORtVfXmJK9I8tbu/uGOCmdhkFfN8H7SA5LsLBT8PUlemuRF48fetXT3yVV1aJLfHFF+1STPS/K8qvp2ks8mOTrJcUlOHnH8M7r7M0sPCwAAAAAAwKYTeggAAAAAAAAArFR3/3dV3SLJazJ8SHhR+yW59+yWJKmq72QIifphhiCB45OcI0OY1HkyBLNcbvZvu5Xu7qr6nSTvyxDAMMY1MgRm/W1VfTnJt5J8N0NQw75JLpLkCkkuPmKt0zOEe3x9wdGncvXZ7YwPV59eVd/KEETxnSQnJDkxyd4ZnjvnzfDcuVTOPtRxZ05Jct8xwZzzdPfXqupfMnxIfqy9k1xrdhvrnRk+7L+w7n58VZ03yW8teOi1Z7c/TtJV9fUkX8vwvDshw9fxnBkCtC6Q4bl3iQyvcdgyuvvdVfXADME4i54zzpnh/LtsIO+Pkty5u3885GrMdfqSfVizqto7ySEZrlHzvCvJU1fZf7a3eFCSj2V++N1Vkzw3Q/jx1E6a9XlvhnCxs9onyeOT/G5VvSXJ25J8NENY6NEZgqjOm+SADNfN22cI3jnPgnO8rLsPXWL+Ubr7hVW1X5JnLnjolZL83uyW2f7nf/OT/c/JGfbK+2TYb198dttvNZPD9mPvu/vYFX4W4+zN9i5Py7BHn+fOs8DmDy3Y46SqumuGP/JxkwUO3TND2N9tZ//75Kr63wznhB9mOCd0hnPCuZJcKD+5Ru+9yIyb4MIZ/kjE3c/4h6r6YZJvZHhf6ccZHk/yk/frLpZhjzI2NP6sntvdb1l24DPr7u9W1VOS/OUCh/3C7HZqVX0uw3n8u0lOzfAYL5UhGHLMfup7Gf6oyG0WmXsX9ewkD8n49ymT4blysQX7vDSJ0EMAAAAAAIBdiNBDAAAAAAAAAGDluvt/quoGSV6c5JdXsORFZ7epfTPJBzahz0p19wdnH9xeJvTocrPbsn6zu99cVQdsYI0p7ZHkkrPbqnWSh3T3+1a45uOS3C3jgq7Wort/u6q+m+VDtirJpWc32OV096FVtW+SF2bzfgfvhCT37O5PL3DMSVMNw+SemeT6I+q+n+R+3X3aqgfo7u9V1f2TvCM7Dhg8swdX1du7+5WrnmMHc72/qp6U5Gk7KTtnhmvp3SYY4Yj8JFh5Mt39rKr6XpIXZfnzzBmBScAG2PvuVrb8z2Ls1L8meVKSq4yofXKSOy/aoLuPqarbJjk0Q3DyMvbOEJJ3xSWP32ouMLstG+y+M29J8rsrXvOvktwxPwmhHGuvJFeb3ZZxTJI7zAJWl1xi19Hdn6uqv0ny6HXPAgAAAAAAwNYy7xfRAAAAAAAAAACW0t3HdPe9kjwkQyjPVnZykj9P8nPd/d51D7Okpyd51Sb26yS/290v3sSeW8nJSR7c3f+0ykW7+xtJHpTk9FWuu2rd/bQk90ryozWPAmvR3S/JEIrz401o9/0kt+7u/zrTv+094rgTJ5qHCVXVXZL8v5HlD5pdNybR3Ycn+ZOR5S+oqk0J7+nuP0ny8s3odRafTHLX7t6UQNHufmmS2yeZ7HsMjGPvu3vYVX4WY8e6+/TsPBT5zO5UVTdcss8JSe6e4T0Yz5XpvD7JPbr75FUuOnue3DfJZ1e57hzHJrlLd39kE3tuBY9P8t/rHgIAAAAAAICtReghAAAAAAAAADCp7v6HJFdO8oJsvQ+En5zk+Umu2N1/2N3HrnugZc0+uH3/JP+2Ce1OTHK/7n72JvTair6W5Dbd/bIpFu/u1yb5lWzxwLLuflWSayf5r3m1m+yk2Q0m1d3/nuQ6mTbI4YgkN+7us/bYZ8SxW/ocws+qqksneenI8r/q7jdNOM4ZnprkXSPqzpvkkKoaE8i5Cg9J8upN6pUkH0hyi+7+wSb2THcfluRaSf51M/uOcFqS49Y9BGwme9/dw67ysxhn65AknxtZ++Rlm3T3ad39xCS3SvKlZdeZyNHrHmCDTsuw/7x7d0/yOuzuo5LcNpsTfPi/SW7a3WP209vK7Pv3C0kOX/MoAAAAAAAAbCFCDwEAAAAAAACAyXX3D7r74UmuluRFWX+AwHeTPCPJFbr7Ed39tTXPsxLdfUqSeyb5s0wXMHlkkpt39yETrX92jsj6wwROzRCSeY3uPmLKRt39L0mul+SwKftsVHf/b3ffJsPz7gtrHue/kzwiycW7+8trnoXdRHd/KcnNkjw0yTdXuPR3kvx2hpC1L+7g/vOPWOP7K5yHiVXVnklemWT/EeUfTvIH00406O7Tktwv455P10/yzGknGnT3yUnuneSvN6HdK5LcerMDD88w20ffO0Ow0kfXMcOZ/E+Sxya5dHe/e82zwKaz99097Co/i/GzZn8M4k9Glv9CVd1og/3eleF9rj9IcsxG1tqgUzL8AYx7JLnmEsd/PUPI9akrnGkZH0pyYHc/qbt7ykbd/fUkByZ5y4RtXp/kRt39iQl7bGnd/cMkt0vye9n1AzkBAAAAAABYAaGHAAAAAAAAAMCm6e7PdvdDk1wmyRMzBOhtlhMyfAj8PhmCWh43+5DzttLdp3f34zOEcH1shUsfneSPklyvuz+0wnVH6e4Xd/cVklw1w4elD8vwwf7NcEKSlyS52iwkc1PCDLr709196yQ3mfXfsgFm3f3aDGEP903y3s1qmyH86U+S/Fx3H9Tdz599qB42TXef1t0vSnLFJL+WIYRoqaWSfDDJw5Ic0N3P3UnYx8VHrPedJedgPZ6S4do9z4+T3HsW+rcpuvsbSR40svz/VdVdJhzn/8z2PI9KctesNnT0DN9L8sDufkB3HzfB+gvp7sMzBEveKcnbMpwzNsOnkzw7wx7wWt39F939rU3qDVuSve/2tyv9LMbPeGXGh5I+eaPNuvuk7n5mkstmeK9is0JIT0zyXxmC0i/e3Xfv7tcts0fs7i909y2TXDjDe2YvT3LUSqfdufcluWd333Az32/q7h919y8keXCSb69w6S8luVd33627d/ufybr71O7+ywzvBT8qw8/Lm7WPBQAAAAAAYIvZa90DAAAAAAAAAAC7n+7+XpKnJ3l6VV05yS8luXWSGyY536raZAhpeWeSdyR561YIrNks3f2+qrpekjtnCNC6XZJzLLHUpzKEPLxws8L+dqa7P5PkM0n+sqrOk+RGSQ6a3Q5McsEVtToxyeFJXpfkVesME+nu9yV5X1XtmeRaGR7r1TIErF0iyYWSnDfJObPc93hVc56S5JAkh1TV1ZLcI8ndklwvSa2ozeczBDu8Pclh3S18hC2ju88ISH1JVV0iw3n3wCRXSXL5JPslOXeS0zIE1x2TIRDjkxlCat/a3XPDNqpqjwyBEfPs9gEbu4qquk2Sx40sf1h3f3HKeXaku99UVc9J8ugR5S+pqutsVrh0d7+hqt6Z5DFJHpmN7wWOTvL3Sf60u4/e6HyrNAtCfXOSN1fVAfnJtfYmSfZcUZuvZbjWviPJ2wUcwo7Z++4edpWfxfiJ7j6tqv4kyUtHlN+xqm7c3e9fQd8fZXif4jlJbp7k7kl+McnlNrr2zKkZgk/fMbsd0d0nrmjtJP/3GA5NcujsZ46r5yfvtxyU5MpZ3fntyCRvSHJod39iRWsupbtfWlWHZAg//PUMIdOLOi3JEUmen+TV3X3aCkfcFmbvKf51kr+uqgsluWmGa+aVkhyQ5CIZ9vH7JNk7yR7rmRQAAAAAAIAp1dn/IWgAAAAAAAAAgM01+1D11TJ8wPgKGT70ekCSi2UIitp3dtsryUkZgumOTfLtJN/KENLy6QwBUp/o7h9s6gPYwqrqgkluleGD6lfP8MH7C2f4uu6Z4et4TIav4SeTfDzJ27r7C2sZeElVdfkMH8S/4pluF88QRHHeJOfJ8Bw6LcNz6MdJvpvh+fPFDIGKH0ryke4+ebPn346q6nxJbpAh1PTKGcLaLpPk/EnONbvtkZ+8pn+Y5HsZvidfzvB9+WSSj81CGGC3VlVXzBCCtDNf6+4xwYiwJVTVwUkOm1P2zu4+eM46+yb55QwhQ7fPcN0f44Qk70ry2iT/vKsFZc8e9/UyXGuvkp9cay+Y4Tq7b4Zr7ckZrrU/SnJUhj30lzMEsH4qyUdn4eTAkux9gTObhaHfKMN1+vJJLpvkkhnen9g3Q8Db6Rmu0ccn+X6Gc8LXM1yfv5Dkf5IcueqQw0VV1QXyk9DPM26XzXB+O8+ZbmfsOU7IsN/4TpKvJPlckk8keW93H7W50483e1/p4AwB9lfO8L7kBTJ8v07P8D7S0Rm+P0cm+XCStwimBQAAAAAAgPmEHgIAAAAAAAAAAMAuoqrum+SVc8re1t132Ix5YBVWFXp4ljX3yhBUc/UkV0qyX4aAob2THJch7PmMsL+PrztICAAAAAAAAAAAYDvba90DAAAAAAAAAAAAAKPdZkTNJyefAra47j41Q6Dhp9Y9CwAAAAAAAAAAwO5uj3UPAAAAAAAAAAAAAMxXVXslucuI0vdPPQsAAAAAAAAAAADAWEIPAQAAAAAAAAAAYNdwpyQXGVF3+MRzAAAAAAAAAAAAAIwm9BAAAAAAAAAAAAB2DY8bUXNkd3938kkAAAAAAAAAAAAARhJ6CAAAAAAAAAAAAFtcVf1ykhuPKP3XqWcBAAAAAAAAAAAAWITQQwAAAAAAAAAAANjCquqiSf52ZPm/TDkLAAAAAAAAAAAAwKKEHgIAAAAAAAAAAMAWVVXnSvK6JBcdUf6O7v7CxCMBAAAAAAAAAAAALEToIQAAAAAAAAAAAIxUVc+qqstuUq/zJ/mPJAeNPOTPppsGAAAAAAAAAAAAYDlCDwEAAAAAAAAAAGC8Byf5XFX9Y1Vdc6omVXWLJB9LcrORh7y7u98x1TwAAAAAAAAAAAAAyxJ6CAAAAAAAAAAAAIvZO0P44Seq6oiq+o2q2n8VC1fV9arqVUnemeSyIw87JcnDV9EfAAAAAAAAAAAAYNX2WvcAAAAAAAAAAAAAsAu76ez2/Kp6T5L/SvLeJB/r7u/NO7iq9k1y7SS3TXK3JNdbYoY/6e5PLnEcAAAAAAAAAAAAwOSEHgIAAAAAAAAAAMDG7ZnkFrNbkqSqfpDkq0m+neTYJCdm+L29fZJcOMmlklwmyR4b6PumJE/bwPEAAAAAAAAAAAAAkxJ6CAAAAAAAAAAAANPYf3abynuT3L+7e8IeAAAAAAAAAAAAABuykb8ODQAAAAAAAAAAAKzHYUl+vruPWfcgAAAAAAAAAAAAADsj9BAAAAAAAAAAAAB2HZ3kz5PcvruPXfcwAAAAAAAAAAAAAPPste4BAAAAAAAAAAAAgFE+luT/dfe71j0IAAAAAAAAAAAAwFh7rHsAAAAAAAAAAAAA2IXcJsnTk3xmE3t+KMkDklxf4CEAAAAAAAAAAACwq9lr3QPAVlNV50xy5SSXSnLeJPsmOT7Jj5N8Pclnu/vk9U249VTV3kkuk+SySS6Q4WvWGb5mP0ryhe7++toG3ICquliG58MFkpxv9s/HJPlhks9197fXNduytuNjAgAAAAAAAADYLN398SQfT/LEqrpGkjskOSjJgUkuscJWn03yxiSv6u4PrHBdAAAAAAAAAAAAgE0l9BCSVNWBSe6W5I5Jrp5kz52Un1ZVn0zy5iSv7+7/nn7CraWqKsMvad89yS2SXDfJ3nOOOTbJe5O8Nclru/urU8+5jKo6f5J7JvmFJLfKEAy4s/ofJDk8yb9neFw/mnbCxW3HxwQAAAAAAAAAsBV095FJjjzjf1fVZZJcL8nlkhyQ4Y+IXjrJfhn+kOgZtz2TnJjkhAx/rPLrs9vnk3wkyYd31T8yCgAAAAAAAAAAAHBW1d3rngHWpqruk+SxGX7ReFkfTvKs7j50NVNtXVV1riS/luTRSa6wgaVOz/BX6J/Z3e9dxWwbVVWXSvK4JA9Mcu4llzkuyUuTPGMr/NL5dnxMAAAAAAAAAAAAAAAAAAAAAAAA7FqEHrJbqqqrJPn7JLdY4bKHJ3lYd392hWtuGVV1zyR/meGvz6/SK5L8bnd/d8XrjlJVe2QIcXxykvOsaNljkzwpyV919+krWnO07fiYAAAAAAAAAAAAAAAAAAAAAAAA2DUJPWS3U1X3SPKyrC4M7syOTfKr3f26CdZei6o6d5K/S/LACdt8M8kvd/d7J+zxM6pqvyT/kuSOE7X49yT37+6jJ1r/Z2zHxwQAAAAAAAAAAAAAAAAAAAAAAMCuS+ghu5WqemSSv01SE7bpJL/V3c+bsMemqKoLJXlLkhtsQrtTktynu1+7Cb1SVRdOcniSq03c6pNJDu7uoybusy0fEwAAAAAAAAAAAAAAAAAAAAAAALs2oYfsNqrqgUlekmkDD8/QSR7U3f+0Cb0mUVUXSHJEpg/QO7NTktyzu984ZZOq2i/JYUmuO2WfM/loklt199FTNdiOjwkAAAAAAAAAAAAAAAAAAAAAAIBdn9BDdgtVdaMMAX7nGFH+3iSvnP3nV5L8OMl5k1w+yU2S3D/JjUesc3KSm3X3B5cYea2qau8kb09y85GH/E+S18+O+UaSb8/+/WJJLpnktknumuSaI9Y6LsmNuvtTi8y8iKp6XZK7jSj9UZJ/SfLmJB9LclSG0MwLJblOkjsluU+S/Uas9bruvsfCw460HR8TAAAAAAAAAAAAAAAAAAAAAAAAuz6hh2x7VXW+DOFul5tT+vkkD+/ud4xY8/ZJnpfkCnNKv5zkOt19zIhRt4yqemaSx44o/WyS/9fdbxu57h2T/FWSK88p/UySG3T3cWPWXURV/U6Sv55T1rOap3T3j+asd4EkT0ny2yPa/053/+2YORexHR8TAAAAAAAAAAAAAAAAAAAAAAAA24PQQ7a9qvqrJP9vTtnbk/xSdx+9wLrnT/LaJLeaU/qc7n7M2HXXraoOSnJEkj3mlL4syW909ykLrr93kn9I8itzSp/V3b+/yNojel8yQ1DjuXdSdnKS+3T36xZc+5eSvDLJOXZSdmySn+vuby6y9py+2+4xAQAAAAAAAAAAAAAAAAAAAAAAsH3MCzWDXVpVXS3JI+eUvS/JXRcJPEyS7v5Rkrsk+cCc0t+uqqsusvaaPTvzzw0vSPLgRQMPk6S7T07ywCQvnlP6qAm+bs/KzsMBO8l9Fw0HTJLufnWS+88pO89shlXajo8JAAAAAAAAAAAAAAAAAAAAAACAbaK6e90zwGSq6tAk99pJyQ+SXKe7v7aBHpdN8rEk599J2SHdfd9le2yWqrpDkrfMKXtfkpt392kb7LXXbK0b7KTsVd29s+/fIv2uluTIJLWTsr/o7sdusM9zkjxqJyWd5Grd/ZmN9Jn12naPCQAAAAAAAAAAAAAAAAAAAAAAgO1lj3UPAFOpqssnueecsiduJPAwSbr7q0meNKfsl6vqgI302SS/Nef+E5I8aKOBh0nS3acmeWCSk3ZSds+quvJGe808NjsPB/xqkj9aQZ8nJPn6Tu6v2SyrsB0fEwAAAAAAAAAAAAAAAAAAAAAAANuI0EO2s0cm2XMn938+yQtX1Ot5Sb60k/v3nM2zZVXVBZPcYU7ZS7r7c6vq2d2fSvLynZTskeRRG+0ze2z3m1P2x9194kZ7dffxmR+C+StVtf9G+mzHxwQAAAAAAAAAAAAAAAAAAAAAAMD2I/SQbamq9kxy3zllz+nu01bRr7tPTfI3c8ruV1Vb+TV35+w8JDJJnj9B3+fNuf8+VbX3BnvcO8nO1vhGkn/ZYI8z++ck39nJ/Xsn+eUN9tiOjwkAAAAAAAAAAAAAAAAAAAAAAIBtZisHsMFG3DrJxXdy/4lJXrHini9LcvJO7r9EkoNX3HOVbjnn/g9195GrbtrdH03y8Z2UXCBDIONG3H/O/S/t7lM22OP/dPdJGZ4POzNvpnm242MCAAAAAAAAAAAAAAAAAAAAAABgmxF6yHZ1lzn3/3t3/3iVDbv7R0neMqds3lzrdMM5979nwt7z1r7rsgtX1QWTHDSn7JBl19+Jf5lz/02rav9lFt6OjwkAAAAAAAAAAAAAAAAAAAAAAIDtSegh29Vt59z/7xP1nbfu7SbquyFVVUmuPKfsvycc4f1z7r/9Bta+TZLayf3/291HbmD9HerujyX5xk5K9khy6yWX346PCQAAAAAAAAAAAAAAAAAAAAAAgG1I6CHbTlVdPMlV55S9faL2/znn/qtX1cUm6r0RF0uy95yaT03Y/5Nz7r9YVV1zybVvM+f+qZ4LY9aeF855drbjYwIAAAAAAAAAAAAAAAAAAAAAAGAbEnrIdnSjOfd/rbu/NkXj7v5Kkm/NKbvhFL036MIjan44Yf8xax+45Nrzng/vWXLdMd475/5lnwvb8TEBAAAAAAAAAAAAAAAAAAAAAACwDQk9ZDu63pz7PzJx/w/Nuf+6E/dfxr4jan40Yf8xa8/7vv6Mqto7ydXnlE35fJj3XLhGVZ1jkQW342MCAAAAAAAAAAAAAAAAAAAAAABg+xJ6yHZ0nTn3f2Li/vPW34qhh2NC6k6YsP+YtRcOPcwQDrizx3Zakk8tse5YRyY5fSf3753kaguuuR0fEwAAAAAAAAAAAAAAAAAAAAAAANuU0EO2oyvPuf/zE/f/wpz7rzRx/2WcNKJmvwn7j1l73vd1mWO+2t0nL7HuKLO1vzanbNHnw3Z8TAAAAAAAAAAAAAAAAAAAAAAAAGxTQg/ZVqqqkhwwp2xeKOFGzVv/gIn7L+O4ETXnn7D/mLXPX1X7L7ju5ebcP/VzYUyPeTMuWr8rPiYAAAAAAAAAAAAAAAAAAAAAAAC2KaGHbDcXTbLPnJpvTjzDvPXPXVUXmXiGRX1nRM2FJ+w/9utx+QXXPWDO/VM/F8b0WDQg8IAN9luFVT8mAAAAAAAAAAAAAAAAAAAAAAAAtimhh2w3lxhR8+2JZxiz/pg5N013H5XkhDllN5hwhLFrL/p1m1c/9XNhTA+PCQAAAAAAAAAAAAAAAAAAAAAAgG1L6CHbzQXn3H9Md5805QDdfXySY+eUzZtzHT495/4DJ+x945F1i37d5tV/d8H1lvGdOfd7TAAAAAAAAAAAAAAAAAAAAAAAAGxbe617AFix/efcf8ymTDH0Oc9O7p835zp8OMn1dnL/rapqz+4+bZVNq+ocSW45snzRML2t8HyY12PR58J2fEyTq6pHJnnEJrS6cpI9Z//9tNkt+f/s3Gms7Vddx+HvKqUt0NIW0AKlDJZSuBfFUkocELBCQEIgZaoWGZQYIIgvRF5oRMD4QmMMECUgMogUAlqGYJBBZJYAymjuZaxAC8rUwXDpAK0/X5xDjM1l/88+Z//2Offf50l2mty19hp2V2/ffZLvJ7l0DfsDAAAAAAAAAAAAAAAAAAAAAAB712lJjjnMn19ZVbdd92HWRfSQuTl5Yvy7aznF9D57LgqX5P1JfnPB+O2TPCLJm1e876OSnLLFucv+bnvhPaz6LczxTuvwY0n2rXnPo/N//589dhf2BwAAAAAAAAAAAAAAAAAAAAAAjgwn7fYBOh212weAFTtuYvx7azlFcmhifOqcu+EdSa6fmPNbDfsus+ayv9teeA+rfgtzvBMAAAAAAAAAAAAAAAAAAAAAAAAzJXrI3BwzMX7dWk4xvc/UOdeuqi7LRvhwkXPHGBesas8xxpOS3G+Jryz7u+2F97DqtzDHOwEAAAAAAAAAAAAAAAAAAAAAADBToofMzV4Iwm1ln70ahXvpFub85Rjj1J1uNMa4Y5IXLfk1gcB53gkAAAAAAAAAAAAAAAAAAAAAAICZEj1kbqbe9PVrOcX0PjdZyymW97YkH5+Yc3KS925GC7dljHHnJO9JcuKSX132d9sL72HVb2GOdwIAAAAAAAAAAAAAAAAAAAAAAGCmjt7tA8CKXTcxvq43P7XPD9ZyiiVVVY0xfjcbQcKxYOoZST44xvi1qvrgMnuMMR6Q5MIkd9jGEa9dcv51SW66YHwd72HVb2GOd1qHbyc5uIZ97p7DhCmPPfbYnH766WvYHgAAAAAAAAAAAAAAAAAAAAAA2KsuvvjiXHvtYXNa31/3WdZJ9JC5mfoPdl1vflGULtnDf7FU1fvGGC9N8vSJqXdM8oExxhuS/FlVffxHTRxjjCRnJ3l2ksctWPO6LP53dM3EmW7o+9n9QOCq38Ic79Suql6c5MXd+4wxDiTZd8M/P/3003PgwIHu7QEAAAAAAAAAAAAAAAAAAAAAgD1s//79OXjw4OGGLl33WdZJ9JC5+cHE+DFrOcURGIW7gWcluU+Sc7Yw9/wk548xvp7kn5N8Pck3N8dOSXJqkl/a/OciL05yXpLbL5izbPRwL7yHVb+FOd4JAAAAAAAAAAAAAAAAAAAAAACAmRI9ZG4OTYwfv5ZTJCdMjE+dc1dV1dVjjEck+UCSM7b4tVOTPHGbW34oG6HF8yfmXb7kuoeSnLRgfB3vYdVvYY53AgAAAAAAAAAAAAAAAAAAAAAAYKaO2u0DwIpNRfFuuZZTTO+zbLxv7arqG0l+Icmnmrf6aJKHVdW1SY6bmPtfS669F97Dqt/CHO8EAAAAAAAAAAAAAAAAAAAAAADATIkeMjeXTYyftI5DJDlxYnzqnHtCVX0zyc8leWXTFq9Lcm5VfXeMcdMkx0/M/8aS6++F9zC1x7JvYY53AgAAAAAAAAAAAAAAAAAAAAAAYKZED5mb70yMHzvGOKnzAGOMWyU5ZmLaEROFq6qrq+opSR6c5OCKlv1GkidU1eOr6qrNP7vNFr731SX3mXoPt11yve2Y2mPZtzDHOwEAAAAAAAAAAAAAAAAAAAAAADBToofMzSVbmHNK8xm2sv5WzrmnVNW7k9wzySOTvD3JD7axzCVJnp3kblV14Q3Gbjfx3auT/Mc29luk+y1sZY9lQ45zvBMAAAAAAAAAAAAAAAAAAAAAAAAzdfRuHwBWqaoOjTEuS3LrBdPulOTzjce488T4t6rqe437t6mqSvLWJG8dY5yc5NwkP5tkf5K7JPnxJLdIcpMkh5J8J8lnk3wiyT8m+djmGodz14ntP1dV/7Pkkb8yMX6nJdfbjjtPjH95yfW+MjF+JN4JAAAAAAAAAAAAAAAAAAAAAACAmRI9ZI6+nMXRwzOSvKtx/6l43yyCcFV1RZI3bn5WYep3+9Q21pz6rc/YxprLWvV7mOOdAAAAAAAAAAAAAAAAAAAAAAAAmKmjdvsA0ODAxPiZzftPrT91vhursyfG37+NNad+69uMMW61jXW3ZIxxmyRT6y/7HuZ4JwAAAAAAAAAAAAAAAAAAAAAAAGZK9JA5+sTE+FnN+997YvyTzfsfqc6ZGH/fsgtW1VeSXDExrfM9TL2Fy6rq0mUWnOOdAAAAAAAAAAAAAAAAAAAAAAAAmC/RQ+ZoKnr402OMm3RsPMY4Osm9JqaJHt7AGOPMJKctmPLFqvrqNpefeg9nb3PdrZhae7tvYY53AgAAAAAAAAAAAAAAAAAAAAAAYIZED5mjf0tyzYLx49MXhbtvkpsvGL8myceb9j6SPXxi/A07WPtDE+MP3MHaU35xYnzqbNv93gO3ue5WdN0JAAAAAAAAAAAAAAAAAAAAAACAGRI9ZHaq6pok/zIx7cFN2z9oYvyDm+fj/3v8xPjrd7D2uyfG7z/GOGYH6x/WGOO4JPebmPZP21x+jncCAAAAAAAAAAAAAAAAAAAAAABghkQPmaup8NqjmvZ9zMT4u5r2PWKNMc5JctaCKR+tqgM72OIjSb67YPwWSR6yg/V/lIcludmC8f9O8rFtrj3HOwEAAAAAAAAAAAAAAAAAAAAAADBDoofM1UUT4/ceY5y5yg3HGPdM8pMLplSmz3Vj9AcT43++k8Wr6rokb5mYdsFO9tjmmm/ePNvS5ngnAAAAAAAAAAAAAAAAAAAAAAAA5kn0kFmqqouTfGRi2jNXvO1vT4x/uKq+suI9j2hjjPslecSCKV9O8qYVbPXaifFHjzFuv4J9kiRjjDsmeeTEtKkzTZnjnQAAAAAAAAAAAAAAAAAAAAAAAJgZ0UPm7JUT478+xrjdKjYaY9whyRMmpv3NKvaaizHGzZK8YmLa71fV9SvY7t1JLl0wftMkz17BPj/07CRHLxi/JMl7d7jHHO8EAAAAAAAAAAAAAAAAAAAAAADAzIgeMmevSfKtBeM3T/InK9rrT5Mct2D8m5vnIckYYyR5VZK7LZj24ap6/Sr22wwnvnBi2jPGGGfudK8xxr4kT5uY9oKdxhzneCcAAAAAAAAAAAAAAAAAAAAAAADmR/SQ2aqqa5K8aGLaE8cY5+1knzHG45JcMDHthVV17Q73ufMYoyY+z9vJHuuwGTz8iyTnL5h2bZKnr3jrlyW5fMH4TZNcOMY4ZrsbjDGOTXJhkqMXTLs8ycu3u8cNzPFOAAAAAAAAAAAAAAAAAAAAAAAAzIjoIXP3wiSXTsx59RjjvttZfIzxM0leMTHtq5mOL+4pY4x9Y4yzGtY9McnfJ3nGxNTfq6rPrHLvqjqU5LkT0+6T5FVjjKX/bhxj3CTJq5NM/W7P2TzLjs3xTgAAAAAAAAAAAAAAAAAAAAAAAMyL6CGzVlVXJfmdiWknJHnXGOPhy6w9xnhkkncmOX5i6rOq6upl1t4DfirJJ8YY7x9jPHqMcexOFhsbHp3kM0kePTH9H7IRq+zwks0zLHJBkovGGLfc6qKbMcc3JTl/Yuqnk/zVVtfdojneCQAAAAAAAAAAAAAAAAAAAAAAgJkQPWT2quqiJK+bmHZikreOMV47xrj7ooljjH1jjNcneUuSqYjca6vqjVs+7N5z/yQXJfn2GOMNY4xfHWPcbqtfHmOcMsZ4RpJPbq5zx4mv/GuSX6mq2vaJF6iq65M8Ick1E1PPS3JgjPHkMcZxP2rSGONmY4ynJDmQ5BETa16d5AmbZ1iZOd4JAAAAAAAAAAAAAAAAAAAAAACA+Th6tw8Aa/LUJGcnOXPBnJHkgiQXjDE+meTDSb6c5FCSE5LcJcnPJ7nXFvf8XJKnbffAe8wJSR63+ckY45vZCBl+IckVSa7MRgDv5kluleSu2fid7rHEHgeSPLyqrlrZqQ+jqj4zxnhmkr+emHqHJK9K8oIxxnuTfDrJd7LxTm6Tjfudm+nw5Q89s6r+fXunXmyOdwIAAAAAAAAAAAAAAAAAAAAAAGAeRA+5UaiqQ2OMhyT5YJLTtvCVszY/23VJkodU1aEdrLGXnZLkoZufVfhwNoKHV6xovYWq6uVjjNOS/OEWpp+U5LzNz3Y9r6pesYPvT5rjnQAAAAAAAAAAAAAAAAAAAAAAADjyHbXbB4B1qaqvJjk3ycXNW30pyblVdUnzPnPxsiQPWlfw8Ieq6rlJ/mgNWz2/qp6/hn1meScAAAAAAAAAAAAAAAAAAAAAAACObKKH3KhU1ZeSnJPknU1bvCPJOVXVHVacg/9Mcl5VPbWqrt6NA2xGAs9Pcqhh+UNJHltVz2tY+0ea450AAAAAAAAAAAAAAAAAAAAAAAA4cokecqNTVVdU1UOTPDnJt1a07LeSPKmqfrmqrlzRmrvp8iRdIcIrkzwnyRlV9ZamPbasqv4uyT2SvHGFy16U5B5VddEK19yyOd4JAAAAAAAAAAAAAAAAAAAAAACAI5PoITdaVfXqJD+R5BlJPrvNZQ5ufv8uVfW3qzrbbquqdyW5dZKHJ3lJks8lqR0seX2SDyX5jSSnVtUfV9VVOz7oilTV16rqMUnOTnJhthd8vDrJa5Lcu6oeW1VfW+UZlzXHOwEAAAAAAAAAAAAAAAAAAAAAAHDkGVU76ZjBfIwx7pbkoUnunWR/klOTnJDk5kmuSvLdJF/LRujwE0neXlVf3J3Trt8Y4+Qk901yVpK7ZiMYeVqSWyY5Pslx2YjkHUpyWZIvJvl8ko8meU9VXbn+U2/PGONmSc5N8oAk90xytyQnZ+M9JBtv4fIkX0hyIMn7kry3qrYTFlyLOd5pLxpjHEiy74Z/vm/fvhw4cGAXTgQAAAAAAAAAAAAAAAAAAAAAAOwV+/fvz8GDBw83dLCq9q/7POty9G4fAPaKqvpCNoJvHEZVXZHknZufWdsM/b1t8zMLc7wTAAAAAAAAAAAAAAAAAAAAAAAAe99Ru30AAAAAAAAAAAAAAAAAAAAAAAAAAGCeRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaih//Lzr2H3V6XdeJ/34BsRBABT5iOiIdoI4ooiDmmeArNrLTRtNSmZjwfKrXGaqpLy5+VY57SRrPU1NHSPB/B1BzNE4jm3mmeIkkEAVEOAgL374/nmS6GgfVdz3rW53n2fni9rmtdXHDf677vtb5rb/57AwAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIbYa7MPgF1NVW1LcrskN0+yf5J9k1yU5Pwkpyf5UndfunkX7pqq6kZJbpHkkKx8b/sk6SQXZ+W7OyPJN7r77E07cg2qas+sfJ5Dkxycld9BZeWzfC/J15Kc1t1XbNaNi6iqGyS5ZVae0wFZeU6Vled0QZIzk5ze3Wds1o0AAAAAAAAAAAAAAAAAAAAAAABsHUIPIUlVHZfkp5M8IMkRSfac0X55Ve1I8p4kb+/uT4y/cNdTVbdP8hNJjk9ydJIbzfm+M5OckuTDSd7d3TtG3bhWVXWHJA9Ncq8kd8lK0OEsF1fVp5O8L8nbunvn2AvXrqoOS/KTWXlOd0nyQ3O+7ztJTs3Kc3pvd3960IkAAAAAAAAAAAAAAAAAAAAAAABsYdXdm30DbJqq+rkkz8xKaN+iTk7yx939puVcteuqqm1JHp3kKUmOXNLYzyV5cZLXdfelS5o5t6raM8kjkzw9yR3XOe5DSZ7f3e9Z92HrsPqZfjbJ05LcbUljv5LkZUle2d0XLGnmlrMaiLr9qv99+/bt2bFjl8n3BAAAAAAAAAAAAAAAAAAAAAAANsERRxyRnTt3Xl1pZ3cfsdH3bJQ9NvsA2AxVdXhVfSTJ/8r6Ag+T5M5J3lhVH6qqH17/dbumqnpYki8neUWWF3iYrAQNvirJl6rqp5c4d1JV3SvJqUlem/UHHibJ8UneXVXvqarDljBvzarqPkk+n+SNWV7gYZLcJskLknytqn65qmqJswEAAAAAAAAAAAAAAAAAAAAAANiihB5yrVNVD0ny6SQ/tuTR90rymar6mSXP3VRVtX9VvTHJm5LcYuCqQ5O8tapeXVX7DtyTqtqrqp6X5O+S3H7AigckOaWqHjxg9tWqqr2r6oVJTkqyfeCqGyX586yEO95w4B4AAAAAAAAAAAAAAAAAAAAAAAC2AKGHXKtU1ZOSvDnJfoNW7JfkLVX1xEHzN1RV3STJ3yd5+AaufUySv6uqg0YMXw1UfHuS30hSI3asOiDJ26rqqQN3JEmqar8k70rytNG7ruQBST5eVbfcwJ0AAAAAAAAAAAAAAAAAAAAAAADsZoQecq1RVY9J8pKMDbrL6vyXVtWjB+8Zqqqun+TEJEdtwvq7JnnfakDh0lTVtiTvTvLAZc6dtTLJi6rq8cMWVF0nyVuT3G/Ujhlum+RDVXXjTdgNAAAAAAAAAAAAAAAAAAAAAADAbkDoIdcKVXVskldmvsDDjyd5cpKjkxyU5Dqr/7xLkqcm+eQ8K5O8sqqOWejgXcNrkhw5Z+8FSV6b5LFJ7pTkh5JcN8m+SW6e5M5JnpDkDUm+P+fMY5L8zzXcO49XJ7nXnL1fTfKCJPdPcniSG2Tl8xyW5O5Jfisrv4WeY9ZLq+reazt1bn+U5L5z9l6a5G+SPCUr3+8tkuyXZJ8kN0tyhyS/lORVSc6bc+atkvxNVfn/CQAAAAAAAAAAAAAAAAAAAAAAAP+P6p4nrwt2X1V1/SSnZiWcbZYvJ3lCd39wjpn3T/KyJLeeaP16kqO6+3tznLrLqKpHZCWgcMr3kzw7ycu7+7tzzj44K6F7z0qy9xxveXB3v3Oe2RN7n5jkT+do/bckv97d83z+VNVdk7w0K6GYs5yVld/CGfPMnXP33ZN8NNNhnpdnJcDxf3T3mXPO3i/JLyd5TpL953jLU7v7JfPM3sqqakeS7Vf979u3b8+OHTs24SIAAAAAAAAAAAAAAAAAAAAAAGBXccQRR2Tnzp1XV9rZ3Uds9D0bZY/NPgA2wLMzHXh4UpJj5gk8TJLu/kBWQu4+NNF6qyS/N8/MXUVV7ZXk9+do/eckx3b38+YNPEyS7j6nu38vyY8m+Zc53vK8qlrX31VVdWiS58/R+oEkh88beJgk3f3JJMcmed5E642TvGjeuXP6/zIdeHhmkuO7+9fnDTxMku6+oLtflOROWQkNnfI7q0GJAAAAAAAAAAAAAAAAAAAAAAAA8O+EHrKlVdX2JE+aaPuHJD+1luC+JOnu85L8ZJJPTbQ+pap+ZC2zN9lPJjlsoudbSe7f3V9YdEl3n5zk/knOnmjdvtq3Hs9Lct2JnncmeXB3X7DW4b3iWUl+Z6L1P1XVfdc6/+pU1Z2S3GOi7cIkD+zujy66p7u/mpXv/6sTrTdM8guL7gEAAAAAAAAAAAAAAAAAAAAAAGBrEnrIVve7SfaaUT83ycO7+6JFhnf3hUkeluS8GW17ZToMb1fy83P0PKq7T1vvou7+cpJfnKN1npuu1mrg5MMn2r6W5BHdfcmie5Kku5+T5G0Tbc9dz44rmec7+ZXuPmW9i7r720kekuSKJdwEAAAAAAAAAAAAAAAAAAAAAADAtYjQQ7asqjosyUMn2n67u7+xnj2r4X+/O9H2n6rq0PXs2QhVVUmOn2j7YHeftKyd3f3uJH8/0Xafdax40tQJSf7zaoDlMjwuydkz6sdU1X2XsGfqO/lSkr9cwp4kSXd/PsnrJtqOq6rrLWsnAAAAAAAAAAAAAAAAAAAAAAAAuz+hh2xlT0qy54z6l5O8Ykm7XpbkazPqe2Y6fG9XcGiSgyZ6/nzA3qnncEhV/dBah1bVnkl+bqLtvd09Fbo4t+4+K8kLJ9p+bT07quo6SY6caHt1d1++nj1XY+o57ZXkqCXvBAAAAAAAAAAAAAAAAAAAAAAAYDcm9JAtaTXs7hETbX+yrFC47r4syYsn2h5ZVbv6n7lbT9SvSHLSgL0nJumJntssMPc/Jjl4ouflC8yd8udJLp1Rv39V3XQd82+Z2YGeSfKBdcy/Jp9M8t2JnkWeEwAAAAAAAAAAAAAAAAAAAAAAAFvUrh7ABou6d5JDZtQvTvK6Je98TWYH3d0syb2WvHPZDpyon9HdZy97aXefleRbE203XGD0PSfqZyV5zwJzZ+ruM5O8d0bLnkkeuY4VU88pSf5xHfOv1mq4586JtkWeEwAAAAAAAAAAAAAAAAAAAAAAAFuU0EO2qp+cqL+7u89f5sLuPi+zg+6S6bs227aJ+tIDD6/k2xP16y4w85iJ+j909xULzJ3HxybqP7WO2VPP6bvd/YN1zJ9lxHMCAAAAAAAAAAAAAAAAAAAAAABgixJ6yFZ134n6uwftnZp7v0F7l+W7E/ULB+6emv29BWYePlH/xAIz5/XJifrdqmr/BWdvtecEAAAAAAAAAAAAAAAAAAAAAADAFiX0kC2nqg5J8iMTbScNWn/iRP2IqrrpoN3LcM5E/eCBu6dmT932f6mqSnKLibada5m5Rjsm6tdJcq8FZ099FwctOHceS31OAAAAAAAAAAAAAAAAAAAAAAAAbG1CD9mKjp2of6O7vzFicXf/S5IzJtqOGbF7Sb6YpGfUbzJw99Tsr61x3v5Jtk30fGeNM9fivMz+LpPkuAVnn5HZt+9TVTdYcPaUqdDOtT4nAAAAAAAAAAAAAAAAAAAAAAAAtjChh2xFR0/UTxm8/zMT9TsN3r+w7j43yY4ZLTeoqiOXvbeq7pjkgBktX+vuqTDJq9p3jp7z1jhzbt19eZILJtqmfqvXNLuTfGyi7R6LzJ6lqg5KcvsZLd9PcvKy9wIAAAAAAAAAAAAAAAAAAAAAALD7EnrIVnTURP3zg/dPzd9lQw9XvWei/sABO39iov7BBWZeZ46e7y8wdy2m5i8UerhqM57TCZn9/43/3d2XDtgLAAAAAAAAAAAAAAAAAAAAAADAbkroIVvR7SbqXx68/ysT9dsO3r9eL0ty+Yz606rqustaVlX7JXnaHDet1SVz9BywwNy1mJp/46pa9Ia/SnLejPpjquqQBWf/P6pqjyTPmmhb5DkBAAAAAAAAAAAAAAAAAAAAAACwhQk9ZEupqkpy6ETbVCjhek3NP3Tw/nXp7tOSvH5GyyFJ/vsSVz47yY1n1P+uu09dYO6Fc/TcYIG5c6mqfZJsm6P11ovM7+4Lkrx4Rst1kzx/kdnX4MlJbj+j/uUk71jiPgAAAAAAAAAAAAAAAAAAAAAAALYAoYdsNTdJss9EzzcH3zA1/3pVNSvkb1fwa0nOnFF/VlU9er1LqurxSX51RsvFSZ64yOzuvjDJBRNtN1pk9pzmfcaHrWPHHyTZMaP+yKpad0BlVT0oyQtmtHSSx3b3FevdBQAAAAAAAAAAAAAAAAAAAAAAwNYi9JCt5mZz9Hxr8A3zzJ/nzk3T3eck+YUkl85o+8uqem5V7bXW+VW1rapemOTlE63P7O4vrXX+lZw+Ub/LOmZPmXf2wr+F7r40ySOTnDej7dlV9aqq2net86tqj6p6VpK3JtlzRusLuvvDa50PAAAAAAAAAAAAAAAAAAAAAADA1if0kK3m4In697r7kpEHdPdFSS6YaJu6c9N190lJHpbksmto2SPJs5L8Y1U9dp5Qvarav6qenGRHkqdNtP9+d790LTdfjZ0T9ePWOX+Wu87Zt67fQnd/PskJSc6f0fZLSb5YVc+oqgOnZq6GUj46ySlJnptkVrDla5M8cw0nAwAAAAAAAAAAAAAAAAAAAAAAcC0yK8gKdkcHTdS/tyFXrOzZb0Z96s5dQne/vaqOT/L6JP/hGtoOT/I/k7ykqk5J8ukkZyX5TpJKcmCSm2QlBPCoTP+984Mkv9Xdf7zuD5CcnOQhM+p3qaoDu/s7S9h1Vfebs2/dAZjd/cmqOi7JG5MceQ1tt0jyx0meV1X/mOQTSb6V5Nwkl2flOd0oyZ2THJNk29TaJC9M8szu7vV+htGq6klJnrgBq269ATsAAAAAAAAAAAAAAAAAAAAAAAB2G0IP2WoOnKifvyFXTO/ZLUIPk6S7/3dV3THJc5L811xzGN7eSY5bfS1qR5Jf7u5PrmPGlX1kor4tyS8m+ZMl7UuSVNVdk9xpzval/Ba6e2dVHZvkN5M8Lcn1r6F1z6yETx61jnWnJXlid79nHTM22o2SbN/sIwAAAAAAAAAAAAAAAAAAAAAAAK5t9tjsA2DJ9pmoX7ghVyQXTNSn7tyldPd53f2UJIcleV6Sryx5xSlJHpHkDksMPEySTyQ5Z6LnCVW15xJ3JsmT19C7tN9Cd1/c3b+T5NAkv53kH5c1e9WXkjwhyW13s8BDAAAAAAAAAAAAAAAAAAAAAAAANonQQ7aavSfql23IFdN7pu7cJXX3N5M8J8kzknxqCSM/l+S47r5zd7+xu69Ywsx/192XJ3njRNttk/zGsnZW1b2S/Pwa3rL030J3fyfJHyV5epL3LWHkvyR5QHcf3t1/1t0/WMJMAAAAAAAAAAAAAAAAAAAAAAAArgWEHrLVCD0cpKpuU1WvTHJ2krclOXYJY++Y5GNV9dGq+qWqus4SZl7Vn83R87tVddR6F1XVAUn+Mkmt4W1L/S1U1SFV9fysPKcPJDlhCWMPTfLeqjq5qn6lqq63hJkAAAAAAAAAAAAAAAAAAAAAAABcCwg9ZKuZ+k1fviFXTO/Zc0OuWIKqOqCq/iLJF5P8lyTXXfKKPZP8xySvSvLVqnp8Va0lNHCm7v5CkrdOtO2d5P1VdcdF91TVQUlOzEpA4Fos5bdQVduq6o+SfD3J05Ncfxlzr+LoJH+S5LSq+s1BIZUAAAAAAAAAAAAAAAAAAAAAAABsIXtt9gGwZJdN1DfqNz+15wcbcsU6VdU9krwuyX/YoJW3SPLyJA+tqsd09zeXNPdZSR6YZNuMnhsn+VBV/VJ3v20tw1fDEl+X5PYL3HbJAu+56v7bJ3lDkiPXO2tOByf5g6w8p5/v7i9u0N71+HaSnRuw59aZ/TsDAAAAAAAAAAAAAAAAAAAAAAC4VhF6yFZz6UR9o37z15moT9256arqQUnekmTvOdq/keT9ST6a5NQk5yY5J0klOSgrIXlHJblHkh9PcvOJefdNcmpV3bu7v7DA+f+X7v5SVf1Okj+caD0wyVur6sQkv5/kY919+TU1V9X2JE9O8tgke15D22WZ/bu7eOKmmarq2CQfSHLAHO3fTvK+rDynk5OcnZVn9YOsPKODkhyRled0/yS3nZh3dJKTq+pB3f2hhT7ABunuP03yp6P3VNWOJNtH7wEAAAAAAAAAAAAAAAAAAAAAANhdCD1kq/nBRH2eAL9l2K1DD6vqhMwXePiFJM9N8jfdfdk19FyU5PQkn0vymqraK8nDk/xmZofD3SjJB6vq+O7euZb7r8Hzk9w9yYPn6L3f6uuc1QDEf01yZlae202SHJLknkluMzHnXVkJE7zbjJ6FQw+r6k6ZL/DwtCTPS/Lq7r6mfd9cfX0hyZuqqpL8RFae06z7903yrqp6YHd/ZC33AwAAAAAAAAAAAAAAAAAAAAAAsPUJPWSruWCivt+GXJHsP1GfunPTVNUhSV6f6cDDlyR5RnevKcBxNRzx9VX15iQvSPLEGe03TvKOqrpjd1+4lj1Xs/eKqnpkkhMzO8Tvyg5O8nMLrvxSksckef9E37mLDK+q6yX560wHHv51kv/a3d9by/zu7qyEGb4nyW8l+d0ke15D+75J/raqjuzub65lDwAAAAAAAAAAAAAAAAAAAAAAAFvbHpt9ACzZVIDc9Tfkiuk9CwXdbZC/SHLQRM/Tuvupaw08vLLuvqS7n5Tk6ROtt07yPxbdc5WdFya5f5KTljFvhq8kuXd3n5tkn4neMxbc8fwkt5noeUF3P3ytgYdX1t1XdPdzshL+2DNaD0ryqkX3AAAAAAAAAAAAAAAAAAAAAAAAsDUJPWSrOWeifoONOCLJARP1qTs3RVXdO8kJE20v6e4XL2tnd78gycsn2h5XVXdY0r4LsvIZn5vkimXMvIqTkhzX3d9c/fepAMlvrXVBVd0myWMn2t6e5JlrnX1NuvvNSf7bRNsJVfXgZe0EAAAAAAAAAAAAAAAAAAAAAABg9yf0kK3m7In6tqq6wcgDquqgJHtPtO2SoYdJnj5R/7ckvz5g7zMyHf73a8ta1t2Xd/dvJTk2yT8saez3kvxqkhO6+8rP9+CJ9522wK5fzey/vy9K8vjuXnao4x8nOXWiZ2nPCQAAAAAAAAAAAAAAAAAAAAAAgN2f0EO2mn+do+cmg2+YZ/48d26oqjokyQMm2v6guy9e9u7uvijJ70+0PWLZgZXdfXJ3/2iS45O8Ocn3Fxjz7STPSXLb7n5hd1/+fwqrAZjbJt6/Yy3LqmqvJI+aaHtpd0+FSK5Zd3eS355ou2dVbV/2bgAAAAAAAAAAAAAAAAAAAAAAAHZPe232AbBM3X1BVZ2T5OAZbbdM8qWBZxw6UT+ruy8cuH9R90xSM+qXJXnDwP2vT/KiJHteQ33vJPdI8s5lL+7uDyf5cFVdLysBiHdLcockt0py0yTXS3KdJBcm+U6SLyY5Ncn7k3y0uy+7htG3mVh9bnefscZz75Jk/4me165x5lq8J8nZSW44o+c+SXYOvAEAAAAAAAAAAAAAAAAAAAAAAIDdhNBDtqKvZ3bo4W2TfGDg/qmgu68P3L0e95iof6q7vztqeXefV1WfTnLcjLYfy4DQwyvdcGGSd62+lmHqt/C5BWZOPad/6+4dC8ydS3d3VZ2U5OdmtP1YkpeMugEAAAAAAAAAAAAAAAAAAAAAAIDdxx6bfQAMMBX49sOD90/NHxZIt06HTdQ/tQE3fHKifvgG3LBMd56of2SBmVPP6dMLzFyrrfacAAAAAAAAAAAAAAAAAAAAAAAAGEToIVvRKRP1Ow3ef/RE/bOD9y/q4In6tzfghqkdUzfuao6ZqH94gZmeEwAAAAAAAAAAAAAAAAAAAAAAALsNoYdsRVOhh0dV1Z4jFlfVXknuONG2q4YeHjhRP3sDbpjasduE6VXV/knuOqPl/CSfWGC05wQAAAAAAAAAAAAAAAAAAAAAAMBuQ+ghW9Fnklw8o75fkjsP2n1skn1n1C9OcvKg3et1+UR92wbcsM9EvTfghmW5f5K9Z9Tf3t2XLDDXcwIAAAAAAAAAAAAAAAAAAAAAAGC3IfSQLae7L07ysYm2+w1af9+J+kdX79sVXThRv9EG3DC146INuGFZfmGi/sYF53pOAAAAAAAAAAAAAAAAAAAAAAAA7DaEHrJVnThRf8igvT87Uf/AoL3L8K2J+s034IZbTNTP3IAb1q2qDknyoBktpyd5/4LjPScAAAAAAAAAAAAAAAAAAAAAAAB2G0IP2arePFE/uqp+eJkLq+r2SY6c0dKZvmszfX2ifvwG3HDvifrUjbuK/5Zkrxn1F3f3ZQvOnvoO7lZV+yw4e173majvLs8JAAAAAAAAAAAAAAAAAAAAAACAwYQesiV191eTfGKi7SlLXvvUifrHu/tflrxzmT47UT+0qg4ftbyqjkxy84m2z4/avyxVdeskj5vRcn6SV6xjxdRz2ifJvdYxf6aqOjDJXSfadvnnBAAAAAAAAAAAAAAAAAAAAAAAwMYQeshW9hcT9f9cVYcsY1FV3TzJoybaXr2MXQN9fI6eZwzc/xtz9Mxz46apqsrK727bjLbndvd317HmU0kum+gZ+ZyenmSviZ5d+jkBAAAAAAAAAAAAAAAAAAAAAACwcYQespX9VZKzZtT3TfK8Je36wyT7zKifuXrPLqu7/ynJlyfaHlNVhy97d1UdleQRE22ndffnl717yf4wyY/NqH89yZ+sZ0F3n5/kwxNt96mq+65nz9VZDQn9lYm2i5J8cNm7AQAAAAAAAAAAAAAAAAAAAAAA2D0JPWTL6u6Lk7xoou3RVfUz69lTVQ9L8siJthd29yXr3HNoVfXE6/fWsyPJGybqeyX526q6wTr3/LuqulGSt2T676M3LmvnCFX160meOaOlkzxhvb+DVa+fo+evquqWS9iVJKmq6yZ5W5LrTbS+s7svXNZeAAAAAAAAAAAAAAAAAAAAAAAAdm9CD9nqXpjkGxM9r6mqYxcZXlXHJXnVRNtpmQ5f3FW8LMnFEz0/kuTtVXXQepdV1U2SvCvJYROtlyZ56Tp33byq7rmeGdcwd1tVvTTJH060vqS737+ktf8ryRkTPTdN8p5lBB9W1f5J/jrJPH9OXrDefQAAAAAAAAAAAAAAAAAAAAAAAGwdQg/Z0rr7oiS/NtG2f5IPVNWD1jK7qn4qyfuT7DfR+vTu/v5aZm+W7j4r8wU0/liSU6vqHovuqqr7Jjk18wXp/Vl3n77orlU3T/LhqvpsVf1iVU09t0lVdZ8kn0nypInWzyT5jfXu+z+6+5Ikz56jdXuSz1bVzyy6q6qOzsr98/z5eEd3f2rRXQAAAAAAAAAAAAAAAAAAAAAAAGw9Qg/Z8rr7zUneMNF2QJJ3VNXrq+rwWY1Vtb2q3pjkbUmuPzH39d39lrmP3TU8O8nX5ui7RZK/r6qTquoBVbX31BuqaltVPbiqPpLkxCQ3nWPP6Ul+e46+eR2V5C+TfLuq3llVv1xVt5z3zVV1YFU9pqo+muSkJLefeMvXkzyouy9e+OKr94ok/zBH34FJ/raqPlVVD6uqfafeUFV7VdXxVfWOrAQe3m6OPecnecocfQAAAAAAAAAAAAAAAAAAAAAAAFyL7LXZB8AGeVySOyf54Rk9leSRSR5ZVZ9N8vGsBNZdkGT/JLdKcvckd5xz5xeTPH7RgzdLd19UVQ/Nyue/7hxvuc/q6+Kq+mSSzyU5J8m5WflOD0pycJI7JTkmybY1nHNJkod29/lreM+89knyoNVXquo7ST6b5J+ycvt5SS5c7TswyWFJjkxyh8wfGPtvSU7o7jOXeXiSdPcVVfWwJCcnufEcbzkmyZuS/KCqTslKmOHZWfmsl2flMx6Ulc94tyTXW8s5SR7d3f+6hvcAAAAAAAAAAAAAAAAAAAAAAABwLSD0kGuF7r6gqn48yUeT3GKOt9xp9bWof03y4919wTpmbJruPrWqHpLkrVkJ/ZvHPknuufpahkuTPKy7P7WkeVMOTHLv1dcyfCnJ/UcGAXb36VX1gCQnZiWwcB7XSXLX1ddSzkjypO5+25LmAQAAAAAAAAAAAAAAAAAAAAAAsIXssdkHwEbp7tOyEmj31cGrvpLk3iPD7jZCd78vyQlJztqE9ecmeVB3v2MTdi/D25L86Eb8Brr7lKwETY7+XV+d7yd5VHe/fBN2AwAAAAAAAAAAAAAAAAAAAAAAsBsQesi1Snd/JckxSd4/aMX7khzT3ZsRQLd03f2RJHfOyufaKB9McnR3n7iBO5flvCSP6+6f6e5zN2ppd38hyV2SvG6jdiY5Oclx3f36DdwJAAAAAAAAAAAAAAAAAAAAAADAbkboIdc63f2d7j4hyS8mOWtJY89K8pjufkB3n7ekmbuE7j69ux+Q5KeTfHbgqs8l+dnuvm93nzZg/vlJvjdgbpJ8P8kLkty6u18xaMdM3X1edz8qyT2TfHjgqq8leWySY7v78wP3AAAAAAAAAAAAAAAAAAAAAAAAsAUIPeRaq7tfk+SwJE9K8k8Ljtm5+v5bdfdrl3Xbrqi7397dRye5V5I/T/LtJYw9J8lfJLl3dx/V3W9Zwsyr1d07ktwwyX2yElD4uSSXr2dkVkIgn5bkZt399O4+d92HrlN3/313H5/kzklemOQbSxh7fpI3JXlwktt19yu7+4olzAUAAAAAAAAAAAAAAAAAAAAAAGCLq+7e7Btgl1BVt0tyQpKjkxyR5IeS7J9k3yQXZSX47fSsBB2ekuS93f3lzbl281VVJTkyyd1W/3m7JDdLcpOsfGfbVlsvycr3d2aSbyb5cpIvJPlEklN7E/8Sqqr9ktwlKwGBt0ly6yS3THJAkutl5XNckuSCJOcm+UqSf07ymSQf7O4zN+HsNVv9bd89K8/p8Kz8tm+alc+4T5LKyuf8fpKzkpyRlc+6I8knk3ymuy/b+Mt3P1W1I8n2q/737du3Z8eOHZtwEQAAAAAAAAAAAAAAAAAAAAAAsKs44ogjsnPnzqsr7ezuIzb6no2y12YfALuK7v7nrATaMYfVsMLPr752S919QZIPr762LL9tAAAAAAAAAAAAAAAAAAAAAAAANssem30AAAAAAAAAAAAAAAAAAAAAAAAAALA1CT0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAID/n517j7q9rurF/56wBVQQgYOIYSJYInjBC4p5Q1PDS1qa1yztnDqalxyl5c9TJ+1mOk6nTE1L7WI/NfWnR80wEUyNo3kBvCR4v6AogoLIHQTm74/nsYHGXt+1nmd9nmfvh9drjDX2YM+55pxrrS/7zzcAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMsW2zD4AdTVXtnuTHkxyUZK8kN0hySZILk5yZ5LPdfcXmXbhjqqr9k9w8yYFZ+d72SNJJLsvKd3dWkq9197c37cgFVNWuWfk8ByfZLyvPQWXls1yQ5EtJzujuqzfrRgAAAAAAAAAAAAAAAAAAAAAAANjRCT2EJFV1dJKfSfKgJEck2XVG+1VVdVqSdyZ5e3d/aPyFO56qum2ShyS5b5I7Jdl/zvedneTUJO9Lclx3nzbqxkVV1e2TPDLJMUnukpWgw1kuq6qPJnlXkrd19+ljL1yeqtojyT2T3CPJrVdfByTZc/V1RZKLk1yU5OtZCXn8YpJ/T/Kh7v7GJpwNAAAAAAAAAAAAAAAAAAAAAADATqa6e7NvgE1TVY9N8ptZCe1bq1OS/K/ufuNyrtpxVdXuSX4xyTOS3G5JYz+R5CVJXtvdVyxp5tyqatckj0/yrCR3WOe49yb5k+5+57oPG2D1sz4kyS8neUCSPdYx7mtJTkpyXJJ3dfd5679w57caiHr4D//94YcfntNO22HyPQEAAAAAAAAAAAAAAAAAAAAAgE1wxBFH5PTTT7+20undfcRG37NRdtnsA2AzVNVhVfX+JP+Q9QUeJsmdk7yhqt5bVbde/3U7pqp6dJLPJ3lllhd4mKwEDf51ks9W1c8sce6kqjomyceT/H3WH3iYJPdNclxVvbOqDlnCvKWoql2r6peTfCXJ25P8dNYXeJgkN89KWOTrkpxTVc9Z5zwAAAAAAAAAAAAAAAAAAAAAAAC2IKGHXOdU1SOSfDTJvZc8+pgkJ1fVzy557qaqqr2q6g1J3piVoLtRDk7y1qr6u6q6wcA9qaptVfXCJP+S5LYDVjwoyalV9bABsxdSVUcn+fckr0py0KA1uyY5cNBsAAAAAAAAAAAAAAAAAAAAAAAAdmJCD7lOqaqnJXlzkj0HrdgzyVuq6qmD5m+oqjogyb8mecwGrn1ikn+pqn1HDF8NVHx7kuckqRE7Vu2d5G1V9WsDd2xXVe1SVb+b5KQkt9mMGwAAAAAAAAAAAAAAAAAAAAAAAEDoIdcZVfXEJC/N2KC7rM5/WVX94uA9Q1XVjZKckOTITVh/tyTvWg0oXJqq2j3JcUkevMy5s1Ym+fOqesoG7VtZuvI535Tk95Js28jdAAAAAAAAAAAAAAAAAAAAAAAAcE3CsLhOqKq7JnlV5gs8/GCS16/++ZUkFybZK8khSX4iyc9nJZRv5sokr6qqT3f3R9d49mZ7TZLbzdl7UZL/k+T/JvloknOSnJeV72HfJAckuWuSeyX52STXn2PmUUn+KskvLHT1bH+X5Jg5e7+Y5O1J3pXkq0m+meSKJDdNcuDqnIdl5XNNPVcvq6rPdfe/LHzxglaDIo/L/J8zSc5IckpWPvO3svJ7Xi/JjVdfP5rkjklumfGhoQAAAAAAAAAAAAAAAAAAAAAAAGwhQg/Z8qrqRknekJUQt1k+n+RXu/s911L7TlZC4U5J8tKqemCSlyc5dMa83ZK8saqO7O4LFr9881TV45L8zBytlyb5/SSv6O7vbqfn66uvU5P8ZVXtl+QZSZ6ble9olidU1Zu6+x1zHT5DVT01yWPnaP16kt/q7tdvp/7l1dcHk7ygqu6W5GVJ7jJj5q5J/mH1WThrgbMXUlW7ZuVZP2aO9i8m+eskb+zuL805/0ZJjs5K2ONPZyUMEQAAAAAAAAAAAAAAAAAAAAAAALZrl80+ADbA7ye55UTPiUmO2k7g4X/S3e/OSsjdeydab5nk+fPM3FFU1bYkfzhH6+eS3LW7Xzgj8PA/6e5zu/v5SX4iyVfmeMsLq2pd/1ZV1cFJ/mSO1ncnOWxG4OF/0t0fTnLXJC+caL1Jkj+fd+4avSQrYYSznJvkV7LyOf943sDDJOnuC7r73d399O6+RVY+92uSXL7miwEAAAAAAAAAAAAAAAAAAAAAANjShB6ypVXV4UmeNtH2b0kevkhwX5J09/lZCZj7yETrM6rqNovM3mQ/neSQiZ5vJnlgd39qrUu6+5QkD0zy7YnWw1f71uOFSa4/0fOOJA/r7osWHd4rnpvkdydaH1VV9190/jyq6lFJnjrR9t4kt+3uV3f3levd2d0f7e4nJTkoydxBkQAAAAAAAAAAAAAAAAAAAAAAAFx3CD1kq3tekm0z6ucleUx3X7KW4d19cZJHJzl/Rtu2TIfh7Uh+fo6eX+juM9a7qLs/n+RJc7TOc9O1Wg2cfMxE25eSPK67L1/rniTp7j9I8raJthesZ8e1qaqDkvzVRNubkhzb3d9c9v7u/nZ3T4V/AgAAAAAAAAAAAAAAAAAAAAAAcB0k9JAtq6oOSfLIibbf6e6vrWfPavjf8ybaHlVVB69nz0aoqkpy34m293T3icva2d3HJfnXibafXMeKp02dkOSXVgMsl+HJSb49o35UVd1/Sbu+78+S7DOjfmKSJ3T3FUveCwAAAAAAAAAAAAAAAAAAAAAAADMJPWQre1qSXWfUP5/klUva9fIkX5pR3zXT4Xs7goOT7DvR8+oBe6d+hwOr6kcWHVpVuyZ57ETbP3f3VOji3Lr7nCQvnmj7jWXtq6p7J/m5GS3fTPLY7v7esnYCAAAAAAAAAAAAAAAAAAAAAADAvIQesiWtht09bqLtz7r7qmXs6+4rk7xkou3xVbWj/z936ET96iQnDth7QpKe6LnVGubeM8l+Ez2vWMPcKa9OcsWM+gOr6qZL2vWiifpTu/vcJe0CAAAAAAAAAAAAAAAAAAAAAACAhezoAWywVvdLcuCM+mVJXrvkna/J7KC7myU5Zsk7l22fifpZ3f3tZS/t7nOSfHOi7b+sYfR9JurnJHnnGubO1N1nJ/nnGS27Jnn8evdU1b2SHD2j5QPd/db17gEAAAAAAAAAAAAAAAAAAAAAAIC1EnrIVvXTE/XjuvvCZS7s7vMzO+gumb5rs+0+UV964OE1fGuifv01zDxqov5v3X31GubO4wMT9YcvYcezJup/tIQdAAAAAAAAAAAAAAAAAAAAAAAAsGZCD9mq7j9RP27Q3qm5Dxi0d1m+O1G/eODuqdkXrGHmYRP1D61h5rw+PFG/e1XttdbhVbV/kofMaPlKkuPXOh8AAAAAAAAAAAAAAAAAAAAAAACWQeghW05VHZjkNhNtJw5af8JE/Yiquumg3ctw7kR9v4G7p2ZP3fYDqqqS3Hyi7fRFZi7otIn69ZIcs475j0qybUb9Dd199TrmAwAAAAAAAAAAAAAAAAAAAAAAwLoJPWQruutE/Wvd/bURi7v7K0nOmmg7asTuJflMkp5RP2Dg7qnZX1pw3l5Jdp/o+c6CMxdxfmZ/l0ly9DrmP2ai/o51zAYAAAAAAAAAAAAAAAAAAAAAAIClEHrIVnSnifqpg/efPFG/4+D9a9bd5yU5bUbLjavqdsveW1V3SLL3jJYvdfdUmOQPu8EcPecvOHNu3X1Vkosm2qae1WtVVTdMcvcZLRcn+fBaZgMAAAAAAAAAAAAAAAAAAAAAAMAyCT1kKzpyov7Jwfun5u+woYer3jlRf/CAnQ+ZqL9nDTOvN0fPpWuYu4ip+WsKPUxyj8z+fKeshi4CAAAAAAAAAAAAAAAAAAAAAADAptq22QfAAD8+Uf/84P1fmKj/2OD96/XyJM9Ksut26s+sqpd091ICA6tqzyTPnOOmRV0+R8/ea5i7iKn5N6mqvbv7uwvOvfdE/dR5hlTVjZIckeSAJDdK0kkuSfLtJGck+ZrwRAAAAAAAAAAAAAAAAAAAAAAAANZD6CFbSlVVkoMn2qZCCddrav7Bg/evS3efUVWvS/KL22k5MMn/TPI/lrTy95PcZEb9X7r742uYe/EcPTdew9y5VNUeSXafo/XQzBlSeA1HTtS3+wxW1V2SPDbJQ5PcemLOxVV1cpL3JHlLd5++yJEAAAAAAAAAAAAAAAAAAAAAAACwy2YfAEt2QJI9Jnq+MfiGqfk3rKpZIX87gt9IcvaM+nOranuhiHOrqqck+fUZLZcleepaZnf3xUkummjbfy2z5zTvb3zIGmbfdqL+xR/+i6o6pqpOSvLRJM/KdOBhktwwyX2yEkx5WlX9W1U9YtFjAQAAAAAAAAAAAAAAAAAAAAAAuO4SeshWc7M5er45+IZ55s9z56bp7nOTPCHJFTPa/raqXlBV2xadX1W7V9WLk7xiovU3u/uzi86/hjMn6ndZx+wp885e6Fmoqhsm+dGJtv8IrKyqG1XVa5O8N8k9F9l1LY5O8paqOqmqbrfOWQAAAAAAAAAAAAAAAAAAAAAAAFwHCD1kq9lvon5Bd18+8oDuviTJRRNtU3duuu4+Mcmjk1y5nZZdkjw3yb9X1X+vqhtMzayqvarq6UlOS/LMifY/7O6XLXLztTh9on70OufPcrc5+xZ9Fg5KUhM930qSqjosyUeT/PyCO6bcM8lHqupXljwXAAAAAAAAAAAAAAAAAAAAAACALWbbZh8AS7bvRP2CDbliZc+eM+pTd+4QuvvtVXXfJK9L8qPbaTssyV8leWlVnZqVkL1zknwnK+F8+yQ5ICshgEdm+t+d7yX57e7+X+v+AMkpSR4xo36Xqtqnu7+zhF0/7AFz9i0aenjgHD0XrAYevi8r3/0IeyR5ZVXdort/Z9COpamqpyV56gasOnQDdgAAAAAAAAAAAAAAAAAAAAAAAOw0hB6y1ewzUb9wQ66Y3rNThB4mSXf/36q6Q5I/SPIrSXbfTutuSY5efa3VaUn+W3d/eB0zrun9E/XdkzwpyZ8taV+SpKruluSOc7Yv+izME3p44yQnZlzg4TX9dlVd3d2/uwG71mP/JIdv9hEAAAAAAAAAAAAAAAAAAAAAAADXNbts9gGwZHtM1C/ekCuSiybqU3fuULr7/O5+RpJDkrwwyReWvOLUJI9LcvslBh4myYeSnDvR86tVtesSdybJ0xfoXfRZ2HuOnr9L8iMz6mcmeXmSh2QlCHC/1TtukeRuSX49yXuTXDnnTf+zqh43Zy8AAAAAAAAAAAAAAAAAAAAAAADXIUIP2Wp2m6jPG+K2XlN7pu7cIXX3N5L8QZJnJ/nIEkZ+IsnR3X3n7n5Dd1+9hJn/obuvSvKGibYfS/KcZe2sqmOS/PwCb1n0Wbj+HD333c7fn5vkqUlu0d1P6+53dvenu/u87r68u7/a3R/p7hd39/2S3DbJu+a86xVVdfM5ewEAAAAAAAAAAAAAAAAAAAAAALiOEHrIViP0cJCqulVVvSrJt5O8LcldlzD2Dkk+UFUnVdV/rarrLWHmD/vLOXqeV1VHrndRVe2d5G+T1AJvW/RZ2GPB/u/7cJJbd/cr5g2X7O7PdveDkjwlydR79k7y0jXeBgAAAAAAAAAAAAAAAAAAAAAAwBYl9JCtZuqZvmpDrpjes+uGXLEEVbV3Vf1Nks8k+eUk11/yil2T3DPJXyf5YlU9paoWCQ2cqbs/leStE227JTm+qu6w1j1VtW+SE5IcvOBbF30W1hIM+f4k9+/uc9fw3nT3XyV5Qqaf64dX1d3XsgMAAAAAAAAAAAAAAAAAAAAAAICtadtmHwBLduVEfaOe+ak939uQK9apqu6V5LVJfnSDVt48ySuSPLKqntjd31jS3OcmeXCS3Wf03CTJe6vqv3b32xYZvhqW+Nokt13DbZcv2L9ocOc5SR7V3Rct+L4f0N3/sPo5nzPR+uwkj1zPrkG+leT0DdhzaGY/ZwAAAAAAAAAAAAAAAAAAAAAAANcpQg/Zaq6YqG/UM3+9ifrUnZuuqh6a5C1Jdpuj/WtJjk9yUpKPJzkvyblJKsm+SfZLcmSSeyX5qSQHTcy7f5KPV9X9uvtTazj/B3T3Z6vqd5O8aKJ1nyRvraoTkvxhkg9093ZDBqvq8CRPT/Lfk+y6nbYrM/u5u2ziph+26LPzlO7+1oLv2Z7nJXlokiNm9Dysqg7s7rOWtHMpuvsvkvzF6D1VdVqSw0fvAQAAAAAAAAAAAAAAAAAAAAAA2FkIPWSr+d5EfZ4Av2XYqUMPq+rYzBd4+KkkL0jy/3X3ldvpuSTJmUk+keQ1VbUtyWOS/I/MDofbP8l7quq+3X36Ivdvx58kuUeSh83R+4DV17mrAYhfTXJ2Vn63A5IcmOQ+SW41MeefshL4ePcZPSNDD0/u7rcuOH+7uvvyqvq9JG+a0bYtycOT/OWy9gIAAAAAAAAAAAAAAAAAAAAAALDzEnrIVnPRRH3PDbki2WuiPnXnpqmqA5O8LtOBhy9N8uzuXijAcTUc8XVV9eYkf5rkqTPab5LkH6vqDt198SJ7rmXv1VX1+CQnZHYI4TXtl+Sxa1z52SRPTHL8RN95C85d5Ht4xYKz5/HWJGdlJfhxex4coYcAAAAAAAAAAAAAAAAAAAAAAAAk2WWzD4AlmwqQu9GGXDG9Z9Ggu430N0n2neh5Znf/2qKBh9fU3Zd399OSPGui9dAk/3ute35o58VJHpjkxGXMm+ELSe7X3ecl2WOi96wFZ8/77Fye5B8WnD1pNbTytRNtRy97LwAAAAAAAAAAAAAAAAAAAAAAADsnoYdsNedO1G+8EUck2XuiPnXnpqiq+yU5dqLtpd39kmXt7O4/TfKKibYnV9Xtl7Tvoqx8xhckuXoZM3/IiUmO7u5vrP73VIDkNxecP++z87HuvnTB2fP64ER9/6o6aNBuAAAAAAAAAAAAAAAAAAAAAAAAdiJCD9lqvj1R372qbjzygKraN8luE207ZOhhkmdN1L+e5LcG7H12psP/fmNZy7r7qu7+7SR3TfJvSxp7QZJfT3Jsd1/z991v4n1nLLhn6hn/vg8tOHcRH56j59CB+wEAAAAAAAAAAAAAAAAAAAAAANhJCD1kq/nqHD0HDL5hnvnz3LmhqurAJA+aaPuj7r5s2bu7+5IkfzjR9rhlB1Z29ynd/RNJ7pvkzUkuXcOYbyX5gyQ/1t0v7u6rvl9YDcDcfeL9py24b95n5/QF586tu89Kcv5E281H7QcAAAAAAAAAAAAAAAAAAAAAAGDnsW2zD4Bl6u6LqurcJPvNaLtFks8OPOPgifo53X3xwP1rdZ8kNaN+ZZLXD9z/uiR/nmTX7dR3S3KvJO9Y9uLufl+S91XVDbMSgHj3JLdPcsskN01ywyTXS3Jxku8k+UySjyc5PslJ3X3ldkbfamL1easBgovceuEcz3gyHUq4Xt9JcuMZ9X0H7wcAAAAAAAAAAAAAAAAAAAAAAGAnIPSQrejLmR0I92NJ3j1w/1TQ3ZcH7l6Pe03UP9Ld3x21vLvPr6qPJjl6Rtu9MyD08Bo3XJzkn1ZfyzD1LHxijXO/lM0PPZyaf4PB+wEAAAAAAAAAAAAAAAAAAAAAANgJ7LLZB8AAp03Ubz14/9T8qfs2yyET9Y9swA0fnqgftgE3LNOdJ+rvX+PcT83Rc+kaZ89rar5QXQAAAAAAAAAAAAAAAAAAAAAAAIQesiWdOlG/4+D9d5qof2zw/rXab6L+rQ24YWrH1I07mqMm6u9b49xT5ujZe42z5zU1f3ToIgAAAAAAAAAAAAAAAAAAAAAAADsBoYdsRVOhh0dW1a4jFlfVtiR3mGjbUUMP95mof3sDbpjasdOEHlbVXknuNqPlwiQfWuP4k+foufEaZ89r6nm5aPB+AAAAAAAAAAAAAAAAAAAAAAAAdgJCD9mKTk5y2Yz6nknuPGj3XZPcYEb9siSnDNq9XldN1HffgBv2mKj3BtywLA9MstuM+tu7+/I1zv5Ykosnem6yxtmTVkND951o+/qo/QAAAAAAAAAAAAAAAAAAAAAAAOw8hB6y5XT3ZUk+MNH2gEHr7z9RP2n1vh3RVIje/htww9SOSzbghmV5wkT9DWsd3N1XJHnfRNtRa50/h9tmOqDyjIH7AQAAAAAAAAAAAAAAAAAAAAAA2EkIPWSrOmGi/ohBe39uov7uQXuX4ZsT9YM24IabT9TP3oAb1q2qDkzy0BktZyY5fp1r3jVRP3qd89cz+/Iknxm4HwAAAAAAAAAAAAAAAAAAAAAAgJ2E0EO2qjdP1O9UVbde5sKqum2S281o6UzftZm+PFG/7wbccL+J+tSNO4r/J8m2GfWXdPeV69zxliRXzajfsqputc4d2/PAifrHuvt7g3YDAAAAAAAAAAAAAAAAAAAAAACwExF6yJbU3V9M8qGJtmcsee2vTdQ/2N1fWfLOZfrYRP3gqjps1PKqul2SgybaPjlq/7JU1aFJnjyj5cIkr1zvnu4+K8kJE21PWe+eH1ZVN0vysIm245e9FwAAAAAAAAAAAAAAAAAAAAAAgJ2T0EO2sr+ZqP9SVR24jEVVdVCSX5ho+7tl7Brog3P0PHvg/ufM0TPPjZumqiorz93uM9pe0N3fXdLKV0/Uf6mq9lzSru/71STbJnreuuSdAAAAAAAAAAAAAAAAAAAAAAAA7KSEHrKV/b9JzplRv0GSFy5p14uS7DGjfvbqPTus7v50ks9PtD2xqg5b9u6qOjLJ4ybazujuTy5795K9KMm9Z9S/nOTPlrjvrUk+N6O+b5I/WdayqrpNpoMvP9rdn1jWTgAAAAAAAAAAAAAAAAAAAAAAAHZuQg/Zsrr7siR/PtH2i1X1s+vZU1WPTvL4ibYXd/fl69xzcFX1xOv569mR5PUT9W1J/k9V3Xide/5DVe2f5C2Z/vfoDcvaOUJV/VaS35zR0kl+db3PwQ8M7L46yR9PtD25qo5d766q2i3JazI73DNZbqgjAAAAAAAAAAAAAAAAAAAAAAAAOzmhh2x1L07ytYme11TVXdcyvKqOTvLXE21nZDp8cUfx8iSXTfTcJsnbq2rf9S6rqgOS/FOSQyZar0jysnXuOqiq7rOeGduZu3tVvSzJiyZaX9rdxy97f5K/T3LKRM+bq+on17qgqq6f5O1JjppoPTU7eDglAAAAAAAAAAAAAAAAAAAAAAAAG0voIVtad1+S5Dcm2vZK8u6qeugis6vq4UmOT7LnROuzuvvSRWZvlu4+J/MFNN47ycer6l5r3VVV90/y8STzBE7+ZXefudZdqw5K8r6q+lhVPamqpn63SatBgicnedpE68lJnrPefdemu69O8pQkV89ou2GS46rql6uqFplfVYcmeXeSYydar0ryjO7uReYDAAAAAAAAAAAAAAAAAAAAAACwtQk9ZMvr7jcnef1E295J/rGqXldVh81qrKrDq+oNSd6W5EYTc1/X3W+Z+9gdw+8n+dIcfTdP8q9VdWJVPaiqdpt6Q1XtXlUPq6r3JzkhyU3n2HNmkt+Zo29eRyb52yTfqqp3VNV/q6pbzPvmqtqnqp5YVSclOTHJbSfe8uUkD+3uy9Z88YTuPjnJ8ybadk/yqiQfqaoHT/1eVXVIVf1xktOS3HOOM17Q3R+c62AAAAAAAAAAAAAAAAAAAAAAAACuM7Zt9gGwQZ6c5M5Jbj2jp5I8Psnjq+pjST6YlcC6i5LsleSWSe6R5A5z7vxMkqes9eDN0t2XVNUjs/L5rz/HW35y9XVZVX04ySeSnJvkvKx8p/sm2S/JHZMclZXwvXldnuSR3X3hAu+Z1x5JHrr6SlV9J8nHknw6K7efn+Ti1b59khyS5HZJbp/5A2O/nuTY7j57mYdvxx8luVtWP88Md0lyXJILq+qErARcnp3k0iT7ZyWI8h6ZDnO8prclef5i5wIAAAAAAAAAAAAAAAAAAAAAAHBdIPSQ64TuvqiqfirJSUluPsdb7rj6WquvJvmp7r5oHTM2TXd/vKoekeStWQn9m8ceSe6z+lqGK5I8urs/sqR5U/ZJcr/V1zJ8NskDu/urS5o3U3d3VT0mK4GGx8zxlr2SPGIJq09M8vjuvnoJswAAAAAAAAAAAAAAAAAAAAAAANhidtnsA2CjdPcZWQm0++LgVV9Icr+NCrsbpbvfleTYJOdswvrzkjy0u/9xE3Yvw9uS/MRGPwPdfUmShyR55watfGOSh3T3pRu0DwAAAAAAAAAAAAAAAAAAAAAAgJ2M0EOuU7r7C0mOSnL8oBXvSnJUd48OVtwQ3f3+JHfOyufaKO9JcqfuPmEDdy7L+Ume3N0/293nbcYBq8GHD03y/CRXDVpzaZKnd/dju/uKQTsAAAAAAAAAAAAAAAAAAAAAAADYAoQecp3T3d/p7mOTPCnJOUsae06SJ3b3g7r7/CXN3CF095nd/aAkP5PkYwNXfSLJz3X3/bv7jAHzL0xywYC5yUoI4J8mObS7Xzlox9x6xe8luVOS9y1zdJI3JblNd//FEucCAAAAAAAAAAAAAAAAAAAAAACwRQk95Dqru1+T5JAkT0vy6TWOOX31/bfs7r9f1m07ou5+e3ffKckxSV6d5FtLGHtukr9Jcr/uPrK737KEmdequ09L8l+S/GRWAgo/keSq9YzMSgjkM5PcrLuf1d3nrfvQJeruT3b3fZPcO8kbk1y+xlHfTPLSJId192MGhVICAAAAAAAAAAAAAAAAAAAAAACwBVV3b/YNsEOoqh9PcmySOyU5IsmPJNkryQ2SXJLkwiRnZiXo8NQk/9zdn9+cazdfVVWS2yW5++qfP57kZkkOyMp3tvtq6+VZ+f7OTvKNJJ9P8qkkH0ry8d7Ef4Sqas8kd0ly5yS3SnJoklsk2TvJDbPyOS5PclGS85J8Icnnkpyc5D3dffYmnL1mVXWDrIRW3jvJ4Vn5zfZLsmeS3ZJcmuSCJF9L8sUkpyT5QJKPdPfVm3DyTqeqTsvKd/sDDj/88Jx22mmbcBEAAAAAAAAAAAAAAAAAAAAAALCjOOKII3L66adfW+n07j5io+/ZKNs2+wDYUXT357ISaMccVsMKP7n62il190VJ3rf62vK6+5Ik71x9AQAAAAAAAAAAAAAAAAAAAAAAwHC7bPYBAAAAAAAAAAAAAAAAAAAAAAAAAMDWJPQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAD/Pzv3Hq19XdZ5/HPxPPAgQiiIiGkheCBTwAOIlYqOkoc00ykPWTZNSYGHMsvsoI6ai5rykBSmdrClho2tdBJPoFmOgScGDVwxlkqaeABFBEQFvvPHs2sRsu/fvfe+r3s/7Of1WmsvFuv67u/13Xvf8OcbAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAcJCufAABAABJREFUAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALTYvtkPAAAAAAAAAAAAAADYVR36q2ds9hMAAABYoE+f8ojNfgIAAAAAwG5nj81+AAAAAAAAAAAAAAAAAAAAAAAAAACwNYkeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWmzf7AfArqaqdiS5c5LbJdkvyT5JrkrytSSfTXLhGOObm/fCXVNVHZTk9kkOyc7f295JRpKrs/N3d3GSz4wxLtm0R65BVW3Lzp/n0CQHZufnoLLzZ7k8ySeTXDTGuG6z3ggAAAAAAAAAAAAAAAAAAAAAAAC7OtFDSFJVxyV5dJKHJfneJNtmHL+2qi5I8rYkbxljnNP/wl1PVd0tySOSPDDJPZMcNOf3fSHJuUnem+SMMcYFXW9cq6o6Msljkxyf5N7ZGTqc5eqq+lCSdyR58xjj470vBAAAAAAAAAAAAAAAAAAAAAAAgJuWPTb7AbCZqurxVfWRJGcneXaSIzM7eJiV+ZFJfjXJ2VX14ap6XO9Ldw1VtaOqfraqPpbkH5OckuQHM2fwcMXB2RmX/O0k51fVeVX101W11+JfPK2qtlXVT1TVeUk+muS5Se6f6eBhkuyd5H5JfivJBVX1nqp6eNtjm1XVE6tqzPl1/Ga/FwAAAAAAAAAAAAAAAAAAAAAAgF2f6CG7pao6oqr+LslfJLnnBq+7V5LTq+pvq+ouG3/drqmqfizJJ5K8KsndF3j1UUn+OMmFVfXoBd47aSXcd16SP195x0Y9MMkZVfW2qjpsAfctTVUdmORlm/0OAAAAAAAAAAAAAAAAAAAAAAAAthbRQ3Y7VfWYJB9Kcv8FX318kg9X1Y8s+N5NVVX7VdXpSd6Y5PaNqw5N8tdV9WdVtU/jnlTV9qo6Jcl7ktytYcXDkpxbVY9quLvLS5MctNmPAAAAAAAAAAAAAAAAAAAAAAAAYGsRPWS3UlUnJ3lTkn2bVuyb5K+q6qSm+5eqqg5O8vdJHrfEtU9O8p6qOqDj8pWg4luSPDtJdexYsX+SN1fV0xt3LERVnZDkJzb7HQAAAAAAAAAAAAAAAAAAAAAAAGw9oofsNqrqyUlekd7QXVbuP7WqfrJ5T6uq+o4kZyY5ehPW3yfJO1YChQtTVTuSnJHk4Yu8d9bKJC+vqp9b0r41W/kdv3Kz3wEAAAAAAAAAAAAAAAAAAAAAAMDWJHrIbqGqjk3y6swXPPyHJE9Ncs8kByTZc+Wf907y9CQfmGdlkldX1THrevCu4bVJ7j7n2SuS/HmSpyS5R5LvTHKzJPskuV2SeyX5+SRvSPL1Oe88JskfreG98/izJMfPefZfkrwkyQlJjkhyi+z8eQ5L8v1Jfj07PwtjjrtOraoHre2pS/OiJHfY7EcAAAAAAAAAAAAAAAAAAAAAAACwNW3f7AdAt6r6jiSnZ2e8cJZPJPn5Mca7b2T2lSQfWfl6RVWdkOQPkxw+4769kryxqo4eY1y+9pdvnqp6QpJHz3H060lekOS0McZXVznzbytf5yZ5ZVUdmORpSZ6Tnb+jWZ5UVX85xvibuR4+Q1WdlOTxcxz9tyS/MsZ4wyrzT618/UOSF1fVfZKcmp1RzNVsS/IXK5+Fi9fw7FYrUc6nb/Y7AAAAAAAAAAAAAAAAAAAAAAAA2Lr22OwHwBK8IMkdJs6cleSYVYKH32aM8a7sjNz97cTROyR5/jx37iqqanuSF81x9P8lOXaMccqM4OG3GWNcOsZ4fpLvS/LpOb7llKra0P+rqurQJL87x9F3JTliRvDw24wxPpDk2CSnTBy9dZKXz3tvt5W/82uyM8h4Q5dlZ+gTAAAAAAAAAAAAAAAAAAAAAAAANkT0kC2tqu6a5OSJY2cn+eG1hPuSZIxxWZJHJvngxNGnVdX3rOXuTfbIJIdNnPl8khPGGOevd8kY4yNJTkhyycTRu66c24hTktxs4szfJHnUGOOKtV4+dnpOkudOHP3RqnrwWu9v8itJjlxl9qtJLl/iWwAAAAAAAAAAAAAAAAAAAAAAANiiRA/Z6p6XZPuM+ZeTPG6McdV6Lh9jXJnkx5JcNuPY9kzH8HYlPz7HmZ8YY1y00UVjjE8k+ak5js7zphu1Epx83MSxTyZ5whjjG+vdkyRjjBcmefPEsRdvZMciVNWdk/zmKuN/SPKqJT4HAAAAAAAAAAAAAAAAAAAAAACALUz0kC2rqg5L8tiJY78xxvjMRvasxP+eN3HsR6vq0I3sWYaqqiQPnDj27jHGWYvaOcY4I8nfTxz7LxtYcfLUE5L8t5WA5SKcmOSSGfNjqurBC9q1Zit/41cl2ftGxt9KcuIYYyz3VQAAAAAAAAAAAAAAAAAAAAAAAGxVoodsZScn2TZj/onsjL8twh8m+eSM+bZMx/d2BYcmOWDizGsa9k79HQ6pqu9c66VVtS3J4yeOvX2MMRVdnNsY44tJXjZx7JmL2rcOP5PkAavMfm+Mcf4yHwMAAAAAAAAAAAAAAAAAAAAAAMDWJnrIlrQSu3vCxLGXjjGuXcS+McY1SX5/4tgTq2pX/2/u8In5dUnOath7ZpIxceaO67j3B5IcOHHmtHXcO+U1Sb45Y35CVd2mYe9MVXVIkt9ZZfypJC9Y4nMAAAAAAAAAAAAAAAAAAAAAAADYDezqATZYrwclOWTG/Ookr1vwztdmdujutkmOX/DORbvlxPziMcYli146xvhiks9PHLvVOq5+wMT8i0neto57ZxpjfCHJ22cc2ZbkiYveO4dTk9xildlJY4yvL/EtAAAAAAAAAAAAAAAAAAAAAAAA7AZED9mqHjkxP2OM8bVFLhxjXJbZobtk+l2bbcfEfOHBw+v50sT8Zuu485iJ+dljjOvWce883j8x/+GmvTeqqh6d5DGrjN84xnjHEp8DAAAAAAAAAAAAAAAAAAAAAADAbkL0kK3qwRPzM5r2Tt37kKa9i/LVifmVjbun7r58HXceMTE/Zx13zusDE/P7VtV+jfv/Q1Xtn+QPVhlfluQXlvEOAAAAAAAAAAAAAAAAAAAAAAAAdj+ih2w5VXVIku+ZOHZW0/ozJ+bfW1W3adq9CJdOzA9s3D1199Tb/pOqqiS3nzj28bXcuUYXTMz3THJ84/7r+50kt11l9pwxxueX9A4AAAAAAAAAAAAAAAAAAAAAAAB2M6KHbEXHTsw/M8b4TMfiMcank1w8ceyYjt0L8k9Jxoz5wY27p+7+5Brv2y/JjokzX1njnWtxWWb/LpPkuMb9SZKqun+Sn11lfHaSP+p+AwAAAAAAAAAAAAAAAAAAAAAAALsv0UO2ontOzM9t3v/hifk9mvev2xjjy0kumHHkFlV190Xvraqjkuw/48gnxxhTMckb2meOM5et8c65jTGuTXLFxLGpz+qGVNWOJK9KUjcyvibJiWOMqTAjAAAAAAAAAAAAAAAAAAAAAAAArJvoIVvR0RPzjzXvn7p/l40ernjbxPzhDTsfMTF/9zru3HOOM19fx71rMXV/a/QwyXOT3GWV2UvGGP/YvB8AAAAAAAAAAAAAAAAAAAAAAIDdnOghW9GdJ+afaN7/zxPzOzXv36g/THLtjPkzqupmi1pWVfsmecYcb1qrb8xxZv913LsWU/ffuqpa3lBVRyb55VXGn07yPzr2AgAAAAAAAAAAAAAAAAAAAAAAwPWJHrKlVFUlOXTi2FSUcKOm7j+0ef+GjDEuSvL6GUcOSfKbC1z5giS3njF/zxjjvHXce+UcZ26xjnvnUlV7J9kxx9HDG3bvkeTVSfZc5chJY4yrFr0XAAAAAAAAAAAAAAAAAAAAAAAAbkj0kK3m4CR7T5z5XPMbpu6/eVXNivztCp6Z5Asz5s+pqp/c6JKq+rkkvzjjyNVJTlrP3WOMK5NcMXHsoPXcPad5/8aHNex+epJjV5n95Rjj7Q07AQAAAAAAAAAAAAAAAAAAAAAA4NuIHrLV3HaOM59vfsM898/zzk0zxrg0yZOSfHPGsT+tqhdX1fa13l9VO6rqZUlOmzj6y2OMC9d6//V8dmJ+7w3cPWXeuxf6Waiq707yolXGX03yC4vcBwAAAAAAAAAAAAAAAAAAAAAAALOIHrLVHDgxv3yM8Y3OB4wxrkpyxcSxqXduujHGWUl+LMk1qxzZI8lzkvxjVT2lqvaZurOq9quqpya5IMkzJo6/aIxx6lrefCM+PjE/boP3z3KfOc8t+rPwyiQ3X2X2a2OMixe8DwAAAAAAAAAAAAAAAAAAAAAAAFa1fbMfAAt2wMT88qW8YueefWfMp965SxhjvKWqHpjk9Um+a5VjRyT5oySvqKpzk3woyReTfCVJJbllkoOzMwJ4dKb/v/OtJL8+xvifG/4Bko8kecyM+b2r6pZjjK8sYNcNPWTOcwuLHlbVk5I8dJXxOdkZRNwtVdXJSU5awqrDl7ADAAAAAAAAAAAAAAAAAAAAAADgJkP0kK3mlhPzry3lFdN7bhLRwyQZY/yfqjoqyQuT/GySHasc3SvJcStf63VBkv8+xvjABu64vr+bmO9I8lNJXrqgfUmSqrpPknvMeXwhn4WqulVW/zmuSXLiGOO6Rey6iTooyV03+xEAAAAAAAAAAAAAAAAAAAAAAAC7mz02+wGwYHtPzK9cyiuSKybmU+/cpYwxLhtjPC3JYUlOSfLPC15xbpInJDlygcHDJDknyaUTZ36+qrYtcGeSPHUNZxf1WXhZklutMnvpGONjC9oDAAAAAAAAAAAAAAAAAAAAAAAAcxM9ZKvZa2J+zVJeMb1n6p27pDHG55K8MMmzknxwAVd+NMlxY4x7jTFOH2Nct4A7/8MY49okp08cu1OSZy9qZ1Udn+TH1/AtG/4sVNVDZ+y8KMnzN7oDAAAAAAAAAAAAAAAAAAAAAAAA1kP0kK1G9LBJVd2xql6d5JIkb05y7AKuPSrJ+6vqfVX101W15wLuvKFXznHmeVV19EYXVdX+Sf40Sa3h2zb0Waiqm2f2z3jyGOOqjewAAAAAAAAAAAAAAAAAAAAAAACA9RI9ZKuZ+kxfu5RXTO/ZtpRXLEBV7V9Vf5Lkn5L8TJKbLXjFtiQ/kOSPk/xLVf1cVa0lGjjTGOP8JH89cWyvJO+sqqPWu6eqDkhyZpJD1/itG/0s/FaS715l9qYxxhkbvB8AAAAAAAAAAAAAAAAAAAAAAADWbftmPwAW7JqJ+bI+81N7vrWUV2xQVd0vyeuSfNeSVt4+yWlJHltVTx5jfG5B9z4nycOT7Jhx5tZJ/raqfnqM8ea1XL4SS3xdkrut423fWMf3/PveY5M8bZXx5Umevt67t6AvJfn4EvYcntmfMwAAAAAAAAAAAAAAAAAAAAAAgN2K6CFbzTcn5sv6zO85MZ9656arqh9K8ldJ9prj+GeSvDPJ+5Kcl+TLSS5NUkkOSHJgkqOT3C/JDya53cR9D05yXlU9aIxx/jqe/5+MMS6squcm+e2Jo7dM8tdVdWaSFyV5/xjj2tUOV9Vdkzw1yVOSbFvl2DWZ/bm7euJNq+3eM8lrkuyxypFfG2NcvJ67t6Ixxh8k+YPuPVV1QZK7du8BAAAAAAAAAAAAAAAAAAAAAAC4qRA9ZKv51sR8noDfItyko4dV9dDMFzw8P8mLk/yvMcY1q5y5Kslnk3w0yWuranuSxyX5tcyOwx2U5N1V9cAxxsfX8v5V/G6S70/yqDnOPmTl69KVAOK/JvlCdv7dDk5ySJIHJLnjxD1vzc7g431nnFlX9DDJs5PcfZXZB5Octs57AQAAAAAAAAAAAAAAAAAAAAAAYGFED9lqrpiY77uUVyT7Tcyn3rlpquqQJK/PdPDwFUmeNcZYU8BxJY74+qp6U5KXJDlpxvFbJ/nfVXXUGOPKtey5kb3XVdUTk5yZ2RHC6zswyePXufLCJE9O8s6Jc19e68VVdZckv7HK+JokJ44xrlvrvQAAAAAAAAAAAAAAAAAAAAAAALBoe2z2A2DBpgJy37GUV0zvWXPobon+JMkBE2eeMcZ4+lqDh9c3xvjGGOPkJL80cfTwJL+33j032HllkhOSnLWI+2b45yQPGmN8OcneE2cvXsvFVVVJXpVkxypHXj7GOG8tdwIAAAAAAAAAAAAAAAAAAAAAAEAX0UO2mksn5rdYxiOS7D8xn3rnpqiqByV56MSxV4wxfn9RO8cYL0ly2sSxE6vqyAXtuyI7f8YXJ7luEXfewFlJjhtjfG7l36cCkp9f4/1PSXL/VWYXJXneGu8DAAAAAAAAAAAAAAAAAAAAAACANqKHbDWXTMx3VNUtOh9QVQck2Wvi2C4ZPUzySxPzf0vyKw17n5Xp+N8zF7VsjHHtGOPXkxyb5OwFXXt5kl9M8tAxxvX/vgdOfN9F8y6oqtsm+e0ZR546xrhy3vsAAAAAAAAAAAAAAAAAAAAAAACgm+ghW82/znHm4OY3zHP/PO9cqqo6JMnDJo791hjj6kXvHmNcleRFE8eesOhg5RjjI2OM70vywCRvSvL1dVzzpSQvTHKnMcbLxhjX/vtgJYC5Y+L7L1jDrlOT7L/K7K/GGG9dw10AAAAAAAAAAAAAAAAAAAAAAADQbvtmPwAWaYxxRVVdmuTAGce+O8mFjc84dGL+xTHGlY371+sBSWrG/Jokb2jc//okL0+ybZX5Xknul+RvFr14jPHeJO+tqptnZwDxvkmOTHKHJLdJcvMkeya5MslXkvxTkvOSvDPJ+8YY16xy9R0nVn95jHHxPG+sqh9I8iOrjC9P8ox57gEAAAAAAAAAAAAAAAAAAAAAAIBlEj1kK/pUZkcP75TkXY37p0J3n2rcvRH3m5h/cIzx1a7lY4zLqupDSY6bcez+aYgeXu8NVyZ568rXIkx9Fj66hrtuNWN2TpKHVc1qVq7ZvhPzR1TVzJ9vjPGaBb4HAAAAAAAAAAAAAAAAAAAAAACAmyDRQ7aiC5Lce8b8Ls37p+6/oHn/eh02Mf/gEt7wgcyOHh6xhDcs0r0m5n+3oD0nrHwt07PmOCN6CAAAAAAAAAAAAAAAAAAAAAAAsJvbY7MfAA3OnZjfo3n/PSfm/7d5/3odODH/0hLeMLVj6o27mmMm5u9dxiMAAAAAAAAAAAAAAAAAAAAAAABgs4geshVNRQ+PrqptHYuranuSoyaO7arRw1tOzC9ZwhumdtxkoodVtV+S+8w48rUk5yzpOQAAAAAAAAAAAAAAAAAAAAAAALApRA/Zij6c5OoZ832T3Ktp97FJ9pkxvzrJR5p2b9S1E/MdS3jD3hPzsYQ3LMoJSfaaMX/LGOMby3oMAAAAAAAAAAAAAAAAAAAAAAAAbAbRQ7acMcbVSd4/cewhTesfPDF/38r7dkVXTswPWsIbpnZctYQ3LMqTJuanL+UVAAAAAAAAAAAAAAAAAAAAAAAAsIlED9mqzpyYP6Zp73+dmL+rae8ifH5ifrslvOH2E/MvLOENG1ZVhyT5oRlHPpvknUt6DgAAAAAAAAAAAAAAAAAAAADw/9m531Dv77qO46+PjLbMpbPCsom6ia5yNbcW/SHNTXKDYZkhNKgZQSukgqzsRjeiG9Hu1CoZIWVZKN5wNi3IpYZLGBaj/aFaudk2W8Euca4252bFuxvnEN2o8zu7zu91znWu6/GAL7vx/Xw/7/c512E3nwDAkRE95HT1/g3vL11rvWKbA9dar0xy8R5HJpv3OkoPbHj/2kPY4YoN7zfteKr4xSRn7fH+t2bmP5/JhTNzy8ysw3qSPLRhpdfu4w4AAAAAAAAAAAAAAAAAAAAAAADOcKKHnJZm5tNJPrnh2E9teexPb3h/+8w8uOWZ23TnhvcvWWtd1Bq+1ro4yfkbjt3Tmr8ta60Lk1y/x5HHk7zzkNYBAAAAAAAAAAAAAAAAAAAAAACAIyV6yOnsXRve/+ha6+u2MWitdX6SH95w7A+2Mavo9n2c+bni/Lfv48x+djwya62Vnb+7s/c49qsz82+HtBIAAAAAAAAAAAAAAAAAAAAAAAAcKdFDTmd/lOTEHu+fneTXtjTrhiTn7PH+kd19Tlkzc2+S+zYcu26tddG2Z6+1LknyQxuOPTQz92x79pbdkOTVe7x/IMlvHNIuAAAAAAAAAAAAAAAAAAAAAAAAcOREDzltzcxTSX5zw7EfWWu98SBz1lpvTnLthmM3zszTB5zzkrXWbHh++SAzkrx3w/uzknxgrfW8A875H2utr0lyczb//+h925rZsNb6hSQ/v8eRSfKTB/07AAAAAAAAAAAAAAAAAAAAAAAAgONE9JDT3Y1J/nnDmXevtb7tZC5fa317kt/bcOyhbI4vnipuSvLUhjPfkOSDa63nH3TYWusFSf40yQUbjn4pyTsOOOv8tdZrDnLH/3Pv2WutdyS5YcPR356ZW7c9HwAAAAAAAAAAAAAAAAAAAAAAAE5looec1mbmySQ/u+HYuUn+fK11zTO5e631fUluTfKcDUffNjNffCZ3H5WZOZH9BRpfneSutdZ3n+ystdbrktyVZD/Byd+ZmYdPdtau85N8fK1151rrLWutTf9uG621rkxyR5K3bjh6R5K3H3QeAAAAAAAAAAAAAAAAAAAAAAAAHDeih5z2Zub9Sd674dhzk3xorfWetdZFex1ca33jWut9SW5J8pUb7n3PzNy872VPDb+S5J/2ce5FSf5yrfXRtdbVa60v2/TBWuvstdYb1lq3JflIkq/dx5yHk/zSPs7t1yVJfj/JZ9daf7LW+rG11ov3+/Fa67y11nVrrU8k+WiSV2745IEk18zMUye9MQAAAAAAAAAAAAAAAAAAAAAAABxTZx31AnBIrk9yWZJX7HFmJbk2ybVrrTuT3J6dYN0TSc5N8tIk35XkW/Y58x+S/MTJLnxUZubJtdabsvPzf/k+Prly93lqrfVXSe5O8rkkj2bnd/r8JF+V5FVJLk9y9jNY5+kkb5qZx5/BN/t1TpJrdp+stT6f5M4k92Zn98eSfGH33HlJLkhycZJvzv6Dsf+S5KqZeWSbiwMAAAAAAAAAAAAAAAAAAAAAAMBxIXrIGWFmnlhrvT7JJ5K8aB+fvGr3OVmfSfL6mXniAHccmZm5a631A0n+ODvRv/04J8lrdp9t+FKSN8/MX2/pvk3OS3LF7rMN/5jke2fmM1u6DwAAAAAAAAAAAAAAAAAAAAAAAI6dZx31AnBYZuah7ATtPl0edX+SK4577G5mPpzkqiQnjmD8o0mumZkPHcHsbbglyXce978BAAAAAAAAAAAAAAAAAAAAAAAAOCjRQ84oM3N/ksuT3Foa8eEkl89MO6x4KGbmtiSXZefnOiwfS3LpzHzkEGduy2NJrp+ZN87Mo0e9DAAAAAAAAAAAAAAAAAAAAAAAABw10UPOODPz+Zm5KslbkpzY0rUnklw3M1fPzGNbuvOUMDMPz8zVSb4/yZ3FUXcn+cGZed3MPFS4//Ek/164N0m+mOTXk1w4M+8szQAAAAAAAAAAAAAAAAAAAAAAAIBjR/SQM9bMvDvJBUnemuTek7zm73e/f+nM/OG2djsVzcwHZ+bSJN+T5HeTfHYL134uybuSXDEzl8zMzVu48/80M3+X5KuTXJmdQOHdSf7rIFdmJwL5M0leODNvm5lHD7woAAAAAAAAAAAAAAAAAAAAAAAAnEbOOuoF4CjNzBeS3JTkprXWy5NcleTSJN+U5OuTnJvk2UmeTPJ4koezEzr8myR/NjP3HeKuDyZZhzVvjz1uS3LbWuvHk1yc5Dt2//vyJC9M8oLs/M7O3v3k6ez8/h5J8q9J7kvyt0k+meSumZlD3P0/kvzF7pO11nOSfGuSy5K8LMmFSV6c5LlJvmL353g6yRNJHk1yf5JPJbkjycdm5pHD2v0I3JjkeXu8f/BQtgAAAAAAAAAAAAAAAAAAAAAAAOBYEz2EXTPzqewE7diH3VjhPbvPsTQzTyT5+O7D/zIzNx71DgAAAAAAAAAAAAAAAAAAAAAAABx/zzrqBQAAAAAAAAAAAAAAAAAAAAAAAACA05PoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAAAAAAAAAAAAAAAAAAAVoocAAAAAAAAAAAAAAAAAAAAAAAAAQIXoIQAAAAAAAAAAAAAAAAAAAAAAAABQIXoIAAAAAAAAAAAAAAAAAAAAAAAAAFSIHgIAAAAAAAAAAAAAAAAAAAAAAAAAFaKHAAAAAAAAAAAAAAAAAAAAAAAAAECF6CEAAAAAAAAAAAAAAAAAAAAAAAAAUCF6CAAAAAAAAAAAAAAAAAAAAAAAAABUiB4CAAAAAAAAAAAAAAAAAAAAAAAAABWihwAAAAAAAAAAAAAAAAAAAAAAAABAheghAAAAAAAAAAAAAAAAAAAAAAAAAFAheggAAAAAAAAAAAAAAAAAAAAAAAAAVIgeAgAAAAAAAPDf7NxpmKV3VS/s30qaJEBCyEQIgoYAMggkTAFUpgARFBVBZgX1eARBRUFEPE4veLjwVRFkUkEEFUQFITIIJEEGZR4CEuYpzAkkhEwkkLDOh6qWTtO9n1279r+qu3Lf17Wvrl1rPWutqtrJxx8AAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADLFtsw+APU1V7Z/k+5NcO8lBSa6S5KIk5yf5fJKPdvc3N+/CPVNVHZHkOkmOysrv7YAkneTirPzuvpTkc9391U07cg2qat+s/DxHJzksK5+DysrPcl6STyU5o7u/vVk3AgAAAAAAAAAAAAAAAAAAAAAAwJ5O6CEkqarbJbl3knsm+YEk+85ov6yqTk/ymiQndffbx1+456mqmyb5sSR3SXLLJEfM+dyZSd6b5I1JXt3dp4+6ca2q6uZJ7pvkzklunZWgw1kurqp3JXltkld094fGXji/qjokyc2y8nm+aZIbJzk8ydVXX1fNSiDlRUnOSvKFJB9J8v4k/9ndH93wowEAAAAAAAAAAAAAAAAAAAAAANhyhB5yhVZVD0zyuKyE9s1r3yQ3X339dlW9J8mfdPc/DThxj1JV+yd5aJJfzUqg3iKOzEq45D2T/HFVvT/JXyT5h+7+5lIOXYOq2jfJg5M8Nsmxa3z8gCR3WH3936r6jyR/2t2vWe6V06rqaknumOSEJHfNyt+nJh67yurr8CQ3SXL3HeZ9Lsk/JXnBnhRMCQAAAAAAAAAAAAAAAAAAAAAAwN5ln80+ADZDVd2oqt6U5B+ztsDDXblVkpdU1X9U1Q3Xf92eqarun+TjSf46iwce7sqxSf4myUer6t5LnDupqu6c5LQkf5e1Bx7uyl2SvLqqXlNVxyxh3kxVdURVPaKq3pDk7CSvTPIbWQnknAo8nHKdJL+Z5INV9aqquvU65wEAAAAAAAAAAAAAAAAAAAAAAHAFJPSQK5yquk+SdyW545JH3znJu6vqp5Y8d1NV1UFV9ZIk/5SVILxRjk7y8qp6QVVdZeCeVNW2qnpKkjckuemAFfdM8t6q+okBs1NVD6mqk5N8KclzshK2uG3ErlU/luQdVfWsqrrqwD0AAAAAAAAAAAAAAAAAAAAAAABsMUIPuUKpqkcleWmSAwetODDJy6rqkYPmb6iqOjLJm5M8YAPXPizJG6rq0BHDVwMVT0ry+CQ1Yseqg5O8oqp+bcDs30tytyT7Dpi9O/skeWSSd1XVjTdwLwAAAAAAAAAAAAAAAAAAAAAAAHsxoYdcYVTVw5I8I2OD7rI6/5lV9dDBe4aqqqslOTnJcZuw/rZJXrsaULg0VbV/klcn+dFlzp21MsnTq+oRG7RvI9w4yX9W1a03+xAAAAAAAAAAAAAAAAAAAAAAAAD2fNs2+wDYCFV1fJLnZr7Aw7cmefHqv59Jcn6Sg5Ick+QHkzwkK6F8M1cmeW5Vfbi737Xg2ZvthUluNmfvBUn+Ncl/JnlXkrOSnJOV38OhSY5McnySOyT5qSRXnmPmbZL8VZKfXdPVs70gyZ3n7P1kkpOSvDbJZ5N8Ock3k1wzyVGrc34iKz/X1OfqmVX1se5+w5ovXo6vJ3lfVn6mM5Kcl+SSJAcnOTzJDZLcMckhc847NMnJVXX77v7I8s8FAAAAAAAAAAAAAAAAAAAAAABgqxB6yJZXVVdL8pIkV5po/XiSX+7uU3dR+1qS96y+nlFVJyZ5dpLrzZi3X5J/qqrjuvu8tV++earqQUnuPUfrN5I8Mclzuvvru+n5wurrvUn+sqoOS/KrSZ6Qld/RLD9TVf/c3a+c6/AZquqRSR44R+sXkvxWd794N/VPr77emuTJVXXbJM9McusZM/dN8o+rn4UvreHsRV2a5NQk/57k9Uk+0t0964Gq2icrYZ6PzUow5T4TO66e5FWrP9MF674YAAAAAAAAAAAAAAAAAAAAAACALWkq1Aq2gicmue5EzylJbrObwMPv0t2vz0rI3X9MtF43yR/OM3NPUVXbkvzRHK0fS3J8dz9lRuDhd+nus7v7D5P8YJLPzPHIU1YD+RZWVUcn+dM5Wl+f5EYzAg+/S3e/I8nxSZ4y0XqNJE+fd+6CPpDkV5Ic1d336O6nd/eHpwIPk6S7v93db+vun05yiyQfmWPf9ZL8/+s7GQAAAAAAAAAAAAAAAAAAAAAAgK1M6CFbWlXdJMmjJtreluQn1xLclyTdfW6SH0/yzonWX62qG69l9ib78STHTPR8OcmJ3f3BRZd093uSnJjkqxOtN1ntW4+nJLnyRM8rk/xEd1+w1uG94glJfn+i9X5Vdbe1zp9an+SkJHfq7mO7+1ndPfU7nT2w+wNZCfV8xRztD9/LPt8AAAAAAAAAAAAAAAAAAAAAAABsIKGHbHV/kGTbjPo5SR7Q3RctMry7L0xy/yTnzmjblukwvD3JQ+bo+dnuPmO9i7r740l+bo7WeW7apdVAvgdMtH0qyYO6+5JF9yRJdz8p00GBT17Pjp28NMmx3X3v7n7zEudu/2w/MMnU3H2S/M4ydwMAAAAAAAAAAAAAAAAAAAAAALB1CD1ky6qqY5Lcd6Ltd7v7c+vZsxr+9wcTbferqqPXs2cjVFUluctE26ndfcqydnb3qzMdrHfXdax41NQJSX5+NeRvGR6e5Ksz6repqrutc8ebkty2u+/X3f+9zlm7tRoCeZ8kX5tovW9VXW3UHQAAAAAAAAAAAAAAAAAAAAAAAOy9hB6ylT0qyb4z6h9P8tdL2vXsJJ+aUd830+F7e4Kjkxw60fO8AXun/g5HVdX3rHVoVe2b5IETbf/e3VOhi3Pr7rOSPG2i7THr3PHw7n7nemasYdfZSZ440XblJCduwDkAAAAAAAAAAAAAAAAAAAAAAADsZYQesiWtht09aKLtz7v7smXs6+5Lk/zFRNuDq2pP/2/uehP1byc5ZcDek5P0RM/1F5j7w0kOm+h5zgJzpzwvyTdn1E+sqmsO2DvKc5JcONFz5w24AwAAAAAAAAAAAAAAAAAAAAAAgL3Mnh7ABos6IclRM+oXJ/mHJe98YWYH3V0re34w3CET9S9191eXvbS7z0ry5Ym2wxcYfaeJ+llJXrPA3Jm6+8wk/z6jZd8kD1723lG6+5JMh13ebCNuAQAAAAAAAAAAAAAAAAAAAAAAYO8i9JCt6scn6q/u7vOXubC7z83soLtk+q7Ntv9EfemBhzv4ykT9ygvMvM1E/W3d/e0F5s7jvybqPzlo7yhvnqgfsyFXAAAAAAAAAAAAAAAAAAAAAAAAsFcReshWdbeJ+qsH7Z2ae/dBe5fl6xP1Cwfunpp93gIzbzRRf/sCM+f1jon67avqoIH7l+3LE/Wrb8QRAAAAAAAAAAAAAAAAAAAAAAAA7F2EHrLlVNVRSW480XbKoPUnT9R/oKquOWj3Mpw9UT9s4O6p2VO3XU5VVZLrTLR9aC0z1+j0ifqVktx54P5l+8pE/cobcgUAAAAAAAAAAAAAAAAAAAAAAAB7FaGHbEXHT9Q/192fG7G4uz+T5EsTbbcZsXtJPpKkZ9SPHLh7avan1jjvoCT7T/R8bY0z1+LczP5dJsntBu5ftqtM1C/ekCsAAAAAAAAAAAAAAAAAAAAAAADYqwg9ZCu65UT9vYP3v3uifovB+xfW3eckOX1Gy9Wr6mbL3ltVxyY5eEbLp7p7KkxyZ1MhfclKMOEQ3X1Zkgsm2qY+q3uS60zURwZIAgAAAAAAAAAAAAAAAAAAAAAAsJcSeshWdNxE/QOD90/N32NDD1e9ZqL+owN2/thE/dQFZl5pjp5vLDB3Labm702hh1O3fnJDrgAAAAAAAAAAAAAAAAAAAAAAAGCvIvSQrej7J+ofH7z/ExP1Gwzev17PTnLZjPqjq+rKy1pWVQcmefQcN63VJXP0HLzA3LWYmn+Nqhp9w7pVVSW550TbBzfiFgAAAAAAAAAAAAAAAAAAAAAAAPYuQg/ZUlbD2Y6eaJsKJVyvqflHD96/Lt19RpIXzWg5KsnvLXHlE5NcY0b9Dd192gJzL5yj5+oLzJ1LVR2QZP85Wq836oYlOiHJNSd63rQRhwAAAAAAAAAAAAAAAAAAAAAAALB3EXrIVnNkkgMmer44+Iap+Vetqlkhf3uCxyQ5c0b9CVX10PUuqapHJPmNGS0XJ3nkIrO7+8IkF0y0HbHI7DnN+zc+ZuANy/LrE/VLkrx+A+4AAAAAAAAAAAAAAAAAAAAAAABgLyP0kK3mWnP0fHnwDfPMn+fOTdPdZyf5mSTfnNH2t1X15Krattb5VbV/VT0tyXMmWh/X3R9d6/wdfH6ifut1zJ4y7+w9+rNQVbdPcq+Jtld299c34h4AAAAAAAAAAAAAAAAAAAAAAAD2LkIP2WoOm6if192XjDyguy9KcsFE29Sdm667T0ly/ySX7qZlnyRPSPLfVfVLVXWVqZlVdVBV/UqS05M8eqL9j7r7mWu5eRc+NFG/3Trnz3LbOfv22M9CVe2T5GlztP754FMAAAAAAAAAAAAAAAAAAAAAAADYS23b7ANgyQ6dqJ+3IVes7DlwRn3qzj1Cd59UVXdJ8qIk37ubthsl+askz6iq9yZ5V5KzknwtSSU5JMmRWQkBPC7T/9/5VpL/091/su4fIHlPkvvMqN+6qg7p7q8tYdfO7j5n3x4bepjkMUmOn+h5Q3e/dSOOWY+qelSSR27AquttwA4AAAAAAAAAAAAAAAAAAAAAAIC9htBDtppDJurnb8gV03v2itDDJOnu/6yqY5M8Kcn/TrL/blr3S3K71deiTk/yv7r7HeuYsaM3TdT3T/JzSf58SfuSJFV12yS3mLN9j/wsVNXNsvI3n+WyJL+xAecswxFJbrLZRwAAAAAAAAAAAAAAAAAAAAAAAFzR7LPZB8CSHTBRv3BDrkgumKhP3blH6e5zu/tXkxyT5ClJPrHkFe9N8qAkN19i4GGSvD3J2RM9v1xV+y5xZ5L8yhp697jPQlUdmORfMn3bM7v7AxtwEgAAAAAAAAAAAAAAAAAAAAAAAHspoYdsNftN1C/dkCum90zduUfq7i8meVKS30zyziWMfH+S23X3rbr7Jd397SXM/B/dfVmSl0y03SDJ45e1s6runOQha3hkj/osVFUleX6SG060fizJ74y/CAAAAAAAAAAAAAAAAAAAAAAAgL2Z0EO2GqGHg1TV9avquUm+muQVSY5fwthjk/xXVb2lqn6hqq60hJk7+8s5ev6gqo5b76KqOjjJ3yapNTy2p30W/iDJ/SZ6vpXkod190QbcAwAAAAAAAAAAAAAAAAAAAAAAwF5M6CFbzdRn+rINuWJ6z74bcsUSVNXBVfX8JB9J8otJrrzkFfsm+eEkf5Pkk1X1iKpaS2jgTN39wSQvn2jbL8nrqurYRfdU1aFJTk5y9Bof3WM+C1X1gCS/P0fr47v7HaPvAQAAAAAAAAAAAAAAAAAAAAAAYO+3bbMPgCW7dKK+UZ/5qT3f2pAr1qmq7pDkH5J87watvE6S5yS5b1U9rLu/uKS5T0jyo0n2n9FzjST/UVW/0N2vWMvw1bDEf0hy0wVuu2SBZ5auqu6U5IVJpgInX9rdf74BJy3bV5J8aAP2XC+zP2cAAAAAAAAAAAAAAAAAAAAAAABXKEIP2Wq+OVHfqM/8lSbqU3duuqq6V5KXJdlvjvbPJXldkrckOS3JOUnOzkqA3qFJDktyXJI7JPmRJNeemHe3JKdV1Qnd/cEFzr+c7v5oVf1+kj+eaD0kycur6uQkf5Tkv7r7st01V9VNkvxKkl9Ksu9u2i7N7M/dxRM3DVdVt0hyUqbD+t6e5GHjL1q+7n5WkmeN3lNVpye5yeg9AAAAAAAAAAAAAAAAAAAAAAAAewuhh2w135qozxPgtwx7dehhVd0j8wUefjDJk5P8S3dfupuei5J8Psn7k7ywqrYleUCS38nscLgjkpxaVXfp7g+t5f7d+NMkP5TkJ+bovfvq6+zVAMTPJjkzK3+3I5McleROSa4/MedVWQl8vP2Mnk0NPayq70/y2iQHT7R+MMmPdfdF468CAAAAAAAAAAAAAAAAAAAAAABgqxB6yFZzwUT9wA25Ijlooj5156apqqOSvCjTgYfPSPKb3b2mAMfVcMQXVdVLkzw1ySNntF8jyb9V1bHdfeFa9uxi77er6sFJTs7sEMIdHZbkgQuu/GiShyV53UTfOQvOX7eq+t4kp2Tl9zzLJ5Oc2N2bdisAAAAAAAAAAAAAAAAAAAAAAAB7p302+wBYsqlQtqttyBXTe/bk8LjnJzl0oufR3f1raw083FF3X9Ldj0ry2InW6yX5s0X37LTzwiQnZiXob6RPJDlhNSTwgIneLw2+ZZeq6ppZ+T1cZ6L1C0nu3t2bcicAAAAAAAAAAAAAAAAAAAAAAAB7N6GHbDVnT9SvvhFHJDl4oj5156aoqhOS3GOi7Rnd/RfL2tndT03ynIm2h1fVzZe074Ks/IxPTvLtZczcySlJbtfdX1x9PxUg+eUBN8xUVYcmOTnJDSZav5Lkbt396fFXAQAAAAAAAAAAAAAAAAAAAAAAsBUJPWSr+epEff+quvrIA1YD5fabaNsjQw+TPHai/oUkvzVg729mOvzvMcta1t2Xdff/SXJ8krctaex5SX4jyT26e8e/72ETz52xpP1zqaqrJXldkptOtH4tyYnd/ZHxVwEAAAAAAAAAAAAAAAAAAAAAALBVCT1kq/nsHD1HDr5hnvnz3LmhquqoJPecaPu/3X3xsnd390VJ/mii7UHLDqzs7vd09w8muUuSlyb5xgJjvpLkSUlu0N1P6+7LthdWAzD3n3j+9AV2LqSqrprkNUluPdF6fpJ7dvdpw48CAAAAAAAAAAAAAAAAAAAAAABgS9u22QfAMnX3BVV1dpLDZrR9X5KPDjzj6In6Wd194cD9i7pTkppRvzTJiwfuf1GSpyfZdzf1/ZLcIckrl724u9+Y5I2roYB3SXL7JDdPct0k10xy1SRXSnJhkq8l+UiS05K8LslbuvvS3Yy+/sTqc7r7S+u9fx5VdUCSf0vyQxOtFyW5V3e/Y/xVAAAAAAAAAAAAAAAAAAAAAAAAbHVCD9mKPp3ZoYc3SPL6gfungu4+PXD3etxhov7O7v76qOXdfW5VvSvJ7Wa03TEDQg93uOHCJK9afS3D1Gfh/UvaM1NV7ZfkZUlOmGi9JMm9u/vN468CAAAAAAAAAAAAAAAAAAAAAADgimCfzT4ABjh9on7Dwfun5k/dt1mOmai/cwNueMdE/UYbcMMy3Wqi/qbRB1TVtiT/mORHJ1ovTXK/7j559E0AAAAAAAAAAAAAAAAAAAAAAABccQg9ZCt670T9FoP333Ki/r7B+xd12ET9Kxtww9SOqRv3NLeZqL9x5PKq2ifJ3yW5z0TrZUke0t2vHHkPAAAAAAAAAAAAAAAAAAAAAAAAVzxCD9mKpkIPj6uqfUcsrqptSY6daNtTQw8Pmah/dQNumNqx14QeVtVBSW47o+X8JG8fuL+SPC/JgyZaO8kvdPc/j7oFAAAAAAAAAAAAAAAAAAAAAACAKy6hh2xF705y8Yz6gUluNWj38UmuMqN+cZL3DNq9XpdN1PffgBsOmKj3BtywLCcm2W9G/aTuvmTg/mcl+fk5+h7R3X838A4AAAAAAAAAAAAAAAAAAAAAAACuwIQesuV098VJ/mui7e6D1t9tov6W1fv2RBdO1I/YgBumdly0ATcsy89M1F8yanFV/VmSX56j9de7+69H3QEAAAAAAAAAAAAAAAAAAAAAAABCD9mqTp6o32fQ3p+eqL9+0N5l+PJE/dobcMN1JupnbsAN61ZVRyW514yWzyd53aDdf5TkMXO0PqG7nz7iBgAAAAAAAAAAAAAAAAAAAAAAANhO6CFb1Usn6resqhsuc2FV3TTJzWa0dKbv2kyfnqjfZQNuOGGiPnXjnuK3k2ybUf+L7r502Uur6glJ/s8crU/s7qcsez8AAAAAAAAAAAAAAAAAAAAAAADsTOghW1J3fzLJ2yfafnXJa39tov7W7v7Mkncu0/sm6kdX1Y1GLa+qmyW59kTbB0btX5aqul6Sh89oOT/JXw/Y++gkT56j9U+6+w+WvR8AAAAAAAAAAAAAAAAAAAAAAAB2ReghW9nzJ+o/X1VHLWNRVV07yc9OtL1gGbsGeuscPb85cP/j5+iZ58ZNU1WVlc/d/jPantzdX1/y3v+d5GlztD6zu39rmbsBAAAAAAAAAAAAAAAAAAAAAABgFqGHbGV/n+SsGfWrJHnKknb9cZIDZtTPXL1nj9XdH07y8Ym2h1XVjZa9u6qOS/KgibYzuvsDy969ZH+c5I4z6p9O8ufLXFhVD0nyl3O0PjfJry1zNwAAAAAAAAAAAAAAAAAAAAAAAEwResiW1d0XJ3n6RNtDq+qn1rOnqu6f5METbU/r7kvWuefoquqJ1x+uZ0eSF0/UtyX516q6+jr3/I+qOiLJyzL9/6OXLGvnCFX1W0keN6Olk/zyej8HO+28d5IXZPp39/dJHtHdvazdAAAAAAAAAAAAAAAAAAAAAAAAMA+hh2x1T0vyuYmeF1bV8YsMr6rbJfmbibYzMh2+uKd4dpKLJ3punOSkqjp0vcuq6sgkr0pyzETrN5M8c527rl1Vd1rPjN3M3b+qnpnkjydan9Hdr1vi3hOT/FNWgihn+eckP9/d317WbgAAAAAAAAAAAAAAAAAAAAAAAJiX0EO2tO6+KMljJtoOSvL6qrrXWmZX1U8meV2SAydaH9vd31jL7M3S3WdlvoDGOyY5rarusOiuqrpbktOSzBM4+Zfd/flFd626dpI3VtX7qurnqmrq7zapqu6a5N1JHjXR+u4kj1/vvh323iHJy5PsN9F6UpKHdPdly9oNAAAAAAAAAAAAAAAAAAAAAAAAa7Ftsw+A0br7pVX14iQPntF2cJJ/q6p/TPKk7v7I7hqr6iZJfj/JA+ZY/6LuftmaDt58T0xyvyTHTPRdJ8mbq+rUJH+W5NTu/uasB6pq/yQ/kuSxWQlOnMfnk/zunL3zOC7J3yZ5TlWdkuQVSU7p7jPmebiqDknyE0l+MckPz/HIp5Pcq7svXuja795/3SSvSnKVidavJXl9kp+rqmWsnsebuvvjG7UMAAAAAAAAAAAAAAAAAAAAAACAPZ/QQ64oHp7kVkluOKOnshKM+OCqel+St2YlsO6CJAcluW6SH0py7Jw7P5LkEYsevFm6+6Kqum9Wfv4rz/HIXVdfF1fVO5K8P8nZSc7Jyu/00CSHJblFktsk2X8N51yS5L7dff4anpnXAUnutfpKVX0tyfuSfDgrt5+b5MLVvkOyEgJ5syQ3T7LPnDu+kOQe3X3mEu/+viRXm6PvkCTPWuLeefx8EqGHAAAAAAAAAAAAAAAAAAAAAAAA/A+hh1whdPcFVfUjSd6S5DpzPHKL1deiPpvkR7r7gnXM2DTdfVpV3SfJy7MS+jePA5LcafW1DN9Mcv/ufueS5k05JMkJq69l+GiSE7v7s0uaBwAAAAAAAAAAAAAAAAAAAAAAAHudfTb7ANgo3X1GVgLtPjl41SeSnLC3h91192uT3CPJWZuw/pwk9+ruf9uE3cvwiiQ/uLd/BgAAAAAAAAAAAAAAAAAAAAAAAGC9hB5yhdLdn0hymySvG7TitUlu092jgxU3RHe/KcmtsvJzbZRTk9yyu0/ewJ3Lcm6Sh3f3T3X3OZt9DAAAAAAAAAAAAAAAAAAAAAAAAGw2oYdc4XT317r7Hkl+LslZSxp7VpKHdfc9u/vcJc3cI3T357v7nknuneR9A1e9P8lPd/fduvuMAfPPT3LegLlJ8o0kT01yve7+60E7AAAAAAAAAAAAAAAAAAAAAAAAYK8j9JArrO5+YZJjkjwqyYcXHPOh1eev291/t6zb9kTdfVJ33zLJnZM8L8lXljD27CTPT3JCdx/X3S9bwsxd6u7Tkxye5K5ZCSh8f5LL1jMyKyGQj05yre5+bHefs+5DAQAAAAAAAAAAAAAAAAAAAAAAYAvZttkHwGbq7guTPDvJs6vq+5PcI8ktk/xAku9JclCSqyS5KMn5ST6flaDD9yb59+7++Abe+pkktVH7ZtzxpiRvqqpfSnKzJLdf/ff7k1wryZFZ+Z3tv/rIJVn5/Z2Z5ItJPp7kg0nenuS07u4NvP1bSd6w+kpVHZjk1kluleT6Sa6X5PuSHJzkqqs/xyVJLkhyTpJPJPlYkncnObW7z9yo27fr7jdmD/gcAAAAAAAAAAAAAAAAAAAAAAAAwDyEHsKq7v5YVgLtmMNqWOEHVl97pe6+IMkbV18AAAAAAAAAAAAAAAAAAAAAAADAku2z2QcAAAAAAAAAAAAAAAAAAAAAAAAAAFuT0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhi22YfwPyq6oZJ7pDkqCSHJ9k/ydeTfCrJu7v7PZt4HgAAAAAAAAAAAAAAAAAAAAAAAABcjtDDPVxVXTnJo5M8Msn3TPR+Oclzkzytu88dfx0AAAAAAAAAAAAAAAAAAAAAAAAA7J7Qw3Woqhsm2Xc35U9198XrnH+3JP+Q5IgkNccjRyX5vSSPqqpHdve/rGc/AAAAAAAAAAAAAAAAAAAAAAAAAKyH0MMFVdXRST6cpHdRPjfJddY5/xeTPCvJlVa/tas9u3w0yWFJXlJVN+/u31vPHQAAAAAAAAAAAAAAAAAAAAAAAACwKKGHi7vf6r+10/c7yfO6+6JFB1fVjyf5q9XZO4Yd7rxrZ71DfyX5naq6tLv/v0VvAQAAAAAAAAAAAAAAAAAAAAAAAIBF7bPZB+zF7p/vhAzuGDZ4aZK/WHRoVR2R5G9y+cDDynTg4c59vfr171fVjy16DwAAAAAAAAAAAAAAAAAAAAAAAAAsSujhAqrq8CS32v52h387ySnd/YV1jH9SksNz+cDDHe0ctLhz6OKOz2wPPnxGVe2/jpsAAAAAAAAAAAAAAAAAAAAAAAAAYM2EHi7mdjNqr1x0aFV9b5JfyOzAw+3f39VrV8GHSfJ9SZ6w6F0AAAAAAAAAAAAAAAAAAAAAAAAAsAihh4u5/Yzav61j7qOTbFv9esfQwl59VZJPJPmtJD+U5Iar//5akv/OdwcfZofnHldVB63jNgAAAAAAAAAAAAAAAAAAAAAAAABYE6GHizl+h6+3hwomyQe7+4uLDKyqKyX52ew+tDBJ/jLJTbv7T7v7bd398dV/n5nkFkn+JJcPPtwxOPGAJD+9yG0AAAAAAAAAAAAAAAAAAAAAAAAAsAihh4u5fnYdTnjaOmbePcnhq19vDyvcHnjYSV7R3Y/s7m/t6uHu/nZ3Pz7Jc3P54MMdPXQd9wEAAAAAAAAAAAAAAAAAAAAAAADAmgg9XKOq2jfJtXdT/sA6Rt9np/c7hhZemuTX55zz60nO3GnG9vDEO1TV4bt6CAAAAAAAAAAAAAAAAAAAAAAAAACWTejh2l07yb6rX9dOtfWEHt4jlw863D6/k7y0uz83z5Du/kaSp+5w2443VpJbreNGAAAAAAAAAAAAAAAAAAAAAAAAAJib0MO1+54ZtS8sMrCqbpTkWtvf7qLlb9c48p9n1I5b4ywAAAAAAAAAAAAAAAAAAAAAAAAAWIjQw7U7cEbt6wvO/OGd3vcOX5+T5A1rGdbdZyT56C5mJckt1nYaAAAAAAAAAAAAAAAAAAAAAAAAACxG6OHaXWVG7bwFZ+4cepgklZXAwtd397cXmPnB1Rk7z7zuArMAAAAAAAAAAAAAAAAAAAAAAAAAYM2EHq7drNDDCxacefusBBzuyusWnPmxnd5vn3/wgvMAAAAAAAAAAAAAAAAAAAAAAAAAYE2EHq7dlWbUDljrsKo6PMkNtr/dRct/rHXmqnN38/2rLTgPAAAAAAAAAAAAAAAAAAAAAAAAANZE6OHanT+jdtUF5t1xp/e9w9df6O7PLTAzSS7YzfeFHgIAAAAAAAAAAAAAAAAAAAAAAACwIYQert15M2qHLzDvzrv4XmUl/PAtC8zb7rLdfH+/dcwEAAAAAAAAAAAAAAAAAAAAAAAAgLkJPVy7r8+o3WCBeXfNSsDhrqwn9PCA3Xz/gnXMBAAAAAAAAAAAAAAAAAAAAAAAAIC5CT1cu7Nm1G66lkFVdf0kN97+dhctb17LvJ0cspvvCz0EAAAAAAAAAAAAAAAAAAAAAAAAYEMIPVyj7j4jyXnb3+5U/pE1jrvPzuN3+Por3f2hNc7b0bV2er89VPH8dcwEAAAAAAAAAAAAAAAAAAAAAAAAgLkJPVzMaflOiGCyElZYSW5fVTuHDc7yv/LdwYm1+r03ruO+JDlmF9/rJF9a51wAAAAAAAAAAAAAAAAAAAAAAAAAmIvQw8W8b4evdww/3JbkcfMMqKp7JbnBLmZsd+pip/2PH8h3ByomySfXORcAAAAAAAAAAAAAAAAAAAAAAAAA5iL0cDEn7eJ7nZXwwkdV1V1mPVxVV03y1Fw+lHDHry9L8spFj6uqayW55va3O5U/sehcAAAAAAAAAAAAAAAAAAAAAAAAAFgLoYcL6O43Jjlj+9t8J1iwk2xL8vKqesCunq2qa2Ql0PD627+1Y3l1xuu7+8vrOPGHZ9SEHgIAAAAAAAAAAAAAAAAAAAAAAACwIbZt9gF7sb9P8rtZCSlMLh98eLUkL66q381KwOFns/K7Pi7JfVfrOz6zs+ev87a7z6i9b52zAQAAAAAAAAAAAAAAAAAAAAAAAGAuQg8X99Qkv5TkiKwEHe4YYLj9/Q8kuclOz9VOPTt+3Un+u7v/dZ233SvfCWPsHb5/Znd/Zp2zAQAAAAAAAAAAAAAAAAAAAAAAAGAu+2z2AXur7j43yeNy+bDD5PJBhtvf7/jqfHdI4o5+ez13VdUdkxy50y3b975tPbMBAAAAAAAAAAAAAAAAAAAAAAAAYC2EHq5Dd/99klfmO6GC2+0ccLjja3v9f8bs0PuC7n7tOs962IzaW9c5GwAAAAAAAAAAAAAAAAAAAAAAAADmJvRw/R6U5C25fMjhdrWb13Y79r49ySPWc0hVXS3J/Xaau6NT1jMfAAAAAAAAAAAAAAAAAAAAAAAAANZC6OE6dfdFSe6e5Hn5Tqhhz/na3v8vSe7e3d9a5zm/kOTA1a+337HdF7r7/eucDwAAAAAAAAAAAAAAAAAAAAAAAABzE3q4BN39ze7+pSQnJPnPfCfMcPtru52/f3qSB3b3A1bDExdWVduSPDrfCTrcMfCwk7x6PfMBAAAAAAAAAAAAAAAAAAAAAAAAYK22bfYBW0l3vzHJHavqBkl+NMnxSY5Jcshqy9lJvpLknUlO7e53LHH9w5J834z6q5a4CwAAAAAAAAAAAAAAAAAAAAAAAAAmCT0coLs/nuTpG7z21CS3mFE/faMOAQAAAPh/7NxnmK13VTf+7zo5pEBCSEILRUICEiBA6E2BhI6CKIKA0iyAgKDC8yD6PHYQ/iDSRVQQFEUFAQU0FBGBSG8PQSI1oSaQkJBe1//FzHDmTGb2PbNnl3NmPp/ruq/Z+/6tvdbaM/c5L78AAAAAAAAAAAAAAAAAAACQCD3cMrr7q/PeAQAAAAAAAAAAAAAAAAAAAAAAAACW2zHvBQAAAAAAAAAAAAAAAAAAAAAAAACArUnoIQAAAAAAAAAAAAAAAAAAAAAAAAAwFUIPAQAAAAAAAAAAAAAAAAAAAAAAAICpEHoIAAAAAAAAAAAAAAAAAAAAAAAAAEyF0EMAAAAAAAAAAAAAAAAAAAAAAAAAYCqEHgIAAAAAAAAAAAAAAAAAAAAAAAAAUyH0EAAAAAAAAAAAAAAAAAAAAAAAAACYip3zXmC7qaoDk9wqyS2SXC/JdZNcNckBSfZLUoul3d33nMuSAAAAAAAAAAAAAAAAAAAAAAAAADABQg9noKpumeRnktwnya2zK9hwzY8k6THm7FjtfndfvtFeAAAAAAAAAAAAAAAAAAAAAAAAALBZq4bkMRlV9fCq+miSTyb5jSS3zcLvvEZc4856UJJLVruq6h828TUAAAAAAAAAAAAAAAAAAAAAAAAAYCxCD6egqn60qj6b5PVJbpPdAw174BpLd/9zks9m9SDFB1bV1cbtDQAAAAAAAAAAAAAAAAAAAAAAAADjEHo4QVW1T1W9OMl7k9w0u0IHV4YarhZMuDwYcVx/vPhz5bx9kzx8k70BAAAAAAAAAAAAAAAAAAAAAAAAYEOEHk5IVR2ShbDDp2Th97o87HBlsGHnisGEk/CGJN9d4+yxE5wDAAAAAAAAAAAAAAAAAAAAAAAAAIOEHk5AVR2a5D1J7prVww6TtUMOKxPS3Rcnef2ynku7VJLbV9V1JzULAAAAAAAAAAAAAAAAAAAAAAAAAIYIPdykqrpSkn9JcuziraVQw5Vhh0v3KsnlSU5P8t9JvrTic5v1+hFn957QDAAAAAAAAAAAAAAAAAAAAAAAAAAYJPRw8/4kyZ1zxWDDrLj3zSTPSXKfJAd39+HdfUyS505yme7+WJJTV8xfcq9JzgIAAAAAAAAAAAAAAAAAAAAAAACAUXbOe4G9WVX9SJJfzu7hhskVww5/O8lfd/clM1rtX5M8YdkevbjL8TOaDwAAAAAAAAAAAAAAAAAAAAAAAADZMe8F9nIvya6gw9UCD9+V5NjufvUMAw+T5D3LXtey19eqqqNmuAcAAAAAAAAAAAAAAAAAAAAAAAAA25jQwzFV1QOSHJuFkMPlgYdLr9+Q5H7d/d3Zb5cPjTi72cy2AAAAAAAAAAAAAAAAAAAAAAAAAGBbE3o4vieueL8UeNhJTkzy2O7umW+VpLu/nmQpbHHlDkfPeB0AAAAAAAAAAAAAAAAAAAAAAAAAtimhh2OoqqsluW92BQouDxa8JMljuvviWe+1wuezEMK4ktBDAAAAAAAAAAAAAAAAAAAAAAAAAGZC6OF47pHkSouva9nPTvLn3f3leSy1wpfWuH+jmW4BAAAAAAAAAAAAAAAAAAAAAAAAwLYl9HA8PzLi7GUz22K0b69yr5IcMutFAAAAAAAAAAAAAAAAAAAAAAAAANiehB6O52bLXvey16d098mzXmYN313xfmnPg2a9CAAAAAAAAAAAAAAAAAAAAAAAAADbk9DD8RyZ3cMOa/H9f8xlm9VdsMZ9oYcAAAAAAAAAAAAAAAAAAAAAAAAAzITQw/FcfY3735rpFqNdssZ9oYcAAAAAAAAAAAAAAAAAAAAAAAAAzITQw/FcZY37p890i9Guusb9nukWAAAAAAAAAAAAAAAAAAAAAAAAAGxbQg/Hs88a9y+Z6RajHbrG/QtmugUAAAAAAAAAAAAAAAAAAAAAAAAA25bQw/Gcv8b9w2a6xWhrhR6eM9MtAAAAAAAAAAAAAAAAAAAAAAAAANi2hB6O59w17u9JoYdHrXhfSTrJN+awCwAAAAAAAAAAAAAAAAAAAAAAAADbkNDD8XwtCyGCK9141ouspqr2SXKnLIQcrnTqjNcBAAAAAAAAAAAAAAAAAAAAAAAAYJsSejier6x431kIQbxrVa0WhjhrxyY5cPH1yn0+N9tVAAAAAAAAAAAAAAAAAAAAAAAAANiuhB6O5zPLXi8PFTwoyW1mvMtqHjji7KMz2wIAAAAAAAAAAAAAAAAAAAAAAACAbU3o4Xg+OOLsCTPbYhVVtX+SX07Si7d62fHlSf5r5ksBAAAAAAAAAAAAAAAAAAAAAAAAsC0JPRzPR5Kcu/h6ebhgJXlUVV1rLlsteEySayy+rmU/O8mJ3f29uWwFAAAAAAAAAAAAAAAAAAAAAAAAwLYj9HAM3X1Rkn/O7qGCS/ZN8vyZL5Wkqg5P8vvZFcS40ptmuA4AAAAAAAAAAAAAAAAAAAAAAAAA25zQw/G9dsX7ykLYYCX52ap67CyXqaodSf4uyTVW7LPkgiSvm+VOAAAAAAAAAAAAAAAAAAAAAAAAAGxvQg/H1N3vSvLppbfLj7IQOPiyqnrADFd6RZK7LZu/ZCn88G+6+6wZ7gMAAAAAAAAAAAAAAAAAAAAAAADANif0cHN+J1cMGEwWQgavnOQtVfX4aS5QVQdV1T8k+aVcMXxxyQVJ/mCaewAAAAAAAAAAAAAAAAAAAAAAAADASkIPN6G7/znJ27MQdrgUMljL3u9M8qdV9e6qOmbS86vqEUk+m+Qh2RW4uDKEsZM8v7u/Men5AAAAAAAAAAAAAAAAAAAAAAAAADCK0MPNe2KS7yy+7hVnnYXgweOSfLKq3l5Vj6qqg8YdVlU3r6rfrKovJvmbJNfPrnDDpcDDXnbv40n+cNx5AAAAAAAAAAAAAAAAAAAAAAAAADCunfNeYG/X3d+oqkckOSELIZJLgYNLoYNL7/dJcr/F67LF0MLPJbnKWr2r6veT7J/kmkmOSHLLJAcvHS+tsMb7JPlekkd092Xjf0MAAAAAAAAAAAAAAAAAAAAAAAAAGI/Qwwno7n+vqkcn+evsCjtcCj5MrhhMuDPJ0UlusqxNrfLzt1aMqmWve5X7y+dckORB3f2lDX0ZAAAAAAAAAAAAAAAAAAAAAAAAAJiQHfNeYKvo7r9L8ugklyzdWna8PJRw+bU8GHE1teJa/tnl51lx79wkP9HdJ475dQAAAAAAAAAAAAAAAAAAAAAAAABg04QeTlB3/22S45Ocnt1DCpPdwwt/8JHsHo54hZZZO+hwZZ+ls28kOa673z32FwEAAAAAAAAAAAAAAAAAAAAAAACACRB6OGHdfWKSY5L8Q3YFE44KLqyVPZZZWbeydqnn0tmbk9yyuz++6S8CAAAAAAAAAAAAAAAAAAAAAAAAAJsk9HAKuvuM7n54kvsk+Wh2DyzsXDEEcd2tV1xLfT+f5IHd/ZDu/t7mvwEAAAAAAAAAAAAAAAAAAAAAAAAAbJ7Qwynq7nd3952S3DPJPyS5JLuCCtcKQRx1ZcXn/zPJQ5Mc091vn8FXAgAAAAAAAAAAAAAAAAAAAAAAAIB12znvBbaD7n5vkvdW1UFJ7p3kfknumOSm2djf4NtJPpLk3Une0t1fn/SuAAAAAAAAAAAAAAAAAAAAAAAAADApQg9nqLvPSfJPi1eqat8kN0py/STXSXJQkgOSXCnJRUnOT3JGklOTfLm7vz2HtQEAAAAAAAAAAAAAAAAAAAAAAABgLEIP56i7L07yucULAAAAAAAAAAAAAAAAAAAAAAAAALaUHfNeAAAAAAAAAAAAAAAAAAAAAAAAAADYmoQeAgAAAAAAAAAAAAAAAAAAAAAAAABTIfQQAAAAAAAAAAAAAAAAAAAAAAAAAJgKoYcAAAAAAAAAAAAAAAAAAAAAAAAAwFQIPQQAAAAAAAAAAAAAAAAAAAAAAAAApkLoIQAAAAAAAAAAAAAAAAAAAAAAAAAwFUIPAQAAAAAAAAAAAAAAAAAAAAAAAICpEHoIAAAAAAAAAAAAAAAAAAAAAAAAAEyF0EMAAAAAAAAAAAAAAAAAAAAAAAAAYCqEHgIAAAAAAAAAAAAAAAAAAAAAAAAAUyH0EAAAAAAAAAAAAAAAAAAAAAAAAACYip3zXmDWquqydZR1d4/83ayzz55m8HsBAAAAAAAAAAAAAAAAAAAAAAAAwKRsxwC82sP6AAAAAAAAAAAAAAAAAAAAAAAAAMCWtB1DD5OkR5xtJMxwVJ89jZBGAAAAAAAAAAAAAAAAAAAAAAAAAGZqu4YeJquHAI4TYrg3hAnuTeGMAAAAAAAAAAAAAAAAAAAAAAAAAGwRO+a9AAAAAAAAAAAAAAAAAAAAAAAAAACwNe2c9wJz1HtYHwAAAAAAAAAAAAAAAAAAAAAAAADYUrZr6GHtYX0AAAAAAAAAAAAAAAAAAAAAAAAAYMvZjqGHv7eH9QEAAAAAAAAAAAAAAAAAAAAAAACALWnbhR5290TCCifVBwAAAAAAAAAAAAAAAAAAAAAAAAC2qh3zXgAAAAAAAAAAAAAAAAAAAAAAAAAA2JqEHgIAAAAAAAAAAAAAAAAAAAAAAAAAUyH0EAAAAAAAAAAAAAAAAAAAAAAAAACYCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMBUCD0EAAAAAAAAAAAAAAAAAAAAAAAAAKZC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMBVCDwEAAAAAAAAAAAAAAAAAAAAAAACAqRB6CAAAAAAAAAAAAAAAAAAAAAAAAABMhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAqhB4CAAAAAAAAAAAAAAAAAAAAAAAAAFMh9BAAAAAAAAAAAAAAAAAAAAAAAAAAmIqd815gb1ZV10zy8DWOv93d/zDjfR6a5PA1jl/T3efMch8AAAAAAAAAAAAAAAAAAAAAAAAAtjehh5vzxCS/s8bZr89ykUXXSfLCNc4uS/LyGe4CAAAAAAAAAAAAAAAAAAAAAAAAwDa3Y94L7K2qakeSX0xSq1zfSfJnc1jrVUm+u8ZOT5zDPgAAAAAAAAAAAAAAAAAAAAAAAABsY0IPx3e/JNdL0suuLP58cXdfOOuFuvuCJC9etsfyvW5WVXeZ9U4AAAAAAAAAAAAAAAAAAAAAAAAAbF9CD8f3oDXud5K/muEeK/1VdgUdrvSTM9wDAAAAAAAAAAAAAAAAAAAAAAAAgG1O6OH47pvdwwVr8f0Huvtb81kp6e5vJnn/4j7LVRZ2BgAAAAAAAAAAAAAAAAAAAAAAAICZEHo4hqr64SQ3WHq74vgNM15nNX+/7PVSGGOS3LyqrjOHfQAAAAAAAAAAAAAAAAAAAAAAAADYhoQejueOI87eM7Mt1vbuEWd3mdkWAAAAAAAAAAAAAAAAAAAAAAAAAGxrQg/Hc9Nlr3vZ67O7+wuzXmalxR3OXnq74vimAQAAAAAAAAAAAAAAAAAAAAAAAIAZEHo4nputeF9ZCBf8xBx2WcvHs7DXSkIPAQAAAAAAAAAAAAAAAAAAAAAAAJgJoYfjOXKN+5+f6RajnbzKvUpy41kvAgAAAAAAAAAAAAAAAAAAAAAAAMD2JPRwPAcn6VXuf2/Wi4xw5or3S/seMutFAAAAAAAAAAAAAAAAAAAAAAAAANiehB6O56A17q8MGpyntXY5cKZbAAAAAAAAAAAAAAAAAAAAAAAAALBtCT0cz1qhh+fOdIvRzlvj/lq7AwAAAAAAAAAAAAAAAAAAAAAAAMBECT0cz6Vr3D9gpluMtv8a9/ed6RYAAAAAAAAAAAAAAAAAAAAAAAAAbFtCD8dz/hr3D53pFqOttcsFM90CAAAAAAAAAAAAAAAAAAAAAAAAgG1L6OF4zlrj/g1nucSAI9e4f+5MtwAAAAAAAAAAAAAAAAAAAAAAAABg2xJ6OJ5Tk9SKe5XktnPYZS23SdLL3i/t+4057AIAAAAAAAAAAAAAAAAAAAAAAADANiT0cDxfXfF+KVzw6Kq65ox3uYLFHW66ylEn+cqM1wEAAAAAAAAAAAAAAAAAAAAAAABgmxJ6OJ5PLntdK17/9Ix3Wc1Ds2uvWnF20ox3AQAAAAAAAAAAAAAAAAAAAAAAAGCbEno4ng+vcb+SPGWWi1xhgapK8uQkvUbJf81wHQAAAAAAAAAAAAAAAAAAAAAAAAC2MaGH4/l4krMWX3cWwg6XQgZvUlWPmsdSix6V5OjF18v3SpKLk5w4840AAAAAAAAAAAAAAAAAAAAAAAAA2JaEHo6huy9N8i9ZCBXc7Wjx3h9X1bVmvVdVHZ7kBdk96DDZFX74zu4+d9Z7AQAAAAAAAAAAAAAAAAAAAAAAALA9CT0c3+tXvF8egHj1JG+uqv1ntUxVHZDknxZnr9xnyd/Mah8AAAAAAAAAAAAAAAAAAAAAAAAAEHo4pu5+Z5LPLb1d/FnLXt8xyTuq6uBp77I44x2LMzu7Ag97WdkpSd407V0AAAAAAAAAAAAAAAAAAAAAAAAAYInQw835o+wKGFyyFHxYSe6e5LNVdc9pLbDY+/8ludtaJYv7PK+7L5/WHgAAAAAAAAAAAAAAAAAAAAAAAACwktDDTeju1yf5r6W3y46WBx9eN8kJVfXKqrrRpGZX1Y2q6pVJTkhyvRUzs+x1J/lUkldNajYAAAAAAAAAAAAAAAAAAAAAAAAArIfQw817QpILF1+vFnzYWfg9/1KSz1fVv1XVQ6rqsI0OqqrDFj97QpLPL/bcsWzO8sDDJRcn+cXu7gAAAAAAAAAAAAAAAAAAAAAAAADADO2c9wJ7u+7+bFU9LcmrsnvYYLJ78GEtXvdevFJVX07y4ST/k+SsZVeSXG3Z9cNJ7pjkyBW9s2xmZXdLs3+1uz85zncDAAAAAAAAAAAAAAAAAAAAAAAAgM0QejgB3f0XVXVUkmfmiiGEy4MPl99PkqOye5DhKCtDDXuNs+X3X9Ddf7bO/gAAAAAAAAAAAAAAAAAAAAAAAAAwUUIPJ6S7n1VVByR5anaFHC4PPkx2Dz/MirPBEavcWysIsZL8cXc/c529AQAAAAAAAAAAAAAAAAAAAAAAAGDidsx7ga2ku381ya9nV/jgypDDWnEtrxm61vr88h6V5LIkT+7u/zW5bwYAAAAAAAAAAAAAAAAAAAAAAAAAGyf0cMK6+0VJ7p7ky7lisOFKK0MMh64rjMvugYj/neSu3f2nE/gqAAAAAAAAAAAAAAAAAAAAAAAAALApQg+noLs/mOSWSX4vybm5YvjhWiGIg61X+XwlOTvJs5Lcprs/Ov7mAAAAAAAAAAAAAAAAAAAAAAAAADA5Qg+npLsv6O7fS3Jkkt9O8rUsBBQuXckVQwyHrqzo8eUkz0xyw+5+XndfNP1vBgAAAAAAAAAAAAAAAAAAAAAAAADrs3PeC2x13X1Gkj+sqmcnuWuSH09y7yTHJLnSBttdnOQzSd6Z5G3d/aFJ7goAAAAAAAAAAAAAAAAAAAAAAAAAkyT0cEa6u5N8YPH6jaraN8nNkxyV5PpJrpXkykkOWPzIBUnOT/LtJF9L8qUkJ3X3JTNeHQAAAAAAAAAAAAAAAAAAAAAAAADGIvRwTrr74iSfXLwAAAAAAAAAAAAAAAAAAAAAAAAAYMvZMe8FAAAAAAAAAAAAAAAAAAAAAAAAAICtSeghAAAAAAAAAAAAAAAAAAAAAAAAADAVQg8BAAAAAAAAAAAAAAAAAAAAAAAAgKkQeggAAAAAAAAAAAAAAAAAAAAAAAAATIXQQwAAAAAAAAAAAAAAAAAAAAAAAABgKoQeAgAAAAAAAAAAAAAAAAAAAAAAAABTIfQQAAAAAAAAAAAAAAAAAAAAAAAAAJgKoYcAAAAAAAAAAAAAAAAAAAAAAAAAwFQIPQQAAAAAAAAAAAAAAAAAAAAAAAAApkLoIQAAAAAAAAAAAAAAAAAAAAAAAAAwFTvnvcB2VlVXSXJAkv2S7DODkRd192kzmAMAAAAAAAAAAAAAAAAAAAAAAAAAQg9noaqOSHJ8klsnOSbJEUmunWTfGa/ysSR3nPFMAAAAAAAAAAAAAAAAAAAAAAAAALYpoYdTUlXXSfK4JI9KcuOVx7PfaK5zAQAAAAAAAAAAAAAAAAAAAAAAANiGhB5OWFVdPcnvZyHwcN+sHjTYM10KAAAAAAAAAAAAAAAAAAAAAAAAAOZA6OEEVdXDkrw8yaHZFXY4KuBwtUDESesZzQEAAAAAAAAAAAAAAAAAAAAAAACA3eyY9wJbRVU9O8nfJTksCyGDnV2Bh7XGBQAAAAAAAAAAAAAAAAAAAAAAAABb1s55L7AVVNUfJXnm4ttefrTs9fL7q50vt1rt0GdGfXZUPwAAAAAAAAAAAAAAAAAAAAAAAACYCqGHm1RVj85C4OF6wg6HAguH6nod/dY7AwAAAAAAAAAAAAAAAAAAAAAAAACmSujhJlTVDyV5WXYFEa4MHOxl976d5IQkpyy+vn2Sxy2rWf7z5xdfH5zkkCSHJjkyyZ0X3y/1Xh6AuDT/0iR/kuRzq6z83Q1+RQAAAAAAAAAAAAAAAAAAAAAAAAAYm9DDzXl2kgOze7hhsnsI4keTPKO737/8g1V1SRZCD6+gu1+71sCqulmSByZ5UpLrZ/fgw87C3/RXkvxGd79kI18GAAAAAAAAAAAAAAAAAAAAAAAAACZpx7wX2FtV1VFJHpndQweTXQGIleQ3u/uOKwMPN6O7P9fdz0tywyQPS3JKrhi4uH+SP6mqP62qWqUNAAAAAAAAAAAAAAAAAAAAAAAAAEyd0MPxPTG7wgaXfi4FHnaSX+vu505reHdf3t1vTHLLJK/NFYMPK8njk/z1tHYAAAAAAAAAAAAAAAAAAAAAAAAAgFGEHo7vYVkIF1yyPPDwzd394lks0d3ndvfjkvzSGvs8oqp+dxa7AAAAAAAAAAAAAAAAAAAAAAAAAMByQg/HUFXHJLn+0tvsHjZ4bpInz3qn7v7LJE9Z3OcHtxff/5+quvOsdwIAAAAAAAAAAAAAAAAAAAAAAABgexN6OJ67rHJvKfzw77v7tBnvkyTp7lcmeW2uGHy4I8krq8rfGwAAAAAAAAAAAAAAAAAAAAAAAICZEYI3nluPOPuLmW2xul9N8r1V7h+T5MEz3QQAAAAAAAAAAAAAAAAAAAAAAACAbU3o4XhuvOx1L3v9/e7+yGabV1WN+9nuPjvJS5Ks1uNpYy8FAAAAAAAAAAAAAAAAAAAAAAAAABsk9HA818vuYYe1+P6TE+q/c5Off3V236+zsONdq+oam+wNAAAAAAAAAAAAAAAAAAAAAAAAAOsi9HA8V1/j/mc20KNHnB2wgT5XbNz9tSQnZSHocLlKcp/N9AYAAAAAAAAAAAAAAAAAAAAAAACA9RJ6OJ61QgnP3ECPS0ecXWUDfdby4TXu33ECvQEAAAAAAAAAAAAAAAAAAAAAAABgkNDD8ey3xv2zNtDj4hFnh26gz1q+vMb9m0ygNwAAAAAAAAAAAAAAAAAAAAAAAAAM2jnvBfZS5yU5cJX7F26gx/kjzq6V5KQNbXRFZ61430kqyVGb7LvlVdV+SX44yfWSHJTkyln4e52T5OtJTu7uUaGV21JVXSPJ9ZMcnoXf2/5ZeO4uzMLv7ltJvtbd353bkhtQVftk4fsckeSwLDwHlYXv8v0sBIue0t2Xz2vHcVXVYUmOzsL3OijJPln4Xmcn+UJ3f22O6wEAAAAAAAAAAAAAAAAAAAAAALCFCD0cz/ezeujhVTfQY1Tw2w9tbJ0NOXiKvfdaVXWnJA9Ocv8kN89CCNxaLquqk5K8I8lbu/tD099wz1NVxyT5sSTHJblNkmus83OnJflEkv9I8vbu3mzA58RU1S2TPCTJPZLcLgtBh6NcWFUfTfJvSd7S3Z+b7objqaorJ/mJLPy97pnk2gP15yT5QJK3J3lTd3976ksCAAAAAAAAAAAAAAAAAAAAAACwJQk9HM85a9y/2gZ6fGfE2VEb6LOWq61xf7Wwxm2rqh6e5H9lIbRvvfZJcsvF6zeq6uNJnt/dfz+FFfcoVbVfkkcn+ZUktxizzbWyEC55/yTPq6pPJ3lJkr/p7osnsugGVNU+SR6Z5OlJbrXBj++f5EcXr2dX1XuTvKC73zHZLcdTVYdk4fl+QpJDN/DRg7Lrb/TCqvr7JM/p7s9PfksAAAAAAAAAAAAAAAAAAAAAAAC2sh3zXmAvdXqSWuX+wRvocUqSyxdf94qzY8fYaaXrr3F/5axtqaqOrqr3Jfm7bCzwcDW3TfKGqnpvVd1k89vtmarqYUm+kORVGT/wcDW3SvKXSU6uqgdPsO+gqrpHkk8leV02Hni4muOSvL2q3lFVR06g39iq6tFJTk7yrGws8HClfZM8Ksmnq+o5i8GXAAAAAAAAAAAAAAAAAAAAAAAAsC5CD8dz8hr3r7PeBt19cZJTV97OQpjincfca7m1epw5gd57tar6qSQfTXK3Cbe+R5KPVdVPTrjvXFXVQVX1hiR/n7XDNCfhiCRvrqq/qqorT3FOqmpnVT03yb8nOWYKI+6f5BNV9aAp9B6pqvarqlcneW2Sa0yw9b5ZCFD8z6pa9/91AAAAAAAAAAAAAAAAAAAAAAAAbG9CD8fz+RXvl8IKb7XBPp9c/FyW/UySQ6rqrmPulqq6QZJbLO71g9uLP88Yt+9WUFVPTvLGJAdOacSBSd5UVU+aUv+ZqqprJfnPJD8zw7GPSfLvVXXoNJovBiq+Nckzs/u/u0k7OMlbquqpU5yxm6o6IMkJSR43xTF3SPLRqjpqijMAAAAAAAAAAAAAAAAAAAAAAADYIoQejmdl6OGSI6pqI2F6Hxpx9vMb6LPSk7Prb7s81K2TfGMTffdqVfWYJC/NdIPustj/ZVX16CnPmaqqumqSdyU5dg7j75jk3xYDCiemqvZL8vYkD5hk31Ejk7y4qp449UFVV0ry5iR3n/asJNdJ8p6qut4MZgEAAAAAAAAAAAAAAAAAAAAAALAXE3o4nk8se10rXt9yA33eucq9Xuzzc1V1zEYXq6pbJHnqYp/VfGCjPbeCqrpDkj/P+gIPT0zylCS3SXJokist/rxdFn63H17PyCR/XlW3H2vhPcNrk9xinbXnJnldkscnuXWS6yY5IMmVk1wvyW2T/HKSv01ywTp73j7Jn21g3/X4qyT3WGftl5K8MMl9khyd5GpZ+D5HJrlrkt/KwrOw1r+15V5WVcdvbNUNe36S+66j7vwkf53kp7PwXa6SZP8s/M3ul+SPk5y2jj43SPKmxbBFAAAAAAAAAAAAAAAAAAAAAAAAWJXQwzF092lJ/nvp7Yrje26gz6eTnLKsTy17faUk/1RVB6+3X1VdN8kbk+y7dGuVsveut99WUVVXTfKGLPxOR/lCknt19127++Xd/cnu/l53X7r48+Pd/dLuvlMWwuW+NNBv3yR/vzh/r1JVj0jy4HWUXpDkWUmu192P6e4/7+5Pdfc3u/vC7r6gu7/R3Z/o7ld2988muX6S30ty8Tr6/1xVPXDsL7JMVT0pycPXUfqNJD/b3Tfq7qd397u6++TuPnvx+3ylu0/s7ucsPgt3TvKxgZ77JPm7qjp8k19jVVX1oCRPW0fp65PcqLsf3d1vWvwu53f3RYt/sxO6+xlJbpjkt5NcOtDvDkmeu7ntAQAAAAAAAAAAAAAAAAAAAAAA2MqEHo7vvbliqGBlfUFxy71mRZ/lwYc3SvKxqrrTUJOqul+SE5PcOFcMUFzy3SQf2eB+W8HvZyHEbZR3J7l9d79nPQ27+51JbpfhEMkbJvnd9fTcU1TVziR/uI7S/0lyh+5+bnefvd7+3X1Gd/9ukrsk+eo6PvLcqtrU/1VVdUSSF6yj9J1Jju7uv11v7+7+cNYX/nfNJC9eb9/1qqqrJHnFQFkneUp3/1x3f2uo52K44x8kOT7J9wfKf7Wqbr2+bQEAAAAAAAAAAAAAAAAAAAAAANhuhB6Ob3nYXWVXuOCxVXX9DfR5VZILF18v9VgeWHhUkg9U1X9W1a9U1T2r6uZVdXRV3a2qnl5VH0jy9iRrzV3a70XdfdkGdtvrVdXNkjx5oOy/kvzERoL7kqS7z0rywAwHSf5KVd10I73n7IFJjhyo+XaS+3T3Z8cd0t0fT3KfLIRxjnKzxbrNeG6SAwZq/iXJg7r73I027wXPSvLbA6UPrap7bbT/gN9Kct2Bml/v7pdvtHF3vz/JA5JcPKJsR5KXbrQ3AAAAAAAAAAAAAAAAAAAAAAAA24PQw/G9M8lFi697xdlPrbdJd387ycuzK+hwyfLgwx1J7prkRYtzP5PkpCwEL/5/Se6cXcGGveKzS76f5GXr3WsL+Z0kO0ecn5nkZ7r7/HGad/d5SR6W5KwRZTszHIa3J/nZddQ8qrtP2eyg7v5Ckseuo3Q9O61qMXDyZwbKvpzkEd190UDdSN39B0neMlD2nM3MWK6qDkvytIGyN3b3i8ad0d0fTPK/B8ruWlX3HXcGAAAAAAAAAAAAAAAAAAAAAAAAW5fQwzF19zlJ/jW7hxUuBQ7+SlWtDDEc5XeTfHVZjyXLwwtr4OoVn1neo5M8bXHnbaOqjkzykIGy/9PdX9vMnMXwv98ZKHtoVR2xmTmzsPjcHjdQ9p7ufvekZnb325P850DZPTcx4slDKyR53GKA5SQ8Icl3R5zfvqruNaFZT0ly5RHn5yzWbNZLknx8oGYoGBEAAAAAAAAAAAAAAAAAAAAAAIBtSOjh5rxhlXud5IZJfmq9TRbD1h6Z5KJlPZYsDz4cdVWuGMC49PNV3f269e6zhTw5yT4jzr+Q5FUTmvWKJF8ecb5PhsP39gRHJDl0oOYvpjB36O9weFVdd6NNq2qfJA8fKPvX7h4KXVy37j49yYsGyn59s3OqameSJw2UvaC7T9vsrO7uDIcaHl9Vx2x2FgAAAAAAAAAAAAAAAAAAAAAAAFuL0MPN+Zck52dX4ODy6xkbadTdH0ryiCQXL93KruDC1fqvvHZrt+xz/5rkqRvZZStYDLt7xEDZn3T3ZZOY192XJnnJQNkjq2pP/zd31MD55UnePYW578ruYZ+rudEYfX8kyWEDNX86Rt8hf5Fd/5ZXc5+quvYmZ9wnyTVHnF+Q5OWbnPED3f3vST45UPazk5oHAAAAAAAAAAAAAAAAAAAAAADA1rCnB7Dt0br7giSHJDlgleseY/R7a5L7JvlWdgUZdnYPQFzz48uupSDElyV5cHdfstFdtoDjkxw+4vzCJH8z4Zmvzeigu+tkjOdixg4ZOP9Wd3930kO7+/Qk3x4ou/oYre8+cH56kneM0Xek7j4tC4Gja9knySM3OWYoYPCfuvuMTc5Y6c8Hzh9ZVStDWAEAAAAAAAAAAAAAAAAAAAAAANjGhB5uUndf0t0XrXaN2e8/k9wiySuSXJJdAYbJ7sGGK68sq/1/Se7V3U/t7kvH/W57uQcOnL+9u8+Z5MDuPiujg+6S4b3mbb+B84kHHi7znYHzA8boefuB8//q7svH6LseHxw4/4lxG1fVjiQPGCh7w7j9R/jHJKP+T/mhJLecwlwAAAAAAAAAAAAAAAAAAAAAAAD2UkIP90Dd/b3ufkqSI5I8M8mJ2T0AcbXrtCSvS3Kf7r5Vd//7HFbfk9xr4PztU5o71PfeU5o7KWcPnJ83xdlDvb8/Rs+jB84/NEbP9frwwPmdq+qgMXvfLsnVRpxflOQ9Y/ZeU3d/N8lHBsr29GccAAAAAAAAAAAAAAAAAAAAAACAGdo57wVYW3d/O8nzkzy/qvZNcqMkN0hyUJJ9k1yQ5DtJvtTd35jbonuYqjo8yU0Hyt49pfHvGji/eVVde/Fvuyc6Y+D8sCnOHuo9tNtuqqqSXH+g7HMb6blBJw2cXynJPZL8yxi97zlw/sHuvmCMvuvxriR3GXF+ryQvmNJsAAAAAAAAAAAAAAAAAAAAAAAA9jJCD/cS3X1xFgLaphnStlXcYeD8a939tWkM7u6vVtW3khw+ouz2GS/obhY+n6ST1Brn15ri7KHeX95gv4OS7DdQ870N9tyIszL6d5kkd8p4z8LQM/7BMXqu14kD57ef4mwAAAAAAAAAAAAAAAAAAAAAAAD2MjvmvQBMwW0Gzj8x5fkfGzi/9ZTnj627z0xy0oiSq1XVLSY9t6puleTgESVf7u5vbbDtlddRc9YGe65bd1+W5NyBsqFnddzPTfMZH3q+D62qH5rifAAAAAAAAAAAAAAAAAAAAAAAAPYiQg/Zio4dOP/MlOcP9d9jQw8XvWPg/AFTmPljA+fvGaPnldZRc8EYfTdiqP+GQw+r6tAkQ6GCU3vGF4MxvzFQtqc/4wAAAAAAAAAAAAAAAAAAAAAAAMyI0EO2oh8eOP/ClOd/ceD8xlOev1mvSHLZiPOnVdUBkxpWVQcmedo6dtqoi9ZRc/AYfTdiqP81q2qjOww9P5ckOWWDPTdqb3/GAQAAAAAAAAAAAAAAAAAAAAAAmJGd815gb1RVj0py6zWOT+ruv5zlPuxSVZXkiIGyocC2zRrqf8SU529Kd59SVa9P8ug1Sg5P8n+T/OaERv5+kmuOOP/37v7UGH3PW0fN1cbouy5VtX+S/dZRelSST2yg9Q0Hzr/S3aNCKyfhi0nuPuJ8aEcAAAAAAAAAAAAAAAAAAAAAAAC2CaGH43l8kruscfbQWS7CFVwryf4DNd+c8g5D/a9SVdfs7tOnvMdm/HqS+2bh97maZ1XV57v7dZsZUlVPTPJrI0ouTPKkcXp393lVdW6SA0eUXWOc3us0KshxuSOzsdDDIwbOp/18r2eG0EMAAAAAAAAAAAAAAAAAAAAAAACSJDvmvcBe6ogktcp1dpK3zm8tklxnHTXfnvIO6+m/nj3nprvPSPJzSS4eUfaaqnpOVW04PLWq9quqFyX504HS/9XdJ2+0/zJfHzi/3SZ6D1lv740+C0P1036+1zNjj36+AQAAAAAAAAAAAAAAAAAAAAAAmB2hh+M5JEkvu7L48/3dfdnctiJJDhs4/353XzTNBbr7/CTnDpQN7Tl33f3uJA9LcukaJTuSPCvJ/6uqx1fVlYd6VtVBVfWUJCcledpA+R9298s2svMqPjdwfqdN9h/ljuus2+izMFR/+gb7jeO0gfM9/vkGAAAAAAAAAAAAAAAAAAAAAABgNnbOe4G91L5r3P/sTLdgNYcOnH9/JlsszDlwxPnQnnuE7n5rVR2X5PVJfmiNsqOT/FmSl1bVJ5J8NAvBe99LUlkICb1WFkIAj83w/zuXJPmt7n7+pr9A8vEkPzXi/HZVdUh3f28Cs1a69zrrNhoQuCc840Mz9rjnu6qenORJMxh11AxmAAAAAAAAAAAAAAAAAAAAAAAA7DWEHo7nvCRXXeX+6bNehCs4ZOD8nJlsMTxnjwuFW0t3f6CqbpXkD5L8UpL91ijdN8mdFq9xnZTkF7r7w5vosdz7Bs73S/LYJH8yoXlJkqq6Y5Jbr7N8o8/CnvCMD824clXt190XzWCX9bpGkpvNewkAAAAAAAAAAAAAAAAAAAAAAIDtZse8F9hLrRX4df5Mt2A1+w+cnzeTLZJzB86H9tyjdPdZ3f0rSY5M8twkX5zwiE8keUSSW04w8DBJPpTkjIGaX66qfSY4M0mesoHajT4Le8IzPvR8J3vZMw4AAAAAAAAAAAAAAAAAAAAAAMB0CD0cz9eS1Cr3hXzN374D55fOZIvhOUN77pG6+5tJ/iDJM5J8ZAItP53kTt192+5+Q3dfPoGeP9DdlyV5w0DZjZM8c1Izq+oeSX52Ax/Z6LOwJzzj65mxVz7jAAAAAAAAAAAAAAAAAAAAAAAATJbQw/GcvMb9a850C1azJwTCrWfOXhcIV1U3qqo/T/LdJG9JcocJtL1Vkg9W1fur6uer6koT6LnSK9dR8ztVdexmB1XVwUlek9VDUdci9BAAAAAAAAAAAAAAAAAAAAAAAIAtS+jheD61xv2bzHIJVjX0TF82ky2G5+wzky0moKoOrqpXJ/l8kl9McsCER+yT5EeS/GWSL1XVE6tqI6GBI3X3Z5O8eaBs3yQnVNWtxp1TVYcmeVeSIzb40Y0+C3vCM76eGXvNMw4AAAAAAAAAAAAAAAAAAAAAAMD07Jz3Anupd65430kqyV3nsAu7u3TgfFbP/NCcS2ayxSZV1Y8m+ZskPzSjkddP8qdJHlJVj+nub06o77OSPCDJfiNqrpnkvVX18939lo00XwxL/Jskx4yx20UbrN8TnvH1zNjTnvHvJPncDOYcldHPGQAAAAAAAAAAAAAAAAAAAAAAwLYi9HAM3f35qvpSkiNXHF27qu7U3R+ax14kSS4eOJ/VM3+lgfOhPeeuqn48yZuS7LuO8q8lOSHJ+5N8KsmZSc7IQhjooUkOS3Jskh9Nct8k1xvod68kn6qq47v7s2Osv5vuPrmqfjvJ8wZKD0ny5qp6V5I/TPLB7r5sreKqulmSpyR5fJJ91ii7NKOfuwsHdlppT3jGh57vZA97xrv75UlePu05VXVSkptNew4AAAAAAAAAAAAAAAAAAAAAAMDeQujh+F6Z5PlJesX9X04i9HB+Lhk4X0+A3yTs1aGHVXW/rC/w8LNJnpPkH7v70jVqzk/y9SSfTvLaqtqZ5GeS/GZGh8NdI8l7quq47v7cRvZfwwuS3DXJg9ZRe+/F64zFAMRTk5yWhb/btZIcnuTuSW400OdtWQh8vPOImo2GHu4Jz/heF3oIAAAAAAAAAAAAAAAAAAAAAADAfAg9HN9fZiG07WqL7ztJJXlkVb2wuz89r8W2uXMHzg+cyRbJQQPnQ3vOTVUdnuT1GQ7Pe2mSZ3T3hsLtFsMRX19Vb0zywiRPGlF+zST/XFW36u7zNjJnlbmXV9Ujk7wro0MIlzssycPHHHlyksckOWGg7swN9t0TnvGh57uzEHYJAAAAAAAAAAAAAAAAAAAAAADANrdj3gvsrbr7rCS/lYWgw+X2SfLXVTWrcD12NxQgd9WZbDE8Z6NBd7P06iSHDtQ8rbufutHAw+W6+6LufnKSpw+UHpXkj8eds2LmeUnuk+Tdk+g3wheTHN/dZybZf6D2WxvsvSc840Mzzu7uy2awBwAAAAAAAAAAAAAAAAAAAAAAAHs4oYeb82dZCE9bCj7sxZ83T/JPVXXluWy1vZ0xcH61WSyR5OCB86E956Kqjk9yv4Gyl3b3SyY1s7tfmORPB8qeUFW3nNC8c7PwHZ+T5PJJ9Fzh3Unu1N3fXHw/FCD57Q323xOe8aEZe+TzDQAAAAAAAAAAAAAAAAAAAAAAwOwJPdyE7u4kD01ycnYPPqwk90zy/kkFtbFu3x0436+qrjbNBarq0CT7DpTtqaFwTx84/0aS/z2Fuc/IcPjfr09qWHdf1t2/leQOSf5rQm2/n+TXktyvu5f/fQ8b+NwpG5wz9Ixfe4P9xjE0Y099vgEAAAAAAAAAAAAAAAAAAAAAAJgxoYeb1N1nJzkuyadzxeDDWyf5aFU9p6oOmdOK282p66i51pR3WE//9ew5U1V1eJL7D5Q9u7svnPTs7j4/yR8OlD1i0oGV3f3x7r5LFv4NvzHJBWO0+U6SP0hy4+5+UXdftnSwGIC538DnT9rgvKFnZ9rP93pmbDTIEQAAAAAAAAAAAAAAAAAAAAAAgC1K6OEEdPe3k9wtyT9m9+DDJLlSkmcm+WZV/X1V/VhVXXUOa24L3X1ukjMGym4w5TWOGDg/vbvPm/IO47h7dj2/q7k0yd9Ocf7rk1w24nzfJD86jcHd/R/d/dAk10jywCTPSfK2LAQSnpHkwsXdvp+FQL8TkjwvyfFJrtPdv93dp6/S+kYDo8/s7m9tcN2vDpxP+/lOhp/xr8xgBwAAAAAAAAAAAAAAAAAAAAAAAPYCO+e9wN6qqh69yu23Jzkwyf2zEHq4FHxYSfZL8tOLV6rqy0k+keTUJGdnIUzt+0kun+be3f26afbfQ3wlyWEjzm+c5J1TnD8UdLenBsINBQp+pLvPntbw7j6rqj6a5E4jyu6W5F+muMN5WQg7fNuEWg49C58eo+fQ83N4VV1lysGae+szDgAAAAAAAAAAAAAAAAAAAAAAwIwJPRzfX2VXqOFqavHnyvDDJUclOXLyaw3aDqGHJyW53Yjzm0x5/lD/k6Y8f1xDz+NHZrDDhzM69PDoGewwSbcdOH/fGD2/kOTiJPuOqLlJFkJVJ66qKgvBoaPsqc84AAAAAAAAAAAAAAAAAAAAAAAAM7Zj3gtsAbXKtdp5sisAcela7bPTvLaLobC3W095/m0Gzj855fnjOmzg/Dsz2GFoxtCOe5rbD5z/x0YbdvfFGQ4VnOYzfuMkB4047ySfnuJ8AAAAAAAAAAAAAAAAAAAAAAAA9iJCDzdvZZBhr1G3WgDhap+d1rWdDIUeHltV+0xjcFXtTHKrgbI9NfTwkIHz785gh6EZe03oYVUdlOSOI0rOSfKhMdsPPeO3HbPvegz1/lJ3f3+K8wEAAAAAAAAAAAAAAAAAAAAAANiLCD3cvJVhhjW6fOTnpnVtNx9LcuGI8wMzvVC4OyS58ojzC5N8fEqzN+uygfP9ZrDD/gPne1OA532S7Dvi/K3dfdGYvT8wcH6PMfuux3ED50O7AQAAAAAAAAAAAAAAAAAAAAAAsI0IPWTL6e4Lk3xwoOzeUxp/r4Hz9y/utyc6b+D8GjPYYWjG+TPYYVJ+buD8DZvo/e6B85tW1XU30X+UoWf8XVOaCwAAAAAAAAAAAAAAAAAAAAAAwF5I6OHm9V5wbUdDwWs/NaW5Pz1w/s4pzZ2Ebw+cX28GO1x/4Py0GeywaVV1eJIfH1Hy9SQnjNu/u7+e5PMDZRN/xqvqNkluOKKkMxzICAAAAAAAAAAAAAAAAAAAAAAAwDYi9HBzai+6tps3DpzfpqpuMsmBVXVMkluMKOkM7zVPXxk4P24GOxw/cD60457iN5LsHHH+ku6+dJMz/nHg/JGb7D9Oz/d19+lTmAsAAAAAAAAAAAAAAAAAAAAAAMBealQoF6PdcN4LsLbu/lJVfSjJnUaU/UqSp0xw7FMHzk/s7q9OcN6kfXLg/IiqOrq7Pz+N4VV1iyTXGyj7zDRmT1JVHZXkCSNKzknyqgmMen2S/zvi/E5Vdbvu/tgEZqWqrpLk59exEwAAAAAAAAAAAAAAAAAAAAAAAPyA0MMxdfcp896BQa/O6NDDx1XVs7v7W5sdVFXXS/KogbK/2uycKTtxHTXPSPKLU5r/zHXUrGfHuamqysJzt9+Isud099mbndXdJ1fViUnuMqLst5L85GZnLXpikkNGnJ+X5I0TmgUAAAAAAAAAAAAAAAAAbBFH/Mbb570CAAAAE/TV5/7YvFcA9kI75r0ATNFfJzl9xPmVkzx3QrOel2T/EeenLe6zx+ru/07yhYGyx1TV0ZOeXVXHJnnEQNkp3f2ZSc+esOcluduI868k+ZMJzvvjgfMHV9Vxmx1SVddI8n8Hyv6iu8/a7CwAAAAAAAAAAAAAAAAAAAAAAAC2FqGHbFndfWGSFw+UPbqqfnIzc6rqYUkeOVD2ou6+aJNzjqiqHrh+dzMzkvztwPnOJP9UVVfb5JwfWAzUe1OG/z96w6RmTkNV/e8k/2tESSf55c0+Byu8JcnJAzWv2czfq6oqyWuSHDyi7JIkLxx3BgAAAAAAAAAAAAAAAAAAAAAAAFuX0EO2uhcl+dpAzWur6g7jNK+qOyX5y4GyUzIcvrineEWSCwdqbprkrVV16GaHVdW1krwtyZEDpRcnedkmZ12vqu6+mR5r9N2vql6W5HkDpS/t7hMmObu7L0/yjIGyG2QhqPKAMcc8P8mPDdS8qLtPHbM/AAAAAAAAAAAAAAAAAAAAAAAAW5jQQ7a07j4/ya8PlB2U5J1V9eMb6V1VP5HkhCQHDpQ+vbsv2Ejveenu07O+gMa7JflUVf3ouLOq6l5JPpVkPYGTr+zur487a9H1kvxHVX2yqh5bVUN/t0FVdc8kH0vy5IHSjyV55mbnraa735bk7QNlx2XhGT98vX0XwxxfleTpA6XfSvIH6+0LAAAAAAAAAAAAAAAAAAAAAADA9iL0kC2vu9+Y5G8Hyg5O8s9V9fqqOnpUYVXdrKrekOQtSa460Pf13f2mdS+7Z/j9JF9eR931k/xnVb27qu5fVfsOfWAxSO9BVfW+JO9Kcu11zPl6kv+zjrr1OjbJa5J8p6r+pap+oapusN4PV9UhVfWYqnp/kncnOWbgI19J8uPdfeHYGw97fJLvDNT8SJLPVtWvVdVBaxVV1c6qekiSTyf5pYGelyd5bHefs6FtAQAAAAAAAAAAAAAAAAAAAAAA2DZ2znsBmJEnJLltkpuMqKkkj0zyyKr6ZJITsxBYd26Sg5LcMMldk9xqnTM/n+SJ4y48L919/mLo3YlJDljHR+65eF1YVR/OQljeGUnOzMLv9NAkhyW5dZLbJ9lvA+tclOQhUwrV2z/Jjy9eqarvJflkkv/Owu5nJTlvse6QJEcmuUWSW2b9gbHfSHK/7j5tkouv1N3frKpHJXlHRu92aJIXJnl2Vb03C9/3tCSXLZ7dIslxSa6xztHP6e53jr04AAAAAAAAAAAAAAAAAAAAAAAAW57QQ7aF7j63qu6b5P1Jrr+Oj9x68RrXqUnu293nbqLH3HT3p6rqp5K8OQuhf+uxf5K7L16TcHGSh3X3RybUb8ghSY5fvCbh5CT36e5TJ9RvpO4+oap+KclfZCFscpQDkjxg8RrXa5L89iY+DwAAAAAAAAAAAAAAAAAAAAAAwDawY94LwKx09ylZCLT70pRHfTHJ8bMKu5uW7v63JPdLcvocxp+Z5Me7+5/nMHsS3pLkLrN+Brr71UmekOTSKY/6yyS/2N095TkAAAAAAAAAAAAAAAAAAAAAAADs5YQesq109xeT3D7JCVMa8W9Jbt/d0w5WnInufl+S22bhe83Ke5LcprvfNcOZk3JWkid0909295nzWKC7/zzJvTOdsMpLkjy1u3+xuy+fQn8AAAAAAAAAAAAAAAAAAAAAAAC2GKGHbDvd/b3uvl+Sx2ZywXCnJ3lMd9+/u8+aUM89Qnd/vbvvn+TBST45xVGfTvLT3X2v7j5lCv3PSfL9KfRNkguSvDDJUd39qinNWLfu/o8kN03yqiSTCid8b5JbdfdLJ9QPAAAAAAAAAAAAAAAAAAAAAACAbUDoIdtWd782yZFJnpzkv8ds87nFz9+wu183qd32RN391u6+TZJ7JPmLJN+ZQNszkrw6yfHdfWx3v2kCPVfV3ScluXqSe2YhoPDTSS7bTMsshEA+Lcl1uvvp3X3mphedkO4+s7ufkOToJC/PeIGPlyR5S5Ljuvv47h733wkAAAAAAAAAAAAAAAAAAAAAAADb1M55LwDz1N3nJXlFkldU1Q8nuV+S2yS5eZLrJjkoyZWTnJ/knCRfz0LQ4SeS/Gt3f2GGu341Sc1q3og93pfkfVX1+CS3SHLnxZ8/nOQ6Sa6Vhd/ZfosfuSgLv7/TknwzyReSfDbJh5J8qrt7hrtfkuTfF69U1YFJbpfktklulOSoJDdIcnCSqyx+j4uSnJvkzCRfTPI/ST6W5D3dfdqsdh/X4jP6lKr6tSQ/muT4JMdkIQzxsCw84/tk4fk+Owvf8aQkH0jyzu4eJywRAAAAAAAAAAAAAAAAAAAAAAAAkgg9hB/o7v/JQqAd67AYVviZxWuv1N3nJvmPxWtLWxn4CAAAAAAAAAAAAAAAAAAAAAAAALMg9HBMVfXqee8whu7uX5j3EgAAAAAAAAAAAAAAAAAAAAAAAABsD0IPx/fYJD3vJTagsrCv0EMAAAAAAAAAAAAAAAAAAAAAAAAAZkLo4ebVvBcAAAAAAAAAAAAAAAAAAAAAAAAAgD2R0MPN63kvsE7CGQEAAAAAAAAAAAAAAAAAAAAAAACYKaGHm7c3hAnuLcGMAAAAAAAAAAAAAAAAAAAAAAAAAGwhQg83b1aBghsJVxRyCAAAAAAAAAAAAAAAAAAAAAAAAMDcCT3ce6wWZLhWEGKtUQ8AAAAAAAAAAAAAAAAAAAAAAAAAMyP0cHynZjbBgvslOTTJvivu97L5SyGHSz9PncFeAAAAAAAAAAAAAAAAAAAAAAAAADCS0MMxdfcRs5xXVVdJcr0kd128HpDkWtk9/HDJp5I8rrvPmuGKAAAAAAAAAAAAAAAAAAAAAAAAALCbHfNegPXp7vO6++TufnV3/0KSH0ryC0k+l6SWyhZ/PijJJ6rqJnNYFQAAAAAAAAAAAAAAAAAAAAAAAACSCD3ca3X3Jd39miTHJnludgUeJgshiEckeX9V3Wb22wEAAAAAAAAAAAAAAAAAAAAAAACA0MO9Xndf1t2/meT+SS5aur14XT3Jv1XVEXNaDwAAAAAAAAAAAAAAAAAAAAAAAIBtTOjhFtHd70ryc1kIO/zB7SwEH76tqvafy2IAAAAAAAAAAAAAAAAAAAAAAAAAbFtCD7eQ7v6nJM9OUiuObprk92e/EQAAAAAAAAAAAAAAAAAAAAAAAADbmdDDred5Sb617H1nIQTxV6vqxvNZCQAAAAAAAAAAAAAAAAAAAAAAAIDtSOjhFtPd5yd5bhaCDpfbJ8kzZr8RAAAAAAAAAAAAAAAAAAAAAAAAANuV0MOt6Y0r3ncWQhB/rqoOmMM+AAAAAAAAAAAAAAAAAAAAAAAAAGxDQg+3oO7+VpJPZyHocLn9k9xn9hsBAAAAAAAAAAAAAAAAAAAAAAAAsB0JPdy6PrLG/eNmugUAAAAAAAAAAAAAAAAAAAAAAAAA25bQw63r9DXu32KmWwAAAAAAAAAAAAAAAAAAAAAAAACwbQk93LpWhh52kkpy1Bx2AQAAAAAAAAAAAAAAAAAAAAAAAGAbEnq4/Vxt3gsAAAAAAAAAAAAAAAAAAAAAAAAAsD0IPdy6rrnG/avMdAsAAAAAAAAAAAAAAAAAAAAAAAAAti2hh1vXEWvcv3iWSwAAAAAAAAAAAAAAAAAAAAAAAACwfQk93IKqqpLcJ0mvcnzOjNcBAAAAAAAAAAAAAAAAAAAAAAAAYJsSerg13TPJNRZf14qfp85+HQAAAAAAAAAAAAAAAAAAAAAAAAC2I6GHW0xV7UjygjWOO8nJM1wHAAAAAAAAAAAAAAAAAAAAAAAAgG1M6OHW84Ikt8xCwGGtcn7ibNcBAAAAAAAAAAAAAAAAAAAAAAAAYLvaOe8FmIyq2jfJHyd5UtYOPEySd8xsKQAAAAAAAAAAAAAAAAAAAAAAAAC2tR3zXoDNqaodVfWTST6bhcDDlWGHSwGIneSD3X3KjFcEAAAAAAAAAAAAAAAAAAAAAAAAYJvaOe8F2Jiq2jfJ0UlumeQuSR6S5OrZFXbYuWLw4ZL/b+oLAgAAAAAAAAAAAAAAAAAAAAAAAMAioYdjqqovz3JckisnuWqSfVc5SxbCDpe/X7pXiz/f091vm+aSAAAAAAAAAAAAAAAAAAAAAAAAALCc0MPxHZFdoYLz1Mterww8XHJGkl+czToAAAAAAAAAAAAAAAAAAAAAAAAAsEDo4eb1cMnUrQxe7GX3z03y4O4+dbYrAQAAAAAAAAAAAAAAAAAAAAAAALDd7Zj3AmxYrXIt6eweePitJPfq7hNnuiEAAAAAAAAAAAAAAAAAAAAAAAAAJNk57wW2gBoumape8X5pnzcleVJ3f2fG+wAAAAAAAAAAAAAAAAAAAAAAAABAEqGHk7AydHAeloIOO8m/Jfmj7n7/HPcBAAAAAAAAAAAAAAAAAAAAAAAAAKGHm1TDJVN3fpIPZSHs8I3d/dX5rgMAAAAAAAAAAAAAAAAAAAAAAAAAC4Qeju+1M5zVSS5NclGSs5OcnuTUJCcn+Z/uvmyGuwAAAAAAAAAAAAAAAAAAAAAAAADAugg9HFN3P27eOwAAAAAAAAAAAAAAAAAAAAAAAADAnmzHvBcAAAAAAAAAAAAAAAAAAAAAAAAAALYmoYcAAAAAAAAAAAAAAAAAAAAAAAAAwFQIPQQAAAAAAAAAAAAAAAAAAAAAAAAApkLoIQAAAAAAAAAAAAAAAAAAAAAAAAAwFUIPAQAAAAAAAAAAAAAAAAAAAAAAAICpEHoIAAAAAAAAAAAAAAAAAAAAAAAAAEyF0EMAAAAAAAAAAAAAAAAAAAAAAAAAYCqEHgIAAAAAAAAAAAAAAAAAAAAAAAAAUyH0EAAAAAAAAAAAAAAAAAAAAAAAAACYCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMBU7Jz3Anu7qrrqGkeXd/e5M97lwKwRZNnd35/lLgAAAAAAAAAAAAAAAAAAAAAAAACwakAe61NVD0/yvTWuF89hpRevscuZVXWvOewDAAAAAAAAAAAAAAAAAAAAAAAAwDYm9HBzHp+kVrkuT/JHc9jneUl6lX12JHniHPYBAAAAAAAAAAAAAAAAAAAAAAAAYBsTejimqrpxkntkIWRw6criz3/s7i/Oeqfu/p8k/7hsj+V7PbCqrj3rnQAAAAAAAAAAAAAA/n927jxatrOs8/jvSW4gxCQMEuYwaoBABEEggNBAABkUARWDMmOjggotSotr0Q4NKt0otqGZRYkyBHBBA2qnG0QUJFEBZQZBwCAkDA0SJCQQnv6jTnHrVk6dnFv3vHXuvfX5rLVX7f3ufd797Ju18ucXAAAAAAAAAID1JXq4vIfOnNfcvd9b5SBbvHt2rj1JTl/xLAAAAAAAAAAAAAAAAAAAAAAAAACsMdHD5X3fzHnPnJ/X3eesephvDdL9jiT/Mr2cu32fFY8DAAAAAAAAAAAAAAAAAAAAAAAAwBoTPVxCVV05ye2zb1SwNq5ftStD7etVmcwz1RvXd6mqK+7OSAAAAAAAAAAAAAAAAAAAAAAAAACsG9HD5dwlyZEb5zV373WrHWVTr5s5n53v6CR3Wu0oAAAAAAAAAAAAAAAAAAAAAAAAAKwr0cPl3GLmvGfOL03yzhXPspl3JfnGxnnP3bvlimcBAAAAAAAAAAAAAAAAAAAAAAAAYE2JHi7n5nPXtfH7ge6+eNXDzOvuryX5QPbONWt+dgAAAAAAAAAAAAAAAAAAAAAAAAAYQvRwOTfbZK2TvHvVg2xh0SyihwAAAAAAAAAAAAAAAAAAAAAAAACshOjhcq6eSeRw3mdXPcgW5mfpJJXkWrswCwAAAAAAAAAAAAAAAAAAAAAAAABrSPRwOcctWP/iSqfY2qJZFs0OAAAAAAAAAAAAAAAAAAAAAAAAADtK9HA5h3L08NiVTgEAAAAAAAAAAAAAAAAAAAAAAADA2hI9XM6eBetHrnSKrS2a5dtWOgUAAAAAAAAAAAAAAAAAAAAAAAAAa0v0cDlfXbB+3Eqn2NqxC9YvWekUAAAAAAAAAAAAAAAAAAAAAAAAAKwt0cPl/PuC9euvdIqt3WDB+qJgIwAAAAAAAAAAAAAAAAAAAAAAAADsKNHD5fxrktpk/earHmQLJy9Y//xKpwAAAAAAAAAAAAAAAAAAAAAAAABgbYkeLueTc9edSQTx9lW1Zxfm2cfGDLfLZK5vLW9cf2I3ZgIAAAAAAAAAAAAAAAAAAAAAAABg/YgeLudDM+c1c36lJPde8SybuVeSYzbOa+7eP614FgAAAAAAAAAAAAAAAAAAAAAAAADWlOjhcs7d4t6jVjXEFh67xb2/XdkUAAAAAAAAAAAAAAAAAAAAAAAAAKw10cPlvCPJNzfOe+a3kjyoqm6+K1MlqaqTkzxwZq55b1vdNAAAAAAAAAAAAAAAAAAAAAAAAACsM9HDJXT3FzKJB9bGUs3cPjLJc1c+1F7Py97/rpW9McYkeV93f2I3hgIAAAAAAAAAAAAAAAAAAAAAAABg/YgeLu81c9fTwGCS3LWqfnW14yQb77xL9g0dTnWSs1Y9EwAAAAAAAAAAAAAAAAAAAAAAAADrS/RweS9NcuHG+TR2OA0fVpKnVdXjVzVMVT0hydNmZsnc+cVJXryqeQAAAAAAAAAAAAAAAAAAAAAAAABA9HBJ3X1hkhdlEjicNRs+PKOqzqiqo0fNUVVHV9UZSX5vZpbZmabz/FF3f3bUHAAAAAAAAAAAAAAAAAAAAAAAAAAwT/TwwDwjyRc2zntmfTZ8+Pgk766qU3f65VV1xyTv3njH7Dvn57kwya/s9PsBAAAAAAAAAAAAAAAAAAAAAAAAYCuihwegu7+Y5KnZGxqcNRshvGmSt1fVX1fV6VW1Z9l3VtWeqnpoVf11krdt7D1916IZntbd5y/7TgAAAAAAAAAAAAAAAAAAAAAAAABYxtLxPSa6+8VV9X1Jfih7I4dTszHCSnKnjeM5VfWOJOckOTfJR5J8qbu/PLt3VR2f5CpJTkpyhySnJrljkqvO7JlN3ju97iRv6O4zDvhDAQAAAAAAAAAAAAAAAAAAAAAAAGA/iR7ujMck+c4k35XF4cPZ9aslud/GsffBqk4yDR8eP7fP7H5TvcVakrw/ySO2+xEAAAAAAAAAAAAAAAAAAAAAAAAAsJOO2O0BDgfdfWGS70vy0eyNHM6q7Bs/7Jm12eOIJFfZOI5Y8Mxme3xrlJn3fSjJPbv7ywEAAAAAAAAAAAAAAAAAAAAAAACAXSB6uEO6+4Ikd05ybvaNE86ajRT2ksf8PpnZb3rvbUnu0t2f3YFPAwAAAAAAAAAAAAAAAAAAAAAAAICliB7uoO7+XJK7JXlhLhs3nFULjnnbeW66/3T9jCSndfcXDuBTAAAAAAAAAAAAAAAAAAAAAAAAAOCAiR7usO6+uLt/Ksl9k3wsl40fzgcQZ11eBPFbr8m+scNK8oEk9+juJ3b31w/oIwAAAAAAAAAAAAAAAAAAAAAAAABgB4geDtLdZye5eZKfTvLR7Bsy7AM8MrPfB5M8Kskp3f3WwZ8FAAAAAAAAAAAAAAAAAAAAAAAAANsmejhQd1/a3S/o7psmuVeSlyS5IHuDhbPHIps9++kkL0xy1+6+RXef2d29xR4AAAAAAAAAAAAAAAAAAAAAAAAAsHJ7dnuAddHdb07y5iSpqlOSfE+SWye5SZITk1wzyTFJrrTxJxcl+WqS85Ocl+RjSd6V5O+7+wOrnB0AAAAAAAAAAAAAAAAAAAAAAAAAliF6uAu6+71J3rvbcwAAAAAAAAAAAAAAAAAAAAAAAADASEfs9gAAAAAAAAAAAAAAAAAAAAAAAAAAwOFJ9BAAAAAAAAAAAAAAAAAAAAAAAAAAGEL0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQvQQAAAAAAAAAAAAAAAAAAAAAAAAABhC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGEL0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQvQQAAAAAAAAAAAAAAAAAAAAAAAAABhC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGGLPbg9wKKuqo5LcfMHtr3X3R1Y8z0lJjl5w+33d/c1VzgMAAAAAAAAAAAAAAAAAAAAAAADAehM9PDCPSvL8Bfd+N8mTVzbJxE8medKCez+c5LWrGwUAAAAAAAAAAAAAAAAAAAAAAACAdXfEbg9wiHtcktrkuCTJs3Zhnt/eePdmM/3ULswDAAAAAAAAAAAAAAAAAAAAAAAAwBoTPVxSVd0myW2T9MyRjd8/7O7PrHqm7v50kpfOzDE712lVdeNVzwQAAAAAAAAAAAAAAAAAAAAAAADA+hI9XN6PzJzX3L3nrXKQOc+dOa+58x9d8SwAAAAAAAAAAAAAAAAAAAAAAAAArDHRw+Xde+a8Z84/3N3vWfUw3xpk8u4PTS/nbt9nxeMAAAAAAAAAAAAAAAAAAAAAAAAAsMZED5dQVddIcuvsGxWsjeuzdmOmOWdlMs9Ub1yfWlXH7s5IAAAAAAAAAAAAAAAAAAAAAAAAAKwb0cPlfG/2RgVr7t4bVjzLZt44cz47355MZgcAAAAAAAAAAAAAAAAAAAAAAACA4UQPl3PyzHnPnF+S5D0rnmUz70ly8cZ5z927xYpnAQAAAAAAAAAAAAAAAAAAAAAAAGBNiR4u5+Zz17Xx+57u/saqh5nX3V/PJHxYm9y+2YrHAQAAAAAAAAAAAAAAAAAAAAAAAGBNiR4u56abrHUmocGDxaJZRA8BAAAAAAAAAAAAAAAAAAAAAAAAWAnRw+VcNZPI4bzPr3qQLczP0kkqyQm7MAsAAAAAAAAAAAAAAAAAAAAAAAAAa0j0cDnHL1j/4kqn2NqiWY5b6RQAAAAAAAAAAAAAAAAAAAAAAAAArC3Rw+UsCgd+aZVDXI4vLVgXPQQAAAAAAAAAAAAAAAAAAAAAAABgJUQPl1ML1q+w0im2dtSC9SutdAoAAAAAAAAAAAAAAAAAAAAAAAAA1pbo4XK+umD9+JVOsbVFs1y80ikAAAAAAAAAAAAAAAAAAAAAAAAAWFuih8v5yoL1G6x0iq3dcMH6v69yCAAAAAAAAAAAAAAAAAAAAAAAAADWl+jhcj6VpDZZv+WqB9nC/CzTeT+36kEAAAAAAAAAAAAAAAAAAAAAAAAAWE+ih8v5xNx1ZxIVvE1VHb36cfZVVVdKcttM5prVST6++okAAAAAAAAAAAAAAAAAAAAAAAAAWEeih8t5/8x5zZxfIcn3r3iWzdw/k1mSfedLkg+veBYAAAAAAAAAAAAAAAAAAAAAAAAA1pTo4XLO3eLe41Y2xWJbzXDOyqYAAAAAAAAAAAAAAAAAAAAAAAAAYK2JHi7nnCRf3zjvJDXze1pVnbpbg1XVnZLcc2aeeW9b7UQAAAAAAAAAAAAAAAAAAAAAAAAArCvRwyV094VJ3pzNo4KV5PlVtWe1UyVVdVSS580tz0YZz+3u81c9FwAAAAAAAAAAAAAAAAAAAAAAAADrSfRwea+au56GBZPklCTPXe04ycY7T8ne0OG8V652HAAAAAAAAAAAAAAAAAAAAAAAAADWmejh8l6R5HMb59PY4TR8WEkeW1XPXNUwG+96bPYNHvbMIxcm+YNVzQMAAAAAAAAAAAAAAAAAAAAAAAAAoodL6u6LkzwnewODU7Phw1+oqtdX1TVGzVFV16iqNyT5hUWPbMzzgu6+cNQcAAAAAAAAAAAAAAAAAAAAAAAAADBP9PDAPCvJeRvnPbM+Gz68f5L3VdXpVTUfSFxaTZye5L1J7jf3zvl5PpvkGTv1bgAAAAAAAAAAAAAAAAAAAAAAAADYDtHDA9DdFyV5YvaGBmfNRgivnuRlST5eVb9UVScs+86qOqGqnprk4xt7npDLBg/nZ3hSd3952XcCAAAAAAAAAAAAAAAAAAAAAAAAwDL27PYAh7rufl1VPTfJ43PZ8OA0Ojg9v36SZyR5elV9MMk5Sc5N8pEkX5o5kuQqM8dJSe6Q5NQkN8skVjl9z+z+mVmbvvvF3X3WgX0lAAAAAAAAAAAAAAAAAAAAAAAAAOw/0cOd8Z8yiRHeI4vDh7NxwkpyiyQnJ3nMfrxnPmx4eet/meRn92N/AAAAAAAAAAAAAAAAAAAAAAAAANgxR+z2AIeD7v56kgckOSeXjRwme0OHyeYBxO0ei/4+M/em629L8gPdfckBfyAAAAAAAAAAAAAAAAAAAAAAAAAALEH0cId091eTnJbkddk3UDhrs3jh/h7z+2Rmv+m9VyW598ZMAAAAAAAAAAAAAAAAAAAAAAAAALArRA93UHdflOSHkvxykq9nEiCcDxYme6OFyx7feuXMUUkuTvLk7j69u7825CMBAAAAAAAAAAAAAAAAAAAAAAAAYJtED3dYT/xWktsl+cvsGyrcLIC436+Y22O6/9lJbt3dzz6AvQEAAAAAAAAAAAAAAAAAAAAAAABgx4geDtLd7+3ueyS5b5K/2FheFEDcn2N2n07y50nu1t337e6PDP4sAAAAAAAAAAAAAAAAAAAAAAAAANi2Pbs9wOGuu89OcnZVfUeS05N8f5LbZW/88DJ/svG76P43k7wjyRuTvLy7/2UHxwUAAAAAAAAAAAAAAAAAAAAAAACAHSN6uCLd/dEkT0/y9Kq6cpLbJLl1kpskOTHJNZMck+RKG39yUZKvJjk/yXlJPpbkXUne3d0XrnR4AAAAAAAAAAAAAAAAAAAAAAAAAFiC6OEu6O5/S/KWjQMAAAAAAAAAAAAAAAAAAAAAAAAADktH7PYAAAAAAAAAAAAAAAAAAAAAAAAAAMDhSfRwTVTVUVV14m7PAQAAAAAAAAAAAAAAAAAAAAAAAMD6ED08zFXVbarqjCSfSfLo3Z4HAAAAAAAAAAAAAAAAAAAAAAAAgPWxZ7cHYOdV1bcneVgmkcNTdnkcAAAAAAAAAAAAAAAAAAAAAAAAANaU6OFhoqqOSHK/TEKH909yVJKaeaR3Yy4AAAAAAAAAAAAAAAAAAAAAAAAA1pfo4SGuqm6WSejw4UmuOV2eeaTnrgEAAAAAAAAAAAAAAAAAAAAAAABgJUQPD0FVdVySh2YSO7z9dHnmkV75UAAAAAAAAAAAAAAAAAAAAAAAAAAwR/TwEFJV98gkdPigJFeaLm/8zocOF60DAAAAAAAAAAAAAAAAAAAAAAAAwEqIHh7kquqGSR6V5JFJrj9dnnlkNmo4uw4AAAAAAAAAAAAAAAAAAAAAAAAAu0r08CBUVUcn+eEkj07yHzKJGe5v6LAXrAMAAAAAAAAAAAAAAAAAAAAAAADASogeHkSq6o6ZhA4fkuS46fLG73zEcLPY4aJnLkjyyiQv34ExAQAAAAAAAAAAAAAAAAAAAAAAAGBbRA93WVVdK8kjMokdnjRdnnlkNmS4Wehw0TNfSfLaJH+c5M3d/c0DnxYAAAAAAAAAAAAAAAAAAAAAAAAAtk/0cBdU1VFJHpBJ6PDeSY7M4tBhsnnscLPQ4TeSnJ1J6PD13X3RjgwMAAAAAAAAAAAAAAAAAAAAAAAAAEsQPVyhqrpVksck+bEkV5sub/xuJ3Q4/9z0mXdkEjp8VXd/YQdGBQAAAAAAAAAAAAAAAAAAAAAAAIADJno4WFVdLcmPJ3l0kltNl2ce2SxiuEjPPPOhJC9L8vLu/vgOjAoAAAAAAAAAAAAAAAAAAAAAAAAAO0r0cICqqiT3SfKYJD+Q5KgsDh0m+xc7fHaSl3X3u3ZgVAAAAAAAAAAAAAAAAAAAAAAAAAAYRvRwB1XVSUkeneThSa49XZ55ZDZ2uFno8PLup7uffCAzAgAAAAAAAAAAAAAAAAAAAAAAAMCqiB4eoKo6NsnpmcQOT50uzzxyuSHDbT4DAAAAAAAAAAAAAAAAAAAAAAAAAIcU0cMlVdXdMgkdPjjJMdPljd+ef3yTLTZ75tIkb07yhSQP3XhGBBEAAAAAAAAAAAAAAAAAAAAAAACAQ5Lo4X6oqusneVSSRya54XR55pHZkOF2Q4dJ8r4kZyZ5WXd/pqoem0n0EAAAAAAAAAAAAAAAAAAAAAAAAAAOWaKHl6Oqrpjkh5I8OsndMwkV7kTo8IIkr0hyZnf/w44MCwAAAAAAAAAAAAAAAAAAAAAAAAAHEdHDBarqDpmEDn80yfHT5Y3fRSHDWZvFEL+W5PVJzkxydndfujPTAgAAAAAAAAAAAAAAAAAAAAAAAMDBR/RwRlVdM8nDM4kd3my6PPPIZiHDLLg/faaTvC2T0OGruvvLOzMtAAAAAAAAAAAAAAAAAAAAAAAAABzc1j56WFVHJnlAJqHD+yQ5MotDh8llY4eL7n8syR8l+aPu/vjOTAsAAAAAAAAAAAAAAAAAAAAAAAAAh461jR5W1Y2T/EySH09y9enyzCOzMcP50OGi+19K8uokZ3b323dmUgAAAAAAAAAAAAAAAAAAAAAAAAA4NK1t9DDJw5M8aW5tmdDhN5KcneTMJK/v7ot3akAAAAAAAAAAAAAAAAAAAAAAAAAAOJStc/Rwqueu52OHi+6/O5PQ4cu7+3MjBgMAAAAAAAAAAAAAAAAAAAAAAACAQ5no4cR86HCq5+5/OsnLkpzZ3e8fPhUAAAAAAAAAAAAAAAAAAAAAAAAAHMJEDzc3Gzv8apLXJjkzyZu6uxf+FQAAAAAAAAAAAAAAAAAAAAAAAADwLaKHl9WZxA47yZuSPLy7L9jdkQAAAAAAAAAAAAAAAAAAAAAAAADg0HPEbg9wEJoGD5PktCQfraqXVtU9d3EmAAAAAAAAAAAAAAAAAAAAAAAAADjkiB5urmaOb0vysCRnV9V5VfWbVXXyrk4HAAAAAAAAAAAAAAAAAAAAAAAAAIcA0cOt9cYxDSBeN8lTkry3qt5ZVT9XVSfs5oAAAAAAAAAAAAAAAAAAAAAAAAAAcLASPZzomWNWbXJ/GkD87iTPTvKpqnpDVT2kqq6wonkBAAAAAAAAAAAAAAAAAAAAAAAA4KC3ztHDlyT5rSSfzt6QYXLZ+GFtcb+SHJXkfklekeSCqnpBVX3v2NEBAAAAAAAAAAAAAAAAAAAAAAAA4OC3ttHD7j6vu385yfWT3DfJa5Jckr2Bw87WAcTZ+9O1Kyf5iSRvraqPVdWvVtVNVvNFAAAAAAAAAAAAAAAAAAAAAAAAAHBwWdvo4VRPnN3dD0lynSQ/l+Td2RsyTLYOIM7fn67fKMnTknykqt5eVY+rqqsM/hwAAAAAAAAAAAAAAAAAAAAAAAAAOGisffRwVnd/sbuf0923TXLrJGck+X/ZPHA4VQvuzwYQT03yvCSfqarXVNUDqurIwZ8DAAAAAAAAAAAAAAAAAAAAAAAAALtK9HCB7n5Pdz8xyXWS/EiSP0vyzWweN5xaFECcrl0xyYOSvDaTAOL/qKrbDf4UAAAAAAAAAAAAAAAAAAAAAAAAANgVooeXo7u/3t1/0t3fn+TEJE9N8pFsHjdcFECcvT9du3qSn0lyTlV9oKp+qaqut4JPAgAAAAAAAAAAAAAAAAAAAAAAAICVED3cD919fnc/s7tvnuTOSV6S5CvZPIA4a1Egcbp+syTPSPKJqnpzknuN/A4AAAAAAAAAAAAAAAAAAAAAAAAAWAXRwyV19zu6+yeSXCvJo5K8dePWZnHDzNzbKoB4RJK7JfmRub0AAAAAAAAAAAAAAAAAAAAAAAAA4JAjeniAuvui7j6zu++e5DuSPD3Jedk8bjhrUQBxdg0AAAAAAAAAAAAAAAAAAAAAAAAADlmihzuouz/e3f8lyY2S3DvJWUkuzt6QYWfrAGIW3N/7YNWrq+qBVXXUDo8PAAAAAAAAAAAAAAAAAAAAAAAAADtK9HCAnnhTdz80ybWTPCHJ32fzuOFs4LDmnpldn3pwkj9Jcn5VPb+q7jrgEwAAAAAAAAAAAAAAAAAAAAAAAADggIkeDtbd/9bdz+vu2yc5Jcmzk3wumwcQL89sFPGqSf5jkrdU1Ser6jeq6pY7/gEAAAAAAAAAAAAAAAAAAAAAAAAAsCTRwxXq7vd395OTXC/Jg5O8IcmluWz8cKsA4uwz0wDiiUn+c5J/rKp/rKpfqKrrjfkKAAAAAAAAAAAAAAAAAAAAAAAAANge0cNd0N3f6O7XdfcPZhJAfEqSD2ZvxDBZHEBc9Mx0/ZQkz0zyiap6S1U9pqquPPJ7AAAAAAAAAAAAAAAAAAAAAAAAAGAzooe7rLs/293P6u5bJjk1yYuSfDmbxw1n1RbPVCb/be+6sd/5VfWaqrrtsA8BAAAAAAAAAAAAAAAAAAAAAAAAgDmihweR7v7b7v7JJNdO8vAkf7Fxaz5suFUAcfaZ6doVkzwoyf1Hzg8AAAAAAAAAAAAAAAAAAAAAAAAAs0QPD0Ld/bXufll33zPJjZP8WpJPZm/EMNk8fphtPgMAAAAAAAAAAAAAAAAAAAAAAAAAw4keHuS6+5Pd/WvdfeMkpyV5eZKvZW/csLN53HA2fggAAAAAAAAAAAAAAAAAAAAAAAAAKyd6eAjp7rd098OSXCvJTyU5N/vGDRcFEAEAAAAAAAAAAAAAAAAAAAAAAABg5UQPD0HdfWF3v7C775jk5CTPSnJBNg8gAgAAAAAAAAAAAAAAAAAAAAAAAMCuED08xHX3h7r7KUlOTPKAJK9L8o3sG0AEAAAAAAAAAAAAAAAAAAAAAAAAgJUTPTxMdPel3f3G7n5wkusmeXKS90b4EAAAAAAAAAAAAAAAAAAAAAAAAIBdInp4GOruz3f3s7v7Vklul+R5Sb60u1MBAAAAAAAAAAAAAAAAAAAAAAAAsG727PYAjNXd70zyzqr6+STX2u15AAAAAAAAAAAAAAAAAAAAAAAAAFgfoodrorsvTvLJ3Z4DAAAAAAAAAAAAAAAAAAAAAAAAgPVxxG4PAAAAAAAAAAAAAAAAAAAAAAAAAAAcnkQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAA3zCNEAAQAASURBVAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCFEDwEAAAAAAAAAAAAAAAAAAAAAAACAIUQPAQAAAAAAAAAAAAAAAAAAAAAAAIAhRA8BAAAAAAAAAAAAAAAAAAAAAAAAgCH27PYAcLCpqismOSnJ9ZIcl+SYJF9NcmGSTyX5cHdfsnsTHpyq6oQkJya5dib/bkcn6SRfy+Tf7jNJzuvuz+/akPupqq6S5AaZfNOVM/mmyuSbvpLkgiSf6u7P7NaM+6uqjk1yw0y+6aqZfNORmXzTvyf5bJJ/zeS7epfGBAAAAAAAAAAAAAAAAAAAAAAA4DAheghJqurUJA9Mct8kt8gkArfIpVX1/iR/luR/dfc54yc8+FTVLZPcP8ndk9wmyQnb/LsLkrwryV8m+dPufv+oGfdXVd04yQ9k8k3fk+S62/y7Lyb5h0y+6c+7++8GjbjfqupaSX4wk2+6QyYRx9rGn15YVe9J8ldJ/k+Sv+rubw4bFAAAAAAAAAAAAAAAAAAAAAAAgMOS6CFrrapOT/KLmUT7tuvIJN+1cfxSVb0zyX/v7rMGjHhQqaorJnlEkp9NcsqS21wzk7jkfZM8s6r+McnvJfnj7r5kRwbdD1V1ZJIfTvLEJHdccpurZhIVvHuSX6uqjyZ5bpIXdfdXdmTQ/VRV90ny80lOS3LEElscl+TOG8dTk3y6ql6U5Dnd/fkdGxQAAAAAAAAAAAAAAAAAAAAAAIDD2jIhLDjkVdXNquqtSV6R/Qsebua2SV5ZVW+pqpse+HQHp6p6SJJ/SvLCLB883Mytkvx+kg9X1QN3cN/LVVWnJXlPkldm+eDhZr4jye8k+eeqemxV1Q7uvaWquk1VvT3Jnye5V3bu//PXSfIrST5eVU+pqqN2aF8AAAAAAAAAAAAAAAAAAAAAAAAOY6KHrJ2qenCSv0ty1x3e+m5J/r6qHrTD++6qqjquql6Z5KwkJw581Q2TvLaq/rCqjhn4nlTVFarqd5O8KcnJA191QpIXJ/nTqrr6wPekJp6a5Nwkdxr4qmOTPDPJ26rqRgPfAwAAAAAAAAAAAAAAAAAAAAAAwGFA9JC1UlVPSPKaTMJtIxyb5E+q6vGD9l+pqrpmkr9K8qMrfO0jk/xFVV1txOZVdWySNyZ54oj9F7hvkr+pqhuM2Lyq9iR5aZLfSLJnxDs2cfsk51TVd6/ofQAAAAAAAAAAAAAAAAAAAAAAAByCRA9ZG1X1yCRnJKnRr0rynKp6xOD3DFVVxyf5v0luvQuvv0OS/11Vx+zkplV1VJLXJrnXTu67Td+Z5C1VdY0Be784ycMH7Ht5rpHkTVV10i68GwAAAAAAAAAAAAAAAAAAAAAAgEOA6CFroapun+RF2V7w8G+S/EyS2yS5WpKjNn6/J8nPJTl3O69M8qKqut1SAx8cXprklG0++5UkZyZ5XJLvTnLdJFdKckyS6yW5bZKfTvLyJBdtc8/bJXnBfsy7Hf8tyT23+ewlSV6d5Gc3ZjkxybFJjk5ynSTfleQxSX4/yZe2ueeNkry6qnbs/71V9cQkj9zm499M8mdJfjHJnZPcMMnxSa6Q5FpJTk7yY0n+Z5Lzt7nn1ZK8YacDlQAAAAAAAAAAAAAAAAAAAAAAABwe9uz2ADBaVR2f5JWZxAu38k9Jfrq737zJvS8meefGcUZV3TvJc5PcZIv9rpDkrKq6dXd/ef8n3z1V9dAkD9zGoxcl+fUkz+vuf1vwzL9uHO9K8vyq+vZMQoJPzeTfaCsPq6pXdfcbtjX4FqrqzkmeuI1HL03yO0l+u7svWPDMZzaO9yb5g6p6UpLHJvmvSY67nP3vmuQJSc7YxixbqqobJ/nNbT7+kiTP6O5/XnD/go3jg0leUVU/n+ShSZ6Z5JqXs/dJmXz7k7c5CwAAAAAAAAAAAAAAwP9n587jra+reoF/FiCgiAOOmCTOaIqIinqdEi3RzCy7dp3LrsPVTM3hXovMa1nc9BpZqQ2aE6alpmZmDlccUiFBRHGeEBJEVJRBQGDdP87RLHj2b5999vc8z7N5v1+v8/KPtfZa6/uczfG/DwAAAAAAAJcTu2zvA2ALPDfJDSd63p3kDtsIPLyU7n5nktsnee9E6w2TPGeemTuKqtotye/N0fq5JId09xEzAg8vpbu/2d3PSfJfknxljo8cUVXL+Fv1B0lqoufrSe7Z3c+cEXh4Kd19Tnf/cZLbJjlhjo88u6quPO/8GZ6T5IoTPeckeVB3/+qMwMNL6e4Lu/uVSQ5MMs9/F79WVT8+73wAAAAAAAAAAAAAAAAAAAAAAAAuH4QestKq6pZJnjjR9uEkP7eR4L4k6e6zkvxskmMnWp9UVbfYyOzt7GeT3Gii5/QkP93dn1x0SXcfl+Snk5w50XrL9b6FVdVtk9xtou3cJPfr7g8suqe7v5i1W7840XrNJA9fdE+SVNW1kzxkou3iJL/U3W9adE93n5G178RHJlp3T/KERfcAAAAAAAAAAAAAAAAAAAAAAACwmoQesup+J8luM+rfyloo3HmLDO/uc5M8OMlZM9p2S/LsReZvJw+bo+cR3X3yZhd19+eT/PIcrfPctNnPP6W7j9/knnT3N5L8QpJLlnDTLA/O7O92khzR3W/f5J509/eS/HyScyZaN/smAAAAAAAAAAAAAAAAAAAAAAAAVozQQ1ZWVd0oyYMm2g7v7lM2s2c9/O93Jtr+a1Xtv5k9W6GqKsk9J9re093vXtbO7v7HJO+faLvXJtdMff6zSf56kzt+qLtPTPKaibY7VdVem1gz9aZvJXn+Jub/B919epIXTrRdv6puvqydAAAAAAAAAAAAAAAAAAAAAAAA7PyEHrLKnphk1xn1zyf5iyXtenGSL82o77p+z45u/yT7TPT81YC9U7+HfavqxxYZXFVXSHLribZXdPfFi8yfYepNuyU5aBPzD56o/113f2cT8y/LX87Rc/sl7wQAAAAAAAAAAAAAAAAAAAAAAGAnJvSQlVRVuyZ5yETbHy0r6K67L0ryoom2h1bVjv7f3I0n6pckefeAve9K0hM9N1lw9g0yO/wySd654OxZjkkyFTq40Juqavck159oW/qbuvvUJCdNtC36ewIAAAAAAAAAAAAAAAAAAAAAAGAF7egBbLCoQ5PsO6N+fpLXLHnnK5NcOKN+vSQ/ueSdy3b1ifpp3X3mspd29xlJTp9ou+aC46felCSfWHD2Nq0HYX5qom3RN10103+/T1xw9pSpuYu+CQAAAAAAAAAAAAAAAAAAAAAAgBUk9JBV9bMT9X/s7rOXubC7z0ryTxNtU3dtb3tM1JceePgjvjFRv+KCc6fe9J3u/v6Cs6dsrzcl435Xo94EAAAAAAAAAAAAAAAAAAAAAADAChJ6yKq690T9HwftnZr7U4P2Lst3JurnDtw9Nfu7C869PL5pnt2LGvUmAAAAAAAAAAAAAAAAAAAAAAAAVpDQQ1ZOVe2b5BYTbe8etP5dE/WfqKrrDtq9DN+cqF9j4O6p2VO3Lfq5fRacO48hb+rus5NcuMndixr1ewIAAAAAAAAAAAAAAAAAAAAAAGAFCT1kFR0yUT+lu08Zsbi7v5LktIm2O4zYvSSfSdIz6tcZuHtq9pcWnHtakm/PqO9ZVVdbcPaUqYDLRd+UJJ/e5O5FjXwTAAAAAAAAAAAAAAAAAAAAAAAAK0boIavo4In68YP3f3SiftvB+xfW3d9KctKMlqtV1a2XvbeqbpPkqjNavtTdU2GSl6m7O8m/TLTdbZHZs1TVPkluNaPle0mO28SKD0zUR7xplyR3mWj74LL3AgAAAAAAAAAAAAAAAAAAAAAAsPMSesgqOmiifuLg/VPzd9jQw3Vvn6jfb8DOn5mov2eT87fHmw7L7L+xH+zuCzcxf3u86U5JrjGj/vnuPmXAXgAAAAAAAAAAAAAAAAAAAAAAAHZSQg9ZRTebqH9+8P4vTNRvOnj/Zr04ycUz6k+uqisua1lVXTnJk+e4aTNeneSsGfVHVdW+m9zxQ1W1S5JnTbRt9k3vyOzv8n2qatkBm4dP1Df7JgAAAAAAAAAAAAAAAAAAAAAAAFaM0ENWSlVVkv0n2qZCCTdrav7+g/dvSnefnOSoGS37JvntJa58bpJrz6j/v+4+YTMLuvucJC+a0XLFJC/YzI7/5NeS3GpG/fNJ3rqZBd3dSf5gRkslObKqdtvMnh8Oq3pAkvvOaPl2kpcvYxcAAAAAAAAAAAAAAAAAAAAAAACrQ+ghq+Y6Sfac6Pna4Bum5u9VVbNC/nYEv5Hk6zPqz6qqR252SVU9PslTZ7Scn+QJm92z7nlJTppRf2hVbTrMsarun+SFM1o6yWO7+5LN7uruv07yrhktd0/y0s3uqarbJ3ntRNtTuvu7m90FAAAAAAAAAAAAAAAAAAAAAADAahF6yKq53hw9pw++YZ7589y53XT3N5M8PMmFM9r+uqp+v6p22+j8qtqjqo5M8pKJ1md092c3Ov+ydPeFSR6a5KwZbc+tqpdV1ZU2Or+qdqmqZyX5+yS7zmh9YXcfvdH5M/xKkq/OqP9qVb2tqq65yPCqenSSo5PsNaPtjd39qkXmAwAAAAAAAAAAAAAAAAAAAAAAsNqEHrJqrjFR/253XzDygO4+L8k5E21Td2533f3uJA9OctE2WnZJ8qwkn6iqx84TFFhVe1fVryU5KcmTJ9p/r7v/dCM3T+nuE5McluTsGW2PTvKZqnp6VV19auZ6gOMjkxyf5PeTzAqBfFWSZ2zg5End/W9J7pXkazPafibJp6vquVW179TMqtq1qh5YVe9P8rLMDjx8d5KHbeRmAAAAAAAAAAAAAAAAAAAAAAAALj9mhXPBzmififp3t+SKtT1XnlGfunOH0N1vqap7JjkqyY9vo+2AJH+e5E+q6vgk/5rkjCTfTlJJrp7kOknumOSgTP/d+X6S3+ru52/6AZehu4+pqjsleV2SW2+jbb8kz09yRFV9IslHkpye5FtJLs7am66V5HZJ7pBkj6m1SY5M8ozu7s2+4VLDu79QVXdM8pok99hG2zWT/HaSw6vqM0k+nOTUrP2eLkhytayFcR6Utd/VrO/vD7w2yWNGB4kuQ1U9MckTtmDVjbdgBwAAAAAAAAAAAAAAAAAAAAAAwE5D6CGr5uoT9bO35IrpPTtF6GGSdPcHq+o2SX43yWOy7YC/3ZPcaf1nUScl+dXuPmYTMyZ196eq6pAkv5nkyUmuso3WXbMWAnjQJtadnOQJ3f32TcyY1N2nVtWhSX49ybOSXHsbrZXkFus/izozawGOr9jEjK12rSS33N5HAAAAAAAAAAAAAAAAAAAAAAAAXN7ssr0PgCXbc6J+7pZckZwzUZ+6c4fS3Wd195OS3CjJEUm+sOQVxyd5SJIDRwce/kB3n9/dz06yf5LDk3xiySs+m+R/JLnp6MDDH+juS7r7yCQ3zFr44UeS9BJXnJK1QMX9d7LAQwAAAAAAAAAAAAAAAAAAAAAAALYToYesmt0n6hdtyRXTe6bu3CF199eS/G6Spyc5dgkjP57kTt19u+5+XXdfsoSZG9Ld307yh0meluQdSxj5lST37e4Duvul3f39JczckO4+L8mfJXlGktcuYeQ3kjw8yQ26+4ju3qrwUAAAAAAAAAAAAAAAAAAAAAAAAHZyQg9ZNUIPB6mqm1TVXyY5M8mbkxyyhLG3SfIvVfWBqnp0VV1hCTPnVlX7VtULsvamdyY5bAlj90/yT1V1XFU9par2WsLMuVXV1arqt5OcluQDSR62hLHXSvKaJCdV1bOrap8lzAQAAAAAAAAAAAAAAAAAAAAAAOByQOghq2bqO33xllwxvWfXLbliCarqqlX18iSfSfLfk1xxySt2TXLXJC9L8sWqenxV1ZJ3/AdVtUdV/WGSLyd5WpKrDFhzcJI/SnJyVf3m6EDHqtqlqp6Z5OQkz01y7QFrbpHkfyf5alW9oKquPGAHAAAAAAAAAAAAAAAAAAAAAAAAK2S37X0ALNlFE/Wt+s5P7fn+llyxSVV1tySvSfLjW7RyvyQvSfKgqnpUd39t2Quq6lZJXpvk1suevQ3XSPK8rL3pYd39mWUvqKr9svZ7uvuyZ2/DXlkLi3xgVT28uz+yRXs34xtJPrUFe26cZI8t2AMAAAAAAAAAAAAAAAAAAAAAALBTEHrIqrlwor5V3/krTNSn7tzuqur+Sd6YZPc52k9J8s9JPpDkhCTfSvLNJJVkn6wF/x2U5G5J7pPk+hPz7p3khKo6tLs/ucD5l6mqDknyziRXnaP9G0nekbU3HZfkzKy96/tZe88+SX4ia2/66SQ3nZh3cJLjqur+3f3ehR5wGarqxkmOzvS/aZKcnX9/0zFJzsjam76Xf/893TRrb7pX1n5ns9w4yQer6hHd/TcLnL9luvvPkvzZ6D1VdVKSW47eAwAAAAAAAAAAAAAAAAAAAAAAsLMQesiq+f5EfZ4Av2XYqUMPq+qwzBd4+Mkkv5/k77r7om30nJfk1CQfT/LKqtotyS8l+c3MDoe7VpL3VNU9u/tTG7n/slTVbTNf4OHJSY5I8oruPn8bPV9b//lkktdXVSX5may96c4zZl8pyduq6n7d/b6N3H9ZquoGSd6b6cDDbyR5QZI/7+7vbKPn6+s/n0rylvX5d0/yv5Lcd8bsXZO8uqq6u1+3gfMBAAAAAAAAAAAAAAAAAAAAAAC4HNhlex8AS3bORP3KW3JFsvdEferO7aaq9k1yVKYDD/8kye26+29mBB5eSndf1N1HJTk4yYsn2q+d5K1Vtde88y/L+uf/NtOBh3+b5MDufumMwMNL6TVvS3LXJM9OcvGM9isleVNVXW/e+ZelqnbJ2u9pv4nW9ya5TXf/4YzAw8vU3e/v7vsleVySWf8eu2Yt0PLAjcwHAAAAAAAAAAAAAAAAAAAAAABg9Qk9ZNV8a6J+lS25YnrP1J3b08uT7DPR8+Tu/vXuvnDRJd19QXc/McnTJlpvnOT/Lrpn3QuS3GSi54Xd/Uvd/d1Fl3T3Jd39u0n+W5Ke0bpPkpctumfdM5LcZaLnDUnu3d2nbWZRd/9FkntldvDh7kleXVVTYZkAAAAAAAAAAAAAAAAAAAAAAABcjgg9ZNV8c6J+ta04IslVJ+pTd24XVXVoksMm2v6ku1+0rJ3d/cIkL5loe1xVHbjI/Kq6SZLHTrS9JWshgkvR3W9I8r8m2g6rqgcsMr+qrprktybajk3yyO6+ZJEd/1l3fyjJL0+0HZjk8cvYBwAAAAAAAAAAAAAAAAAAAAAAwGoQesiqOXOivkdVXW3kAVW1T5LdJ9p2yNDDJE+bqP9bkmcO2Pv0JKdP9PzGgrOfmtl/685L8vhlhQP+iOcnOWGiZ9E3PTbJ3jPqneSx3f29Bedf9tDu1yd560Tbk6vK/7cAAAAAAAAAAAAAAAAAAAAAAACQROghq+erc/RcZ/AN88yf584tVVX7JrnvRNvzuvv8Ze/u7vOS/N5E20M2GlhZVbslecRE259291Tg4oZ1dyc5fKLtHlV1ywXGP3qi/nfd/fEF5s7j8KyFKm7LjZIcNmg3AAAAAAAAAAAAAAAAAAAAAAAAOxmhh6yU7j4nyTcn2m4w+Iz9J+pndPe5g29YxD2S1Iz6RUleO3D/UUkunlHfPcndNjjz9kn2nuh51QZnbsTbk5w50XOvjQysqusmOWCibdibuvsTST420bahNwEAAAAAAAAAAAAAAAAAAAAAALC6hB6yir48Ub/p4P03mahP3be9TAUKHtvd3xm1vLvPSvKvE2133+DYqTf9W3eftMGZc+vuTvLuibZlv+nCJEdvcOZGvXOivtE3AQAAAAAAAAAAAAAAAAAAAAAAsKKEHrKKpkLsbj54/9T8YSF7m3SjifqxW3DDMRP1AzY4b+pNUyGLy7DVb/pUd5+7wZkbtew3AQAAAAAAAAAAAAAAAAAAAAAAsKKEHrKKjp+o33bw/oMn6h8bvH9R15iof2MLbpjaMXXjRvu9aTFTO65cVbtvwR0AAAAAAAAAAAAAAAAAAAAAAADs4IQesoqmQg8PqqpdRyyuqt2S3GaibUcNPbz6RP3MLbhhasdGAwK9aYx5dmz0XQAAAAAAAAAAAAAAAAAAAAAAAKwgoYesoo8mOX9G/cpJbjdo9yFJrjSjfn6S4wbt3qyLJ+p7bMENe07Ue4PzvGmMqTclG38XAAAAAAAAAAAAAAAAAAAAAAAAK0joISunu89P8i8TbT81aP29J+ofWL9vR3TuRP1aW3DD1I7zNjjPm8aYZ8dG3wUAAAAAAAAAAAAAAAAAAAAAAMAKEnrIqnrXRP0XBu39xYn6OwftXYbTJ+rX34Ib9puof32D87xpjKk3fa+7v7sFdwAAAAAAAAAAAAAAAAAAAAAAALCDE3rIqnrDRP3gqrr5MhdW1a2S3HpGS2f6ru3pyxP1e27BDYdO1Kdu3Gj/natqzw3O3Kh7TdSX/aYbVtX+G5y5UVNv+srg/QAAAAAAAAAAAAAAAAAAAAAAAOwkhB6ykrr7i0k+MtH2pCWv/fWJ+oe6+ytL3rlMH5uo719VB4xaXlW3TnL9ibYTNzh26k17JvnJDc6cW1VdPckdJ9qW/aYkOWyDM+dWVbsm+amJto2+CQAAAAAAAAAAAAAAAAAAAAAAgBUl9JBV9vKJ+q9U1b7LWFRV10/yiIm2Vyxj10AfmqPn6QP3/885eua58Ucdm+SiiZ6Rb3pakt0mejb0pvVAz69PtD1lPZxwhF9Jcu2Jno3+ngAAAAAAAAAAAAAAAAAAAAAAAFhRQg9ZZa9OcsaM+pWSHLGkXf8nyZ4z6l9fv2eH1d2fTvL5ibZHVdUBy95dVQclechE28ndfeJG5nb32UmOnmi7V1XdeyNz57EeqPmUibbzkrxngfH/MFG/edbCCZeqqvZK8uw5Wt+27N0AAAAAAAAAAAAAAAAAAAAAAADsnIQesrK6+/wkfzzR9siq+vnN7KmqByd56ETbkd19wSb37F9VPfHznM3sSPLaifpuSd5UVVfb5J4fqqprJXljpv8evW7BFUfN0fPqqrrBgvMvpaqumOTNSfaaaP2H7j53gRXzvOnIqjp4gdmXqap2Wd+730TrMd39pWXtBQAAAAAAAAAAAAAAAAAAAAAAYOcm9JBVd2SSUyZ6XllVhywyvKrulORlE20nZzp8cUfx4iTnT/TcIslbqmqfzS6rquskeVuSG020XpjkTxdc8zdJTpvouW6Sty8j+LCq9k7yt0nm+U69cJEd3X10kuMn2vZK8g9VdeAiO35UVV0hyUuT/Nwc7Qu9CQAAAAAAAAAAAAAAAAAAAAAAgNUk9JCV1t3nJfmNiba9k7yzqu6/kdlV9XNJ/jnJlSdan9bd39vI7O2lu8/IfAGNd09yQlXdbdFdVXXvJCdkvnDAl3b3qYvs6e4Lkjx3jtZbJvlYVf38InuSpKoOTvLRJPN8l97a3ccuuivJb83Rc70kx1TV46qqFllSVTdO8sEkj5mj/WNJ3rDIHgAAAAAAAAAAAAAAAAAAAAAAAFaT0ENWXne/IclrJ9qumuStVXVUVR0wq7GqbllVr0vy5iRXmZh7VHe/ce5jdwzPTfKlOfr2S/L+qnp3Vd23qnaf+kBV7VFVD6iq9yV5V5LrzrHn1CSHz9E3y18k+fAcfVdP8qaqOraqHlxVV5r6QFXtVlX3rKq3Zi3w8GZz7Dk7yZPm6Num7n5HktfP0bpnkpcm+VRVPaaqrjr1garapaoOqarXJPlM5gumvCjJ47r7kjl6AQAAAAAAAAAAAAAAAAAAAAAAuJzYbXsfAFvkcUlul+TmM3oqyUOTPLSqPpbkQ0m+nOScJHsnuWGSuyS5zZw7P5Pk8YsevL1093lV9aCsvf+Kc3zkXus/51fVMUk+nuSbSb6VtX/TfZJcI8ltk9whyR4bOOeCJA/q7rM38JlL6e5LqurBSY5Lcu05PnKHrAUKfr+qjs9amOGZWXvTxVkLR9wnya2T3DnJXhs5J8kju/urG/jMtjw2yUGZ/b3+gQOyFv740qo6McmxSc7I2pvOz7+/6eZJ7prkahu85Wnd/a8b/AwAAAAAAAAAAAAAAAAAAAAAAAArTughlwvdfU5V3SfJB5LsN8dHbrv+s6ivJrlPd5+ziRnbTXefUFW/kOTvk+w558f2THKP9Z9luDDJg7v72GUM6+5Tq+q+Sd6VtXC/eVwhyR3Xf5ZyRpIndveblzKs+7tVdViS9ybZf86P7ZK1oMSDlnHDuiO6+0VLnAcAAAAAAAAAAAAAAAAAAAAAAMCK2GV7HwBbpbtPTnJoki8OXvWFJId291cH7xmqu9+R5LAkZ2yH9d9Kcv/ufusyh3b38VkLZRz9Hbgs30vyiO5+yTKHdvdXktwtyXHLnDuni5M8s7uftR12AwAAAAAAAAAAAAAAAAAAAAAAsBMQesjlSnd/IckdkvzzoBXvSHKH7t4eoXpL193vS3K7rL1rq7wnycHd/a4Rw7v7k0lun+Q1I+Zvw3FJ7tTdR40Y3t2nJrlrkiOzFkS4Fb6Q5N7d/fwt2gcAAAAAAAAAAAAAAAAAAAAAAMBOSOghlzvd/e3uPizJLyc5Y0ljz0jyqO6+b3eftaSZO4TuPrW775vkgUk+NnDVx5P8Ynffu7tPHrgn3X1Wdz8iyT2SHD1w1ZeSPDbJId194sA96e7zu/upWQupfEuSHrTq9CTPTHKr7j560A4AAAAAAAAAAAAAAAAAAAAAAABWhNBDLre6+5VJbpTkiUk+veCYT61//obd/apl3bYj6u63dPfBSX4yyV8l+cYSxn4zycuTHNrdB3X3G5cwc27d/f7uvmfWggKPTHLKEsaeneT1SR6Q5Gbd/ZfdfckS5s6luz/e3Q9Mcoskz0vyuSWMPT/J25I8NMn+3f387r5gCXMBAAAAAAAAAAAAAAAAAAAAAABYcbtt7wNge+ruc5O8OMmLq+pmSQ5LcnCSn0jyY0n2TnKlJOdlLczu1KwFHR6f5J+6+/NbeOtXktRW7Ztxx/uSvK+qHpvk1knuvP6/N0tyvSTXydq/2R7rH7kga/9+X0/ytSSfT/LJJB9JckJ395Y+4DJ09/FZ+50+df17cJesvemArH0PrptkryR7Zu13cEGS7yU5I8lpSb6Q5KQkxyT5aHdftNVv+M+6+7NJDk9yeFXtl+TuSQ7MWhji9bP2pr2z9qZds/am87MWZnlaki9l7U0fTfJhIYcAAAAAAAAAAAAAAAAAAAAAAAAsQughrOvuzyX53Pa+Y2exHlZ44vrPyljF70F3n5LkqPUfAAAAAAAAAAAAAAAAAAAAAAAA2DK7bO8DAAAAAAAAAAAAAAAAAAAAAAAAAIDVJPQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAA+P/s3Hm09/d0L/D3Tp4MIjGEiBgq5gipJCS4vWhQDUVVXGrucGtorkarblXV7TX0urSqpWjNrSiuoRQ11tS6hESQKELMMghCBhGJff84R5dLnt/3d37n9znneU5er7XOylrZ+7f3/pznl8da/ngDAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADDEts0+AHY0VbVHkpskuU6SfZLsleTCJOcl+WqSz3T3xZt34Y6pqvZLct0kB2Tl97Znkk5yUVZ+d2ck+Up3n7NpR65RVV0lyfWy8qYrZ+VNlZU3nZ/krCRf7e4zNutGAAAAAAAAAAAAAAAAAAAAAAAA2JEJPYQkVXXbJPdOcrckN0+y64z2S6vq1CRvTfLG7v7Q+At3PFV1iyS/lOSoJIcn2W/Oz52V5KQk703ylu4+ddSNa1VVN0hyz6y86dZJrj3n576d5OSsvOmfu/sjg05cmqraluQ2Se6Q5GZZCfq8VlYCK/dOcmlWgh0vSHJmks8nOT3JqUk+1N1f2ISzAQAAAAAAAAAAAAAAAAAAAAAA2MkIPeRyrap+NcnjshLaN69dk/zs6s/jq+rEJM/s7lcPOHGHUlV7JHlokkcnOWTBMftnJVzybkn+d1V9PMlfJXlFd1+8lEPXoKp2TXLfJMclud2CY66alaDEo5L8z6r6XJLnJXlhd5+/lEOXoKoqKzf+VlYCK/eZ0b4tyR5JrpbkZ5Ic+ROzzk7yr1kJ/3xrd58x4mYAAAAAAAAAAAAAAAAAAAAAAAB2brts9gGwGarqoKp6X5J/yNoCDy/LrZK8qqreU1U3Xf91O6aqul+S05L8bRYPPLwst0zy4iSfqap7L3HupKq6c5JPJHlVFg88vCw3SvKsJKdX1W+uhg1uqqo6Jsmnkrw7ya9mduDhPK6R5D5JXpTka1X11+ucBwAAAAAAAAAAAAAAAAAAAAAAwBYk9JDLnaq6T5KPJLnDkkf/fJKPVtWvLHnupqqqfarqVUleneS6A1cdmOQNVfWyqtpr4J5U1e5V9ewk70py8MBV+2UlFPAtVXX1gXu2q6puWlUfTPLaJAeNWpPk2oNmAwAAAAAAAAAAAAAAAAAAAAAAsBMTesjlSlUdm5Xwt70Hrdg7yeuq6rcHzd9QVbV/kvcnuf8Grn1Ykn+pqn1HDK+qvZO8OclxI+Zvx92SfLCqrreBO1NVj0hyUpLbbeReAAAAAAAAAAAAAAAAAAAAAAAA+BGhh1xuVNXDkjwnSY1eleS5VfXQwXuGqqorJXlnkkM3Yf1tkrytqvZa5tCq2i3JG5L8wjLnzunGSd5TVdcYvahW/GWSFyRZ6u8QAAAAAAAAAAAAAAAAAAAAAAAA1mLbZh8AG6GqjkzywswXePjBJK9c/ecXk5yXZJ8kN0jyn5I8KCuhfDNXJnlhVf17d39kwbM328uTHDJn7/lJXp/kX5N8JMnZSb6Vld/Dvkn2T3Jkktsn+ZUkV5hj5hFJ/ibJQ9Z09WzPSHKXOXsvTvLGJO9P8qEkZyb5dpJLsvKmqye5dZKfS3JMkqvMMfP6Sf5PVR3V3T9c0+Vzqqpdkhyf5FfX8LGzkpyQ5PNJzkhyweq/v2pW3nWtJIdlJbhx12XdCgAAAAAAAAAAAAAAAAAAAAAAwNYn9JAtr6qulORVSXabaD0tyaO6+92XUft2khNXf55TVXdN8rwkN5wxb/ckr66qQ7v7u2u/fPNU1QOS3HuO1u8leXKS53f3d7bT87XVn5OSvKCqrpbk0Un+MCu/o1keXFWv6e5/muvwGarq55IcN0frpUmeleTPu/us7fScsfrzySQvrarHJPnNJE/JSkDmLHdIcmyS58xxyyKem/kCD89I8tIkr+zuU+cZXFVXyErQ4z2T3CvJTRc9EgAAAAAAAAAAAAAAAAAAAAAAgMuHXTb7ANgAT05y/YmedyU5YjuBhz+lu9+RlfC390y0Xj/Jn8wzc0dRVduSPHWO1s8mObK7nz4j8PCndPc3u/tPkvynJF+c4yNPr6pl/F31v5LURM9ZSY7q7v8+I/Dwp3T3+d39l0kOS3LyHB95UlXtPe/8eVXV45I8aqLtwiR/kOSG3f1H8wYeJkl3f6+7P7D6+zkoycFZCW88b+GjAQAAAAAAAAAAAAAAAAAAAAAA2NKEHrKlVdXBSY6daPu/SX55LcF9SdLd5ya5Z5ITJlofXVU3W8vsTXbPJDeY6DkzyV27+5RFl3T3iUnumuScidaDV/sWVlWHJbn9RNsFSe7e3R9YdE93fz4rt35+ovXqSR686J7LUlW3TfKnE22fSHJodz+ju7+33p3d/e/d/TtJrp3kWeudBwAAAAAAAAAAAAAAAAAAAAAAwNYj9JCt7n8k2Taj/q0k9+/uCxcZ3t0XJLlfknNntG1L8qRF5m+SB83R85Du/tJ6F3X3aUl+bY7WeW5a7+cf090nrXNPuvsbSe6T5IdLuGkuVbVPkldm9nf9fUluv/o7X6ruPq+737/suQAAAAAAAAAAAAAAAAAAAAAAAOz8hB6yZVXVDZIcM9H2xO7+ynr2rIb//Y+Jtv9SVQeuZ89GqKpKctRE27u7+13L2tndb0kyFZh353Wumfr8Z5K8dJ07/kN3fyLJKybabltVV1zSyicluf6M+ieT3Ku7v7ukfQAAAAAAAAAAAAAAAAAAAAAAADAXoYdsZccm2XVG/bQkf7ukXc9LcvqM+q6r9+zoDkyy70TPiwbsnfpzOKCqrr3I4KraLckhE20v6+5LF5k/w9SbtiU5dL1LqupGSX5nRsuFSY4ReAgAAAAAAAAAAAAAAAAAAAAAAMBmEHrIllRVuyZ5wETbXywr6K67L0nyVxNtD6yqHf2/uRtO1H+Y5F0D9r4zSU/03GjB2dfL7PDLJHnHgrNn+XCS70z0LPqmH/fUJLvPqD+xu09bwh4AAAAAAAAAAAAAAAAAAAAAAABYsx09gA0WdackB8yoX5TkFUve+fIkF8+oXyvJzy9557JddaJ+Rnefs+yl3X12kjMn2q6+4PipNyXJJxecvV2rQZifmmhb9E1Jkqo6MMl9Z7R8Iclfr2cHAAAAAAAAAAAAAAAAAAAAAAAArIfQQ7aqe07U39Ld5y1zYXefm+SfJ9qm7tpse0zUlx54+GO+MVG/woJzp970ne7+wYKzp4x60488JsmuM+rP7O5ZQZwAAAAAAAAAAAAAAAAAAAAAAAAwlNBDtqq7TNTfMmjv1NxfGLR3Wb4zUb9g4O6p2d9dcO5WfFOqarckD57Rcn6SVyw6HwAAAAAAAAAAAAAAAAAAAAAAAJZB6CFbTlUdkORmE23vGrT+nRP1m1fVNQftXoZvTtSvNnD31Oyp2xb93L4Lzp3HqDclKwGas+a/ubvPW8d8AAAAAAAAAAAAAAAAAAAAAAAAWDehh2xFR07Uv9LdXxmxuLu/mOSMibYjRuxekk8n6Rn1/Qfunpp9+oJzz0jy7Rn1PavqKgvOnjIVcLnom5Lk/hP1f1rHbAAAAAAAAAAAAAAAAAAAAAAAAFgKoYdsRYdP1E8avP+jE/XDBu9fWHd/K8mpM1quUlWHLHtvVd0yyZVntJze3VNhkpepuzvJv0203X6R2bNU1b5JbjGj5XtJTlzHirtM1P9lHbMBAAAAAAAAAAAAAAAAAAAAAABgKYQeshUdOlH/xOD9U/N32NDDVW+dqN99wM5fmqi/e53zN+NNR2f237H/2t0XLzK4qm6S5FozWr7c3WcuMhsAAAAAAAAAAAAAAAAAAAAAAACWSeghW9FNJuqnDd7/uYn6jQfvX6/nJbl0Rv24qrrCspZV1d5JjpvjpvX4+yTnzqg/rKoOWOeO/1BVuyT5w4m29bzpDhP1k+YZUlV7VdWtq+peVfXgqnpoVd23qu5cVTeqqt3WcSMAAAAAAAAAAAAAAAAAAAAAAABk22YfAMtUVZXkwIm2qVDC9Zqaf+Dg/evS3V+qquOTPHQ7LQck+eMkT1jSyicnucaM+r9098nrWdDd51fVXyV50nZarpDkz5I8aD17fsx/S3KLGfXTkrxpHfMPnahv9ztYVQdl5Z33THJIZoffXlxVJyV5b5LXd/dH1nYmAAAAAAAAAAAAAAAAAAAAAAAAl3ezwq5gZ7R/kj0ner4++Iap+VesqlkhfzuC30ty1oz6H1bV9kIR51ZVj0zyuzNaLkry2+vds+ppSU6dUX9gVf3xepdU1T2SPGtGSyd5eHf/cB1rZgUqJsnnL+OuW1bVW5J8KskTk9wy0/8bsHuS2yZ5fJITqurUqvr1qhKYCwAAAAAAAAAAAAAAAAAAAAAAwFyEHrLVXGuOnjMH3zDP/Hnu3DTd/c0kD05y8Yy2l1bVny4SgFdVe1TVs5M8f6L1cd39mbXOvyzdfXGSByY5d0bbk6vqxVW111rnV9UuVfWHSd6QZNcZrc/q7veudf5POHii/h+BlVW1e1X9RZITk9w9Sa1z70uSfKyq7rCOOQAAAAAAAAAAAAAAAAAAAAAAAFxOCD1kq7naRP273f39kQd094VJzp9om7pz03X3u5LcL8kl22nZJckfJvlkVT18nqDAqtqnqv5bklOTHDfR/tTufu5abp7S3Z9IcnSS82a0/UaST1fV71fVVadmrgY4PjTJSUn+NMmsEMi/S/K4NZx8mfuS7DfR9o3V3gOSvD/JYzI7iHGtbpHkPVX1pKpaT4giAAAAAAAAAAAAAAAAAAAAAAAAW9yscC7YGe07Uf/uhlyxsmfvGfWpO3cI3f3GqjoqyfFJfmY7bQcl+Zskz6mqk5J8JMnZSb6dpJJcNcn+SW6T5NBM/73zgyR/1N3PXPcDLkN3f7iqbpvkVUkO2U7bdZM8M8nTq+qTST6U5Mwk30pyaVbetF+SWyU5IskeU2uTPDvJ47q71/mEA+bo+W5VXTPJe5LcdJ37tmeXJP8zyQ2r6je6+9JBe5aiqo5N8tsbsOqGG7ADAAAAAAAAAAAAAAAAAAAAAABgpyH0kK3mqhP18zbkiuk9O0XoYZJ0979W1S2TPCXJb2X7AX+7J7nt6s+iTk3ym9394XXMmNTdn6qqI5M8IclxSa60ndZdsxLUeOg61n0pyW9391vXMePHzRN6uGuSt2Vc4OGPe+jqPx+2AbvWY78kB2/2EQAAAAAAAAAAAAAAAAAAAAAAAJc3u2z2AbBke07UL9iQK5LzJ+pTd+5Quvvc7n50khskeXqSzy15xUlJHpDkZ0cHHv5Id1/U3U9KcmCSJyb55JJXfCbJo5LceImBh0ly5Tl6/jzJLWfUv5HkZUmOSXJIkmtkJczyOkluleQRSd6a5KI5b3poVT1+zl4AAAAAAAAAAAAAAAAAAAAAAAAuR7Zt9gGwZLtP1C/ZkCum90zduUPq7q9X1VOSfCjJE5Icuc6RH0/yiI0KOrws3f3tqnpGkhOS/F6So9c58otJHtXdb1vvbdtxhTl6jtrOv78gyf9K8mfd/f3LqH9t9eekJH9bVddO8owkD5xj51Oq6h3dfdIcvQAAAAAAAAAAAAAAAAAAAAAAAFxO7LLZB8CSCT0cpKpuVFUvTHJOkn/M+gMPk+SWSf6tqj5QVb9RVbstYebcquqAqvqzrLzpHVl/4GGSHJjkn6vqxKp6TFVdcQkzf9yeC37uc0kO6e6nbSfw8Kd099e6+0FJ7pXkoon2bUleVFW14H0AAAAAAAAAAAAAAAAAAAAAAABsQUIP2WqmvtOXbsgV03t23ZArlqCqrlxVL0ny6ST/NckVlrxi1yT/OcmLk3y+qh45Ojivqvaoqmck+UKSxya50oA1hyf5iyRfqqonLDHQcZE5n0py++7+wiILu/ufktw9yQUTrYcluf8iOwAAAAAAAAAAAAAAAAAAAAAAANiatm32AbBkl0zUN+o7P7XnBxtyxTpV1e2TvCLJz2zQyusmeX6SY6rqYd399WUvqKpbJHllkkOWPXs7rpbkaVl504O6+9PrnLfW4M6LkhzT3WeuZ2l3v6eqfj8rfz6zPC7Jq9aza5BvZCX8cbQbJtljA/YAAAAAAAAAAAAAAAAAAAAAAADsFIQestVcPFHfqO/8bhP1qTs3XVXdI8nrkuw+R/tXkrw9yQeSnJzkW0m+maSS7JuV4L9Dk9w+yS8muc7EvLskObmq7tTdpyxw/mWqqiOTvCPJledo/0aSt2XlTScmOScr7/pBVt6zb5KbZ+VNd01y44l5hyc5saru0d3vWegBK9b63XnCEoIWkyTd/YKquk+SX5jRdnhVHdHdH1nGzmXp7r9O8tej91TVqUkOHr0HAAAAAAAAAAAAAAAAAAAAAABgZyH0kK3mBxP1eQL8lmGnDj2sqqMzX+DhKUn+NMn/6e5LttNzYZKvJvl4kpdX1bYk90/yhMwOh9svybur6qju/tRa7r8sVXVY5gs8/FKSpyd5WXdftJ2er6/+nJLk1VVVSX4pK2+63YzZeyV5c1Xdvbvft5b7f8xavjtnJHnOgnu2548yO/QwSY5JskOFHgIAAAAAAAAAAAAAAAAAAAAAALA5dtnsA2DJzp+o770hVyT7TNSn7tw0VXVAkuMzHXj4nCS36u5/mBF4+FO6+5LuPj7J4UmeN9F+jSRvqqorzjv/sqx+/jWZDjx8TZKf7e4XzAg8/Cm94s1J/nOSJyW5dEb7XkleX1XXmnf+T7hgDb0vWsufzTy6+yNJPjrRdvdl7gQAAAAAAAAAAAAAAAAAAAAAAGDnJfSQreZbE/UrbcgV03um7txML0my70TPcd39O9198aJLuvv73X1sksdOtN4wyZ8vumfVnyW50UTPs7r7/t393UWXdPcPu/spSX41Sc9o3TfJixdcs5bvzssW3DHlpRP1m1fVRgWMAgAAAAAAAAAAAAAAAAAAAAAAsAMTeshW882J+lU24ogkV56oT925KarqTkmOnmh7Tnf/1bJ2dvezkjx/ou0RVfWzi8yvqhslefhE2xuTPG6R+Zelu1+b5PETbUdX1b0WGD/vd+eM7j59gfnz+OBEfZckhw7aDQAAAAAAAAAAAAAAAAAAAAAAwE5E6CFbzTkT9T2q6iojD6iqfZPsPtG2Q4YeJnnsRP1rSf77gL2/n+TMiZ7fW3D272b233UXJnlkd/9wwfnb88wkJ0/0LPKmqe/4j3x4gdnzOiUrv7dZbjhwPwAAAAAAAAAAAAAAAAAAAAAAADsJoYdsNV+eo2f/wTfMM3+eOzdUVR2Q5G4TbU/r7ouWvbu7L0zy1Im2B6w1sLKqtiV5yETbc7t7KnBxzbq7kzxxou2OVXXwGudekORbc7R+ai1z13jDJUk+O9F23VH7AQAAAAAAAAAAAAAAAAAAAAAA2HkIPWRL6e7zk3xzou16g884cKJ+9mpw3Y7mjklqRv2SJK8cuP/4JJfOqO+e5PZrnHnrJPtM9PzdGmeuxVuTnDPRc+cF5n5hjp5zF5i7Ft+eqO87eD8AAAAAAAAAAAAAAAAAAAAAAAA7AaGHbEVTgXA3Hrz/RhP1eQLrNsNUoOAJ3f2dUcu7+9wkH5lou8Max0696WvdfeoaZ86tuzvJuyba1vqmJDl9jp5zF5i7FlPz9xq8HwAAAAAAAAAAAAAAAAAAAAAAgJ2A0EO2oqkQu5sO3j81f1jI3jrdYKJ+wgbc8OGJ+kFrnDf1pqmQxWVY9puS5JQ5er63wNy1mJq/bfB+AAAAAAAAAAAAAAAAAAAAAAAAdgJCD9mKTpqoHzZ4/+ET9Y8N3r+oq03Uv7EBN0ztmLpxrf0745uS5MQ5eq68wNy1mJo/OnQRAAAAAAAAAAAAAAAAAAAAAACAnYDQQ7aiqdDDQ6tq1xGLq2pbkltOtO2ooYdXnaifswE3TO1Ya0DgVnxTknx0jp6rLDB3LaZ+t+cP3g8AAAAAAAAAAAAAAAAAAAAAAMBOQOghW9FHk1w0o753klsN2n1kkr1m1C9KcuKg3et16UR9jw24Yc+Jeq9x3lZ8U7r7rCSfm2i7xlrnrtHU/K8N3g8AAAAAAAAAAAAAAAAAAAAAAMBOQOghW053X5Tk3ybafmHQ+rtM1D+wet+O6IKJ+n4bcMPUjgvXOG8rvulH3j5RP2LBuZOq6ipJbjjR9qVR+wEAAAAAAAAAAAAAAAAAAAAAANh5CD1kq3rnRP0+g/bed6L+jkF7l+HMifp1NuCG607Uz1rjvK34ph9520T9sKrabcHZU26TpCZ6Pj5oNwAAAAAAAAAAAAAAAAAAAAAAADsRoYdsVa+dqB9eVTdd5sKqukWSQ2a0dKbv2kxfmKgftQE33GmiPnXjWvtvV1V7rnHmWt15or7WN/3IO5N8e0Z9zyR3XHD2lLtO1M/s7i8P2g0AAAAAAAAAAAAAAAAAAAAAAMBOROghW1J3fz7JhybaHr3ktb8zUf9gd39xyTuX6WMT9QOr6qBRy6vqkCTXmWj7xBrHTr1pzyQ/v8aZc6uqqya5zUTbWt+UJOnu7yd59UTbIxeZPctqSOSvTbS9fdl7AQAAAAAAAAAAAAAAAAAAAAAA2DkJPWQre8lE/der6oBlLKqq6yR5yETby5axa6APztHz+wP3/8EcPfPc+ONOSHLJRM/INz02ybaJnrW+6ce9aKL+y1X1M+uYf1kelGTfiZ43LHknAAAAAAAAAAAAAAAAAAAAAAAAOymhh2xlf5/k7Bn1vZI8fUm7/neSPWfUz1q9Z4fV3f+e5LSJtodV1UHL3l1VhyZ5wETbl7r7E2uZ293nJXnvRNudq+oua5k7j9VAzcdMtF2Y5N2L7ujuEyc+vy3J3yw6/ydV1TUy/d/M15O8ZVk7AQAAAAAAAAAAAAAAAAAAAAAA2LkJPWTL6u6LkvzlRNtDq+pX1rOnqu6X5IETbc/u7u+vc8+BVdUTP3+ynh1JXjlR35bk9VV1lXXu+Q9VtV+S12X676NXLbji+Dl6/r6qrrfg/J9SVVdI8o9JrjjR+k/dfcE61z11on50VT1ynTtSVZWVAMWrT7Q+t7svWe8+AAAAAAAAAAAAAAAAAAAAAAAAtgahh2x1z07ylYmel1fVkYsMr6rbJnnxRNuXMh2+uKN4XpKLJnpuluSNVbXvepdV1f5J3pzkBhOtFyd57oJr/iHJGRM910zy1mUEH1bVPklek2Se79Sz1ruvu9+b5A0Tbc+tqgcvuqOqdsnK9/zeE61fz87zXQcAAAAAAAAAAAAAAAAAAAAAAGADCD1kS+vuC5P83kTbPkneUVX3WMvsqvrlJG9PsvdE62O7+3trmb1ZuvvszBdad4ckJ1fV7RfdVVV3SXJy5gsHfEF3f3WRPd39/SRPnqP14CQfq6pfWWRPklTV4Uk+mmSe79KbuvuERXf9hOOSnD+jvmtWwj2fUFW7rWXwajDla5P8+hztv7v63xwAAAAAAAAAAAAAAAAAAAAAAAAkEXrI5UB3vzbJKyfarpzkTVV1fFUdNKuxqg6uqlcl+cckV5qYe3x3v27uY3cMT05y+hx9103y/qp6V1Xdrap2n/pAVe1RVfeqqvcleWeSa86x56tJnjhH3yx/m+T/ztF31SSvr6oTqup+VbXX1AeqaltVHVVVb8pK4OFN5thzXpJHz9E3l+7+SpJHTbTtkuRpSU6pqvtX1RVnNVfVtarqD5J8Nsk8QZB/392vmetgAAAAAAAAAAAAAAAAAAAAAAAALje2bfYBsEEekeRWSW46o6eSPDDJA6vqY0k+mOQLSc5Psk+S6yf5uSS3nHPnp5M8ctGDN0t3X1hVx2Tl/VeY4yN3Xv25qKo+nOTjSb6Z5FtZ+Z3um+RqSQ5LckSSPdZwzveTHNPd563hMz+lu39YVfdLcmKSa8zxkSOSvDrJD6rqpKyEGZ6TlTddmpVwxH2THJLkdklmBgj+5DlJHtrdX17DZ6aHdr+iqm6b5NiJ1pskeVVW/rz+JclnkpyV5LtJ9kuyf5JbZ+V3UHOu/3BW/hsDAAAAAAAAAAAAAAAAAAAAAACA/4/QQy4Xuvv8qvrFJB9Ict05PnLY6s+ivpzkF7v7/HXM2DTdfXJV3SfJG5LsOefH9kxyx9WfZbg4yf26+4RlDOvur1bV3ZK8MyuBhfPYLcltVn+WckaSY7v7H5c07ycdl5W3PWCO3j2T3H31Zz0+nuQe3f29dc4BAAAAAAAAAAAAAAAAAAAAAABgC9plsw+AjdLdX0pypySfH7zqc0nu1N1fHrxnqO5+W5Kjk5y9Ceu/lZUgvTctc2h3n5SVUMbR34HL8r0kD+nu549a0N2XJnlIkheP2vET3pvkjt19zgbtAwAAAAAAAAAAAAAAAAAAAAAAYCcj9JDLle7+XJIjkrx90Iq3JTmiuzcjVG/puvt9SW6VlXdtlHcnOby73zlieHefkuTWSV4xYv52nJjktt19/OhF3X1pd//XJI9MctGgNZcmeWqSu3T3dwbtAAAAAAAAAAAAAAAAAAAAAAAAYAsQesjlTnd/u7uPTvJrSc5e0tizkzysu+/W3ecuaeYOobu/2t13S3LvJB8buOrjSe7b3Xfp7i8N3JPuPre7H5LkjkneO3DV6UkenuTI7v7EwD0/pbv/JsnNkrx+yaPfmeTQ7v7j7r50ybMBAAAAAAAAAAAAAAAAAAAAAADYYoQecrnV3S9PcoMkxyb59wXHfGr189fv7r9b1m07ou5+Y3cfnuTnk7woyTeWMPabSV6S5E7dfWh3v24JM+fW3e/v7qOS3CrJs5N8ZQljz0vy6iT3SnKT7n5hd/9wCXPXrLu/2N3HJDk0K39m5y846twkL01yq+6+a3efspwLAQAAAAAAAAAAAAAAAAAAAAAA2Oq2bfYBsJm6+4Ikz0vyvKq6SZKjkxye5OZJrp1knyR7JbkwK2F2X81K0OFJSf65u0/bwFu/mKQ2at+MO96X5H1V9fAkhyS53eo/b5LkWkn2z8rvbI/Vj3w/K7+/s5J8PclpSU5J8qEkJ3d3b+gDLkN3n5SVP9PfXf0e/FxW3nRQVr4H10xyxSR7ZuXP4PtJvpfk7CRnJPlcklOTfDjJR7v7ko1+wyzd/fEkv1VVx2blbUdl5Tt+0yT7ZeV7vkeSi7ISjPjVJKcn+ViSDyb5t+7+wSacDgAAAAAAAAAAAAAAAAAAAAAAwE5O6CGs6u7PJvnsZt+xs1gNK/zE6s+WsZW/B919cZL3rP4AAAAAAAAAAAAAAAAAAAAAAADAcLts9gEAAAAAAAAAAAAAAAAAAAAAAAAAwNYk9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQ2zb7AMAAGB7Dnz8Wzb7BAAAAJboi0//pc0+AS53/P8rAAAAW4v/fwUAAAAAAAAAAICd0S6bfQAAAAAAAAAAAAAAAAAAAAAAAAAAsDUJPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAA/h879xqm/T3dC/y7kkcSOSAhJUrFWREikTR2S51KtLSq3ZQ69IgKolVttbYq2q1apKUOdSgqGqe2tA4lWqpFQiJClMYppSIRBElEJFn7xczupfXM/b9n5v7NzDPP53Nd9+XFWvda6zfPmLz7AgAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADDEjs0+ALaaqto7yc2SXC/JAUn2TXJJkm8k+XyST3T3ZZt34dZUVQcnuX6SQ7L0c9snSSe5NEs/u3OTfK67L9i0I1epqq6R5AZZetPVs/SmytKbLkpyXpLPd/e5m3UjAAAAAAAAAAAAAAAAAAAAAAAAbGVCDyFJVR2T5L5J7pXkVkn2nNF+RVWdleQtSd7Y3e8ff+HWU1W3TvJjSe6S5IgkB8/5vfOSnJ7kXUne3N1njbpxtarqRknuk6U33T7J9875va8mOSNLb3prd39g0IkAAAAAAAAAAAAAAAAAAAAAAACwS9ljsw+AzVRVP1NVpyV5X5LfTHKbzA48zHL9Nkl+K8n7quqDVfWAsZduDVW1d1X9clWdmeQjSZ6R5J6ZM/Bw2bWzFC75h0k+WlVnVNUvVNVei794WlXtWVUPqKr3JvlUkhOS/ETmDDxcdmCWghJ/L8mpVXV2Vf1qVe2/8IMHqqofrKorq6rn+PzcZt8LAAAAAAAAAAAAAAAAAAAAAADA1if0kN1SVd2iqt6d5K+SHLHOcUcmOamq/qmqbr7+67amqrp/krOT/HmSwxY4+rZJXprkE1V13wXOnVRVd0tyZpKTktxhgaNvkuTZST5dVb9YVbXA2UMsh06+OMmWvxUAAAAAAAAAAAAAAAAAAAAAAIBdh9BDdjtVdb8kH0hypwWPvnOSD1bVTy547qaqqgOq6qQkr0ly/YGrDk3yN1X18qrad+CeVNVeVXVCkpOT3HLgqoOTvCTJm6vqWgP3LMKTknz/Zh8BAAAAAAAAAAAAAAAAAAAAAADA9iL0kN1KVR2X5PVJ9h+0Yv8kb6iqRw2av6Gq6tpJ/jnJAzZw7cOS/GNVHTRieFXtn+Tvkxw/Yv4K7pXkvVV1gw3cObequlWS39rsOwAAAAAAAAAAAAAAAAAAAAAAANh+hB6y26iqhyV5bpIavSrJ86rqoYP3DFVVV0vyjiSHb8L6H0jytqrad5FDq+oqSf4myY8scu6cbprkn6rqezZh94qqao8kL0lylc2+BQAAAAAAAAAAAAAAAAAAAAAAgO1H6CG7hao6OsmLM1/g4XuTPDrJEUkOylIY3EFJbp/ksUlOmWdlkhdX1VFrOnhreEWSw+bsvSjJK5M8PMntknxvkqsm2TfJ9ZIcmeRXkrw6yTfnnHlUkhet4t55PDPJ3efsvSzJ65I8ZvmW6yfZP8k+Sa6b5DZJfiHJS5NcOOfMGyZ53XLQ4Fbx6CTHbPYRAAAAAAAAAAAAAAAAAAAAAAAAbE87NvsAGK2qrpbkpCyFF85ydpJf6e537qT21SSnLX+eW1X3SPL8JDeeMW+vJK+pqsO7++urv3zzVNUDk9x3jtZvJnlqkhd099dW6PnP5c/pSV5YVdfMUpDgE7P0M5rlwVX12u7+u7kOn6GqfjDJ8XO0XpHk2Ume1d3nrdBz7vLnI0n+oqoel+QXkzwtyQET8++U5Lgkz53jlqGq6vuS/P5m3wEAAAAAAAAAAAAAAAAAAAAAAMD2tcdmHwAb4KlJbjjRc3KSo1YIPPwu3f32JLdP8k8TrTdM8pR5Zm4VVbUjydPnaP33JEd39zNmBB5+l+7+cnc/Jcn/SvLZOb7yjKpaxN+q/5ukJnrOS3KX7v6NGYGH36W7L+ruP0lyuyRnzPGVJ1fV/vPOH+gFSXZ2x7eTfG6DbwEAAAAAAAAAAAAAAAAAAAAAAGAbEnrItlZVt0xy3ETb+5L8xGqC+5Kkuy9Mcp8kp060Pqaqvn81szfZfZLcaKLni0nu0d0fXeuS7j4tyT2SXDDResvlvjWrqtslueNE28VJfrS737PWPd39qSzd+qmJ1mslefBa9yxCVT0oyY+uUH5Wkk9v4DkAAAAAAAAAAAAAAAAAAAAAAABsU0IP2e5+N8mOGfWvJHlAd1+yluHdfXGS+ye5cEbbjiRPXsv8TfKzc/Q8pLvPWe+i7j47yc/N0TrPTev9/uO6+/R17kl3fynJ/ZJcuYCbhqiqayY5YYXyp5M8deOuAQAAAAAAAAAAAAAAAAAAAAAAYDsTesi2VVU3SvJTE21P6u7PrWfPcvjf7060/e+qOnQ9ezZCVVWSu0y0vbO7T17Uzu5+c5J/nmi72zrXTH3/E0n+Yp07/kt3n5nkVRNtx1TVfovauUrPSXLwCrVHdfc3N/IYAAAAAAAAAAAAAAAAAAAAAAAAti+hh2xnxyXZc0b97CR/vqBdz0/y6Rn1PZfv2eoOTXLQRM9LBuyd+nc4pKq+dy2Dq+oqSQ6baHt5d1+xlvkzTL1pR5LDF7xzUlX9SJKHrFA+qbv/YSPvAQAAAAAAAAAAAAAAAAAAAAAAYHsTesi2VFV7JnngRNtzFhV0192XJ/nTibYHVdVW///cjSfqVyY5ecDedyTpiZ6brHH2DTI7/DJJ3r7G2bOckuRrEz1rfdOaVNW+SV60QvnCJI/bsGMAAAAAAAAAAAAAAAAAAAAAAADYLWz1ADZYq7smOWRG/dIkr1rwzlckuWxG/bpJ7rzgnYt24ET93O6+YNFLu/v8JF+caLvWGsdPvSlJPrLG2StaDsL82ETbWt+0Vk9LcsMVak/s7vM28hgAAAAAAAAAAAAAAAAAAAAAAAC2P6GHbFf3mai/ubu/sciF3X1hkrdOtE3dtdn2nqgvPPDwO3xpon7VNc6detPXuvvba5w9ZdSbVq2qbp/k+BXK70vyoo26BQAAAAAAAAAAAAAAAAAAAAAAgN2H0EO2q7tP1N88aO/U3B8ZtHdRvjZRv3jg7qnZX1/j3O34plWpqh1JXpJkz52UL0/yiO7ujbgFAAAAAAAAAAAAAAAAAAAAAACA3YvQQ7adqjokyfdPtJ08aP07Juq3qqrrDNq9CF+eqF9z4O6p2VO3rfV7B61x7jxGvWm1npDktivUntXdH9mgOwAAAAAAAAAAAAAAAAAAAAAAANjNCD1kOzp6ov657v7ciMXd/dkk5060HTVi94J8PEnPqF974O6p2Z9e49xzk3x1Rn2fqrrGGmdPmQq4XOub5lZVN03y5BXKn0nye6NvAAAAAAAAAAAAAAAAAAAAAAAAYPcl9JDt6IiJ+umD939won67wfvXrLu/kuSsGS3XqKrDFr23qm6b5OozWj7d3VNhkjvV3Z3kXyfa7riW2bNU1UFJbj2j5ZtJTlv03v9xQyX58yT7rNByXHd/c+QNAAAAAAAAAAAAAAAAAAAAAAAA7N6EHrIdHT5RP3Pw/qn5Wzb0cNlbJuo/OmDnj03U37nO+ZvxpmMz+2/sv3T3ZQP2fqdfSnLnFWqv7e63Dt4PAAAAAAAAAAAAAAAAAAAAAADAbk7oIdvRzSbqZw/e/8mJ+k0H71+v5ye5Ykb9+Kq66qKWVdX+SY6f46b1+MskF86oP6yqDlnnjv9SVXskeeJE23rfNHXDIUmeuUL5a5n+mQMAAAAAAAAAAAAAAAAAAAAAAMC6CT1kW6mqSnLoRNtUKOF6Tc0/dPD+denuc5KcOKPlkCT/Z4Ern5rke2bU/7G7z1jPgu6+KMmfzmi5apI/Xs+O/+HRSW49o352kjctcN/OPC/JNVaoPbG7vzh4PwAAAAAAAAAAAAAAAAAAAAAAAAg9ZNu5dpJ9Jnq+MPiGqfn7VdWskL+t4NeSnDej/sSqeuh6l1TVI5P86oyWS5M8ar17lv1+krNm1B9UVesOc6yqeyd59oyWTvLw7r5yvbtm3HDfJPdbofz+JC8atRsAAAAAAAAAAAAAAAAAAAAAAAC+k9BDtpvrztHzxcE3zDN/njs3TXd/OcmDk1w2o+0vquoPqmrHaudX1d5VdUKSF0y0PqG7P7Ha+TvT3ZcleVCSC2e0PbWqXlpV+652flXtUVVPTPI3Sfac0frs7n7Xauev4o6rJfmzFcqXJ3nEyMBFAAAAAAAAAAAAAAAAAAAAAAAA+E5CD9lurjlR/3p3f2vkAd19SZKLJtqm7tx03X1ykvtnKShvZ/ZI8sQkH6mqh88TFFhVB1TVo5OcleT4ifand/fzVnPzlO4+M8mxSb4xo+0Xkny8qn69qg6cmrkc4PjQJKcn+YMks0IgX5nkCas4eS2emZVDNZ+z/DMAAAAAAAAAAAAAAAAAAAAAAACADTErnAt2RQdN1L++IVcs7dl/Rn3qzi2hu99YVXdJcmKS71uh7RZJXpTkuVV1epIPJDk/yVeTVJIDk1w7yQ8kOTzTf3e+neR3uvuP1v2AnejuU6rqmCQnJTlshbbrJ/mjJM+oqo8keX+SLyb5SpIrsvSmg5McmeSoJHtPrU1yQpIndHev9w0rqao7Jnn4CuXPJnnKqN1bXVUdl+RRG7DqxhuwAwAAAAAAAAAAAAAAAAAAAAAAYJch9JDt5sCJ+jc25IrpPbtE6GGSdPe/VNVtkzwtyS9n5YC/vZIcs/xZq7OS/GJ3n7KOGZO6+2NVdXSS305yfJKrrdC6Z5aCGg9fx7pzkjyqu9+yjhmTqmrvJC/OUtDkzhzX3ZeMvGGLOzjJLTf7CAAAAAAAAAAAAAAAAAAAAAAAgN3NHpt9ACzYPhP1izfkiuSiifrUnVtKd1/Y3Y9JcqMkz0jyyQWvOD3JA5PcZnTg4f/X3Zd295OTHJrkSUk+suAVn0jyK0luOjrwcNn/SXLzFWqv26AbAAAAAAAAAAAAAAAAAAAAAAAA4L8Resh2s9dE/fINuWJ6z9SdW1J3fyHJ05L8epJTFzDyw0mO6e4ju/uk7r5yATNXpbu/muSZSR6f5G0LGPnZJPfq7lt09wu7+9sLmDlTVR2W5DdWKH89yfGjbwAAAAAAAAAAAAAAAAAAAAAAAICdEXrIdiP0cJCquklVvTjJBUn+NsnRCxh72yT/WlXvqapfqKqrLGDm3KrqkKr64yy96e1Jjl3A2EOTvLWqTquqx1XVfguYuaKq2iPJS5Ks9LP77e4+d+QNAAAAAAAAAAAAAAAAAAAAAAAAsBKhh2w3U7/TV2zIFdN79tyQKxagqq5eVS9L8vEkv5TkqgtesWeSH0ry0iSfqqpHVlUteMd/U1V7V9Uzk3wmyeOTXG3AmiOSPCfJOVX12wMDHR+blQMoT0nygkF7AQAAAAAAAAAAAAAAAAAAAAAAYNKOzT4AFuzyifpG/c5P7fn2hlyxTlV1xySvSvJ9G7Ty+lkK6fupqnpYd39h0Quq6tZJXp3ksEXPXsE1k/x+lt70s9398UUNrqobJHn6CuXLkzyiu69c1L5d3JeSfGwD9tw4yd4bsAcAAAAAAAAAAAAAAAAAAAAAAGCXIPSQ7eayifpG/c5fZaI+deemq6p7J3lDkr3maP9ckn9I8p4kZyT5SpIvJ6kkB2Up+O/wJHdMcs8k15uYd/ckZ1TVXbv7o2s4f6eq6ugkb09y9Tnav5TkbVl602lJLsjSu76dpfcclORWWXrTPZLcdGLeEUlOq6p7d/c/rekB3+2FSfZboXZCd394QXt2ed39Z0n+bPSeqjoryS1H7wEAAAAAAAAAAAAAAAAAAAAAANhVCD1ku/n2RH2eAL9F2KVDD6vq2MwXePjRJH+Q5HXdffkKPZck+XySDyd5RVXtSPKAJL+d2eFwByd5Z1Xdpbs/tpr7d6aqbpf5Ag/PSfKMJC/v7ktX6PnC8uejSV5TVZXkx7L0pjvMmL1vkr+vqh/t7nev5v7/qaoenOTYFcrnJHnKeuYDAAAAAAAAAAAAAAAAAAAAAADAIuyx2QfAgl00Ud9/Q65IDpioT925aarqkCQnZjrw8LlJjuzuv5oRePhduvvy7j4xyRFJnj/R/j1J3lRV+807f2eWv//aTAcevjbJbbr7hTMCD79LL/n7JD+U5MlJrpjRvm+Sv66q6847/3+qqmslec6Mlkd398VrnQ8AAAAAAAAAAAAAAAAAAAAAAACLIvSQ7eYrE/WrbcgV03um7txML0ty0ETP8d392O6+bK1Luvtb3X1cksdPtN44ybPWumfZHye5yUTPs7v7Ad399bUu6e4ru/tpSX4mSc9oPSjJS9e6J0uBh9daofaG5QBGAAAAAAAAAAAAAAAAAAAAAAAA2HRCD9luvjxRv8ZGHJHk6hP1qTs3RVXdNcmxE23P7e4/XdTO7n52khdMtD2iqm6zlvlVdZMkD59oe2OSJ6xl/s509+uT/NZE27FV9eOrnV1V90zy4BXKX0/y2NXOBAAAAAAAAAAAAAAAAAAAAAAAgFGEHrLdXDBR37uqrjHygKo6KMleE21bMvQwyeMn6v+Z5DcG7P31JF+c6Pm1Nc7+1cz+W3dJkkd295VrnL+SP0pyxkTPqt5UVfsleeGMlt/p7i+sZiYAAAAAAAAAAAAAAAAAAAAAAACMJPSQ7eY/5ui59uAb5pk/z50bqqoOSXKvibbf7+5LF727uy9J8vSJtgeuNrCyqnYkechE2/O6eypwcdW6u5M8aaLth6vqlqsY+/Qkh65Q+0CS569iFgAAAAAAAAAAAAAAAAAAAAAAAAwn9JBtpbsvSvLlibYbDD7j0In6+d198eAb1uKHk9SM+uVJXj1w/4lJrphR3yvJHVc58/ZJDpjoeeUqZ67GW5JcMNFzt3kGVdUNkjx2hfIVSR7e3Veu4jYAAAAAAAAAAAAAAAAAAAAAAAAYbsdmHwADfCbJNWfUb5rk7QP332Si/pmBu9djKlDw1O7+2qjl3X1hVX0gyTEz2u6U5O9WMXbqTf/Z3WetYt6qdHdX1clJfmZG252SPHeOcQdm5aDaDye5fVXdfpUnznLIRP1OVTX135CTloNIAQAAAAAAAAAAAAAAAAAAAAAA2E0JPWQ7OivJrPC3mw/ePzV/WMjeOt1oon7qBtxwSmaHHt5ilfOm3vSBVc5bi1MyO/RwtW/amSOSvHgBc1bj55c/s5ycROghAAAAAAAAAAAAAAAAAAAAAADAbmyPzT4ABjh9on67wfuPmKh/aPD+tbrmRP1LG3DD1I6pG1fbvyu+CQAAAAAAAAAAAAAAAAAAAAAAAHYZQg/ZjqZCDw+vqj1HLK6qHUluO9G2VUMPD5yoX7ABN0ztWG1A4HZ8EwAAAAAAAAAAAAAAAAAAAAAAAOwyhB6yHX0wyaUz6vsnOXLQ7qOT7DujfmmS0wbtXq8rJup7b8AN+0zUe5XztuObAAAAAAAAAAAAAAAAAAAAAAAAYJch9JBtp7svTfKvE20/Mmj93Sfq71m+byu6eKJ+8AbcMLXjklXO245vAgAAAAAAAAAAAAAAAAAAAAAAgF2G0EO2q3dM1O83aO9PT9TfPmjvInxxon69Dbjh+hP181Y5bzu+CQAAAAAAAAAAAAAAAAAAAAAAAHYZQg/Zrl4/UT+iqm6+yIVVdeskh81o6UzftZk+M1G/ywbccNeJ+tSNq+2/Q1Xts8qZq3W3ifpcb+ruM7q7NuqT5N0TJ/38HHM+O8/bAAAAAAAAAAAAAAAAAAAAAAAA2L6EHrItdfenkrx/ou0xC1772In6e7d4CNyHJuqHVtUtRi2vqsOSXG+i7cxVjp160z5J7rzKmXOrqgOT/MBE22rfBAAAAAAAAAAAAAAAAAAAAAAAALsMoYdsZy+bqP98VR2yiEVVdb0kD5loe/kidg303jl6fn3g/t+co2eeG7/TqUkun+gZ+abHJ9kx0bPaNwEAAAAAAAAAAAAAAAAAAAAAAMAuQ+gh29lfJjl/Rn3fJM9Y0K4/TLLPjPp5y/dsWd39b0nOnmh7WFXdYtG7q+rwJA+caDunu89czdzu/kaSd0203a2q7r6aufNYDtR83ETbJUneuejdAAAAAAAAAAAAAAAAAAAAAAAAsFUIPWTb6u5Lk/zJRNtDq+on17Onqu6f5EETbSd097fWuefQquqJz1PWsyPJqyfqO5L8dVVdY517/ktVHZzkDZn+e3TSGlecOEfPX1bVDdY4/7tU1VWT/G2S/SZa/667L17UXgAAAAAAAAAAAAAAAAAAAAAAANhqhB6y3Z2Q5HMTPa+oqqPXMryqjkny0om2czIdvrhVPD/JpRM935/kjVV10HqXVdW1k/x9khtNtF6W5HlrXPNXSc6d6LlOkrcsIviwqg5I8tok8/xOPXu9+wAAAAAAAAAAAAAAAAAAAAAAAGArE3rIttbdlyT5tYm2A5K8varuvZrZVfUTSf4hyf4TrY/v7m+uZvZm6e7zM19A452SnFFVd1zrrqq6e5IzMl844Au7+/Nr2dPd30ry1Dlab5nkQ1X1k2vZkyRVdUSSDyaZ53fpTd196lp3AQAAAAAAAAAAAAAAAAAAAAAAwK5A6CHbXne/PsmrJ9qunuRNVXViVd1iVmNV3bKqTkryt0muNjH3xO5+w9zHbg1PTfLpOfqun+Sfq+rkqrpXVe019YWq2ruqfryq3p3kHUmuM8eezyd50hx9s/x5kvfN0Xdgkr+uqlOr6v5Vte/UF6pqR1XdparelKXAw5vNsecbSR4zRx8AAAAAAAAAAAAAAAAAAAAAAADs0nZs9gGwQR6R5MgkN5/RU0kelORBVfWhJO9N8pkkFyU5IMkNk/xgktvOufPjSR651oM3S3dfUlU/laX3X3WOr9xt+XNpVZ2S5MNJvpzkK1n6mR6U5JpJbpfkqCR7r+KcbyX5qe7+xiq+8126+8qqun+S05J8zxxfOSrJa5J8u6pOz1KY4QVZetMVWQpHPCjJYUnukGS/1ZyT5KHd/R+r+A4AAAAAAAAAAAAAAAAAAAAAAADskoQeslvo7ouq6p5J3pPk+nN85XbLn7X6jyT37O6L1jFj03T3GVV1vyR/k2SfOb+2T5IfXv4swmVJ7t/dpy5iWHd/vqruleQdWQosnMdVkvzA8mchZyQ5rrv/dkHzAAAAAAAAAAAAAAAAAAAAAAAAYEvbY7MPgI3S3eckuWuSTw1e9ckkd+3u/xi8Z6jufluSY5Ocvwnrv5Lk3t39pkUO7e7TsxTKOPp3YGe+meQh3f2CTdgNAAAAAAAAAAAAAAAAAAAAAAAAm0LoIbuV7v5kkqOS/MOgFW9LclR3b0ao3sJ197uTHJmld22UdyY5orvfMWJ4d380ye2TvGrE/BWcluSY7j5xA3cCAAAAAAAAAAAAAAAAAAAAAADAphN6yG6nu7/a3ccm+bkk5y9o7PlJHtbd9+ruCxc0c0vo7s93972S3DfJhwau+nCSn+7uu3f3OQP3pLsv7O6HJPnhJO8auOrTSR6e5OjuPnPgHgAAAAAAAAAAAAAAAAAAAAAAANiShB6y2+ruVyS5UZLjkvzbGsd8bPn7N+zuVy7qtq2ou9/Y3UckuXOSlyT50gLGfjnJy5LctbsP7+43LGDm3Lr7n7v7LkmOTHJCks8tYOw3krwmyY8nuVl3v7i7r1zAXAAAAAAAAAAAAAAAAAAAAAAAANjl7NjsA2AzdffFSZ6f5PlVdbMkxyY5IsmtknxvkgOS7JvkkiyF2X0+S0GHpyd5a3efvYG3fjZJbdS+GXe8O8m7q+rhSQ5Lcofl/71ZkusmuXaWfmZ7L3/lW1n6+Z2X5AtJzk7y0STvT3JGd/eGPmAnuvv0LP2b/ury78EPZulNt8jS78F1kuyXZJ8s/Rt8K8k3k5yf5Nwkn0xyVpJTknywuy/f6DcM8PIk75pRP2NDrgAAAAAAAAAAAAAAAAAAAAAAAGCXJvQQlnX3vyf5982+Y1exHFZ45vJn2/B7sKS7X77ZNwAAAAAAAAAAAAAAAAAAAAAAALDr22OzDwAAAAAAAAAAAAAAAAAAAAAAAAAAtiehhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAAAAAMAQQg8BAAAAAAAAAAAAAAAAAAAAAAAAgCGEHgIAAAAAAAAAAAAAAAAAAAAAAAAAQwg9BAAAAAAAAAAAAAAAAAAAAAAAAACGEHoIAAAAAAAAAAAAAAAAAAAAAAAAAAwh9BAAAAAAAAAAAAAAAAAAAAAAAAAAGELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwhNBDAAAAAAAAAAAAAAAAAAAAAAAAAGAIoYcAAAAAAAAAAAAAAAAAAAAAAAAAwBBCDwEAAAAAAAAAAAAAAAAAAAAAAACAIYQeAgAAAAAAAAAAAAAAAAAAAAAAAABDCD0EAAAAAAAAAAAAAAAAAAAAAAAAAIYQeggAAAAAAAAAAAAAAAAAAAAAAAAADCH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYQughAAAAAAAAAAAAAAAAAAAAAAAAADCE0EMAAAAAAAAAAAAAAAAAAAAAAAAAYAihhwAAAAAAAAAAAAAAAAAAAAAAAADAEEIPAQAAAAAAAAAAAAAAAAAAAAAAAIAhhB4CAAAAAAAAAAAAAAAAAAAAAAAAAEMIPQQAAAAAAAAAAAAAAAAAAAAAAAAAhhB6CAAAAAAAAAAAAAAAAAAAAAAAAAAMIfQQAAAAAAAAAAAAAAAAAAAAAAAAABhC6CEAAAAAAAAAAAAAAAAAAAAAAAAAMITQQwAAAAAAAAAAAAAAAAAAAAAAAABgCKGHAAAAAAAAAAAAAAAAAAAAAMD/Y+feg22vyzqOfx4ucgkQjxCSmoiKiRJ4I1MhUdFAyVtp5oVm/KdiCP/J0RlHzZpJa7KSBktt1HIKKjPHDpZAJhpiCCJqpUcZGyhuKqB4maPw7Y+9c5rxsH7rrL2evffZvF4ze84M32d9n+8653D+fAMAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAtRA8BAAAAAAAAAAAAAAAAAAAAAAAAgBaihwAAAAAAAAAAAAAAAAAAAAAAAABAC9FDAAAAAAAAAAAAAAAAAAAAAAAAAKCF6CEAAAAAAAAAAAAAAAAAAAAAAAAA0EL0EAAAAAAAAAAAAAAAAAAAAAAAAABoIXoIAAAAAAAAAAAAAAAAAAAAAAAAALQQPQQAAAAAAAAAAAAAAAAAAAAAAAAAWogeAgAAAAAAAAAAAAAAAAAAAAAAAAAt9tnoB8BmU1X7JTkmyQOSHJzkwCTfTvLNJNcn+cIYY+fGvXBzqqrDkzwwyZFZ+X3bP8lI8t2s/N7dkOS6McZXN+yRu6mqDk3yoKx8p3tn5TtVVr7THUluSnL9GOOGjXojAAAAAAAAAAAAAAAAAAAAAAAAbGaih5Ckqp6Q5LlJTkvyyCR7zxi/s6o+n+TCJB8YY1ze/8LNp6oeleRZSU5J8pgkh8/5uZuSXJXkX5JsH2N8vuuNu6uqjk5yRla+0+OS3H/Oz92a5OqsfKcPjTGuaHri3Krq4Kz8XX7U6q/HJjkiyaGrPwcl2ZnkO0luSfI/Sb6Y5DNJLktyzRhjrPe7AQAAAAAAAAAAAAAAAAAAAAAA2FpED7lHq6pfTPIbWYn2zWvvJD+5+vPqqroyye+NMS5oeOKmUlX7JXl5krOTHLfgNUdkJS55WpI3V9Vnkrw1yXvHGDuX8tDdUFV7J/n5JOck+ekFr7lPVkKJpyT5zar6UpLzkrxjjHHHUh46oaoOSPKkJE9N8rQkj83seGeSHLD6sy3Jw7Py/v9zS1W9L8m7xxifXP6LAQAAAAAAAAAAAAAAAAAAAAAAuCfYa6MfABuhqn6iqj6a5K+ye8HDXXlskvOr6iNV9fC1v25zqqoXJtmR5O1ZPHi4K8cn+bMkX6iq5y7x3klV9bQk1yQ5P4sHD3floUnekuTaqnpFVdUS7/6Bqjqkql5WVR9McmuSi5K8JsmJmQ4eTjk8ya8kubyqPlZVT1/jfQAAAAAAAAAAAAAAAAAAAAAAANwDiR5yj1NVz09yRZKTl3z1U5J8qqqet+R7N1RVHVxV5ye5IMkDG1cdleT9VfXuqjqwcU+q6l5V9YdJLk5ybOOqw5O8M8n2qjpsWZdW1elV9YEkNyf58yTPTrLfsu7fhScnuaiqLqiqwxv3AAAAAAAAAAAAAAAAAAAAAAAAsMWIHnKPUlVnJfnbJAc1rTgoyfuq6tea7l9XVXVEkkuTvGgd156Z5J+ralvH5VV1UJJ/SHJOx/1347Qkl1XVg5Z0368n+bn0hg535YVJPl1VT1znvQAAAAAAAAAAAAAAAAAAAAAAAOyhRA+5x6iqM5Ocm6S6VyX546p6efOeVlV1SJKLkpywAet/Ksk/VtWBy7y0qvZN8v4kpy7z3jk9LMlHqupHN2D3Mt0/ySVVdfpGPwQAAAAAAAAAAAAAAAAAAAAAAIDNb5+NfgCsh6o6Mck7Ml/w8LIkf7n661eSfDPJwUmOTvLEJC/JSpRv5sok76iq/xhjXLHgszfae5IcN+fsHUn+LsnHk1yR5OYkX8/K78O2JEckOTHJSUmel+SAOe58fJI/TfKy3Xr1bL+b5Olzzu5M8oEklya5PMmNSW5N8v2sfKfDkjwuyZOSvCDJoXPc+eAkf1NVp4wx7tqtl6/Nt5NcnWRHVv5O37763w5Jct8kRyX5mST3m/O+/ZO8v6pOHWNcuuS3AgAAAAAAAAAAAAAAAAAAAAAAsIWIHrLlVdUhSc5Psu/E6I4kvzrGuGQXZ7cmuXL159yqekaS85I8ZMZ990pyQVWdMMb4xu6/fONU1YuTPHeO0e8keWOSt40xbr+bmf9e/bkqyZ9U1X2TnJ3kNVn5PZrlpVX112OMD8718Bmq6klJzplj9M4kb0ny+2OMm+5m5obVn88meVdVvTLJK5L8VlYCmbOcnOSsJOfO8ZZFjawEKC9M8k9Jrhlj3Dn1oao6Lskrk7w0038298pK+PD4Mcb1a3suAAAAAAAAAAAAAAAAAAAAAAAAW9VeG/0AWAdvTPLgiZmLkzz+boKHP2SM8eEkj0vykYnRByd5wzx3bhZVtU+S355j9ItJThxjvGlG8PCHjDG+NsZ4Q5InJvnKHB95U1Ut49+q30lSEzM3JTlljPGqGcHDHzLGuGOM8UdJHp3k6jk+8rqqOmje+3fDl5O8OsmPjzFOXv2z+fQ8wcMkGWN8dozxiiTHJPnEHB/ZluTtiz8XAAAAAAAAAAAAAAAAAAAAAACArU70kC2tqo5NctbE2CeSPGd3wn1JMsa4LckZSf5tYvTsqnrE7ty9wc5IcvTEzI1JnjHG+NyiS8YYVyZ5RpKvToweuzq3sKp6dJKTJsa+leT0McbHFt0zxvhyVt765YnRw5K8dNE9u/DRJM9OcswY481jjOvXctkY47+SnJzkbXOMn1ZVz1zLPgAAAAAAAAAAAAAAAAAAAAAAALYu0UO2utcn2WfG+deTvGiM8e1FLh9jfCvJC5PcNmNsnySvW+T+DfKSOWZethrGW5Mxxo4kvzzH6DxvWuvnXznGuGqNezLGuCXJ85PctYQ3Tbk4yUljjKeMMbaPMaZ2zm2M8f2sBEMvmGP8tcvaCwAAAAAAAAAAAAAAAAAAAAAAwNYiesiWVVVHJ3nBxNhrxxjXrWXPavzv9RNjv1BVR61lz3qoqkpyysTYJWOMi5e1c4yxPcmlE2NPW+Oaqc9/Icm71rjjB8YY1yR578TYE6rqRxZccXWSZ44xTh1jfHzBOyaNMUaSlyfZMTH65Kp6WNc7AAAAAAAAAAAAAAAAAAAAAAAA2HOJHrKVnZVk7xnnO5K8fUm7zkty7YzzvVffs9kdlWTbxMw7G/ZO/TkcWVX3X+Tiqto3yXETY+8eY9y5yP0zTH2nfZKcsMjFY4xXjzE+vMhnF9i1M8mr5hh9TvdbAAAAAAAAAAAAAAAAAAAAAAAA2POIHrIlVdXeSV48MfYHywrdjTG+n+StE2O/VFWb/f+5h0yc35Xk4oa9FyUZEzMPXfDuB2V2/DJJOgKCn0xy+8TMot9pXY0x/j7JlybGntL/EgAAAAAAAAAAAAAAAAAAAAAAAPY0mz3ABot6apIjZ5x/N8l7l7zzPUl2zjj/sWz+MNx9Js5vGGN8ddlLxxg3J7lxYuywBa+f+k5J8tkF775bqyHMf58YW/Q7bYTtE+fHrcsrAAAAAAAAAAAAAAAAAAAAAAAA2KOIHrJVnTFxvn2M8c1lLhxj3JbkQxNjU+/aaPtNnC89ePj/3DJxfsCC9059p9vHGN9b8O4pXd9pI1w6cf6Aqtp3XV4CAAAAAAAAAAAAAAAAAAAAAADAHkP0kK3q6RPn25v2Tt17atPeZbl94vxbjbun7v7Ggvduxe+0EW6cON8rySHr8RAAAAAAAAAAAAAAAAAAAAAAAAD2HKKHbDlVdWSSR0yMXdy0/qKJ80dW1f2adi/D1ybO79u4e+ruqbct+rltC947j67vtBFumWPmgPZXAAAAAAAAAAAAAAAAAAAAAAAAsEcRPWQrOnHi/LoxxnUdi8cYX0lyw8TY4zt2L8l/Jhkzzo9o3D1197UL3ntDkltnnO9fVYcuePeUqcDlot9pIxw4x8x3218BAAAAAAAAAAAAAAAAAAAAAADAHkX0kK3oMRPnVzXv/9TE+aOb9y9sjPH1JJ+fMXJoVR237L1VdXySe88YuXaMMRWT3KUxxkjyrxNjJy1y9yxVtS3Jo2aMfCfJlcve2+iBE+d3Jbl9PR4CAAAAAAAAAAAAAAAAAAAAAADAnkP0kK3ohInza5r3T92/aaOHqy6cOD+9YeezJs4vWeP9G/Gdfjaz/439+BhjZ8PeLlMx0evGGN9bl5cAAAAAAAAAAAAAAAAAAAAAAACwxxA9ZCs6ZuJ8R/P+L02cP6x5/1qdl+TOGefnVNUBy1pWVQclOWeON63FXyS5bcb5mVV15Bp3/EBV7ZXkNRNja/1O620qTPm5dXkFAAAAwP+yc7fR3t/Tncc/OwmJmyCxUIqmqprRIpJSVosKHdFanbYxDC1m2nEzFCW0Q2+m425pKRltMe0w1TbuBiW1VKkZatoplQhFtRiiqoSoIdIkkux5cK52tZXr/P7nnP8+57pOXq+1zvLgu893f3/JkYdvAAAAAAAAAAAAAAAAAAAOK6KH7CtVVUlOWBhbihLu1NL9Jwzv35HuPj/JWZuM3DTJz65x5dOS3HiT8//Z3eftZEF3X5TkBZuMXCvJc3ey45/58STftsn5R5KcvcZ9o6rq1knutDD2jt14CwAAAAAAAAAAAAAAAAAAAAAAAIcX0UP2m5skOWZh5tPDb1i6/zpVtVnk71DwxCSf3eT8KVX10J0uqapHJXnCJiOXJHn0Tvcc8MwkH9zk/MFVteOYY1XdL8nzNhnpJI/o7it3umsXPT5JLcz87m48BAAAAAAAAAAAAAAAAAAAAAAAgMOL6CH7zc1WmPnM8BtWuX+Vd+6Z7r4wyY8kuWyTsf9eVc+qqqO2en9VHV1VZyZ50cLok7v7L7Z6/1Xp7suSPDjJFzcZe1pVvaSqrr3V+6vqiKp6SpLfSXLkJqPP6+63b/X+vVJVJyR5+MLYe7r7w7vwHAAAAAAAAAAAAAAAAAAAAAAAAA4zoofsNzdcOP9Sd186+YDuvjjJRQtjS+/cc939B0kekOTyg4wckeQpSf6sqh6xSiiwqo6tqh9P8sEkj18Yf0Z3/8pW3ryku9+f5LQkX95k7EeTfLiqnlRVxy3deSDg+NAk5yZ5VpLNIpC/meTJW3jyoeC5SY5emHn+bjwEAAAAAAAAAAAAAAAAAAAAAACAw89mcS44HB2/cP6lXXnFxp7rbnK+9M5DQne/oarumeSsJLc8yNiJSf5rkl+uqnOT/GmSC5L8bZJKclySmyT5jiQnZfm/O19N8tPd/Zwdf8BV6O53VdVdkrwyye0OMnaLJM9J8uyq+rMkf5LkM0m+kOSKbHzTjZKckuROWY4CdpIzkzy5u3un37Bbqur+SU5fGPvLJK/ahefsSFU9Jsmjd2HVN+3CDgAAAAAAAAAAAAAAAAAAAAAAgMOG6CH7zXEL51/elVcs7zksoodJ0t3/u6rukOTpSR6egwf+rpnkLgd+tuuDSX6su9+1gzsWdfeHqurOSZ6a5PFJrneQ0SOzEWo8aQfrzk/y6O5+0w7u2HVVdbMkL1xh9AndfcX0e9bgRkluu9ePAAAAAAAAAAAAAAAAAAAAAAAAuLo5Yq8fAGt2zML5V3blFclFC+dL7zykdPcXu/uxSW6V5NlJPrrmFecmeVCS208HD/9ed1/S3T+X5IQkP5Pkz9a84i+S/Ick33wYBg+PTPKKbIQCN/OGw+3bAAAAAAAAAAAAAAAAAAAAAAAA2F2ih+w311w4v3xXXrG8Z+mdh6Tu/nSSpyd5UpJ3r+HK9yW5S3ef0t2v7O4r13DnlnT33yb5xSRnJHnzGq78RJL7dveJ3f3i7v7qGu7cbb+Q5O4LM59P8shdeAsAAAAAAAAAAAAAAAAAAAAAAACHMdFD9hvRwyFVdeuq+vVsxO5en+TOa7j2Dkn+qKreWVU/WlXXWMOdK6uqm1bVc7PxTW9Jctoarj0hye9V1TlV9RNVdZ013Llrquph2QhALnlkd392+j0AAAAAAAAAAAAAAAAAAAAAAAAc3kQP2W+W/qav2JVXLO85cldesQZVdf2qemmSDyf590muteYVRyb5riQvSfKxqnpUVdWad/wTVXV0Vf1iko9nI/B3vYE1Jyd5fpLzq+qpux103I6q+q4kv7bC6Au6+3XT7wEAAAAAAAAAAAAAAAAAAAAAAODwd9RePwDW7PKF8936m1/a89VdecUOVdXdkvx2klvu0spbJHlRktOr6mHd/el1L6iqb0vy8iS3W/fdB3HDJM/Mxjf9cHd/eJf2bklV3TbJ2UmuuTD6f5I8af5Fa/e5JB/ahT3flOToXdgDAAAAAAAAAAAAAAAAAAAAAABwWBA9ZL+5bOF8t/7mr7FwvvTOPVdV90vy2ixH8JLkr5L8fpJ3JjkvyReSXJikkhyfjfDfSUnuluQ+SW6+cN+9k5xXVad29we28fyrVFV3TvKWJNdfYfxzSd6cjW86J8nns/FdX83G9xyf5Fuz8U3/Msk3L9x3cpJzqup+3f2/tvUBQ6rqltn493fcwuhHk/xAdx8W0c5/rLt/NcmvTu+pqg8mue30HgAAAAAAAAAAAAAAAAAAAAAAgMOF6CH7zVKMbZWA3zoc1tHDqjotqwUPP5DkWUn+R3dffpCZi5N8Ksn7krysqo5K8sAkT83mcbgbJXlbVd2zuz+0lfdflaq6Y1YLHp6f5NlJfqO7LznIzKcP/HwgyauqqpJ8Xza+6a6b3H3tJG+squ/t7nds5f1TqurGSd6a5RDlp5N8T3dfMP8qAAAAAAAAAAAAAAAAAAAAAAAA9osj9voBsGYXLZxfd1dekRy7cL70zj1TVTdNclaWg4e/nOSU7n7FJsHDr9Hdl3f3WUlOTvLChfEbJzm7qq6z6v1X5cDvvzrLwcNXJ7l9d794k+Dh1+gNb0zyXUl+LskVm4xfO8nrqupmq94/papukI0Q5G0WRi/MRvDwE9NvAgAAAAAAAAAAAAAAAAAAAAAAYH8RPWS/+cLC+fV25RXLe5beuZdemuT4hZnHd/fjuvuy7S7p7ku7+zFJzlgY/aYkv7TdPQc8N8mtF2ae190P7O4vbXdJd1/Z3U9P8m+S9Cajxyd5yXb3rMOBEOSbktxhYfRLSe7T3R+afxUAAAAAAAAAAAAAAAAAAAAAAAD7jegh+82FC+c32I1HJLn+wvnSO/dEVZ2a5LSFsV/u7hesa2d3Py/JixbGHllVt9/O/VV16ySPWBh7Q5Inb+f+q9Ldr0nyHxfGTquq71/Xzq2oqmOSnJ3krgujFye5X3efM/8qAAAAAAAAAAAAAAAAAAAAAAAA9iPRQ/abzy+cH11VN5h8QFUdn+SaC2OHZPQwyRkL53+d5CcH9j4pyWcWZp64zbufkM3/W3dxkkd195XbvP9gnpPkvIWZ7X7TtlXVNZK8JsmpC6OXJvnB7n7n/KsAAAAAAAAAAAAAAAAAAAAAAADYr0QP2W8+ucLMTYbfsMr9q7xzV1XVTZPcd2Hsmd19ybp3d/fFSZ6xMPagrQYrq+qoJA9ZGPuV7l4KLm5Zd3eSn1kYu0dV3Xbduw+mqo5M8vIk37cwenmSB3T3W+ZfBQAAAAAAAAAAAAAAAAAAAAAAwH4mesi+0t0XJblwYewbhp9xwsL5Bd39leE3bMc9ktQm55dnI5g35awkV2xyfs0kd9vind+e5NiFmd/c4p1b8aYkn1+Yudfg/n9QVUck+Y0k918YvSLJj3T32eOPAgAAAAAAAAAAAAAAAAAAAAAAYN8TPWQ/+vjC+TcP77/1wvnS+/bKUlDw3d39/6aWd/cXk/zpwtjdt3jt0jf9dXd/cIt3rqy7O8kfLIxt9Zu2rKoqyYuT/MjCaCf5se5+1fSbAAAAAAAAAAAAAAAAAAAAAAAAuHoQPWQ/WorYfcvw/qX7xyJ7O3SrhfN378Ib3rVwfuIW71v6pqXI4jqs+5u248wkD19h7jHd/bLhtwAAAAAAAAAAAAAAAAAAAAAAAHA1InrIfnTuwvkdh/efvHD+3uH923XDhfPP7cIblnYsvXGr84fjN21JVf1CksetMPqk7n7R5FsAAAAAAAAAAAAAAAAAAAAAAAC4+hE9ZD9aih6eVFVHTiyuqqOS3GFh7FCNHh63cP75XXjD0o6tBgL34zetrKp+PslPrjD6s939S1PvAAAAAAAAAAAAAAAAAAAAAAAA4OpL9JD96D1JLtnk/LpJThnafeck197k/JIk5wzt3qkrFs6P3oU3HLNw3lu8bz9+00qq6qeS/KcVRp/V3c+YeAMAAAAAAAAAAAAAAAAAAAAAAACIHrLvdPclSf5oYex7htbfe+H8nQfedyj6ysL5jXbhDUs7Lt7iffvxmxZV1eOSPHuF0ed390+vez8AAAAAAAAAAAAAAAAAAAAAAAD8PdFD9qu3Lpz/0NDe+y+cv2Vo7zp8ZuH85rvwhlssnH92i/ftx2/aVFU9PMmZK4y+uLufuM7dAAAAAAAAAAAAAAAAAAAAAAAA8M+JHrJfvWbh/OSq+pZ1Lqyqb0tyu01GOsvv2ksfXzi/5y684dSF86U3bnX+rlV1zBbv3Kp7LZxv9ZsOqqp+OMmLk9TC6MuSPHpdewEAAAAAAAAAAAAAAAAAAAAAAOBgRA/Zl7r7Y0n+ZGHssWte+7iF8z/u7k+seec6vXfh/ISqOnFqeVXdLsnNF8bev8Vrl77pmCTfvcU7V1ZVxyX5joWxrX7TwXadno2Y4dJ/11+Z5Ee7u9exFwAAAAAAAAAAAAAAAAAAAAAAADYjesh+9tKF839XVTddx6KqunmShyyM/cY6dg364xVmnjS4/6dWmFnljf/Yu5NcvjAz+U1nJDlqYWar3/Q1qup7k7w8yZELo69P8pDuvnKnOwEAAAAAAAAAAAAAAAAAAAAAAGAVoofsZ7+V5IJNzq+d5Nlr2vULSY7Z5PyzB95zyOruP0/ykYWxh1XVieveXVUnJXnQwtj53f3+rdzb3V9O8vaFsXtV1b23cu8qDgQ1f2Jh7OIkb9vhnnsmeW2Say6MvinJA7t7KQIJAAAAAAAAAAAAAAAAAAAAAAAAayN6yL7V3Zck+S8LYw+tqh/cyZ6qekCSBy+Mndndl+5wzwlV1Qs/P7+THUlevnB+VJLXVdUNdrjnH1TVjbIR7Vv679Ert7nirBVmfquqvmGb93+NqrpWktcnuc7C6O9291d2sOeuSc7O5sHNZCOseHp3X7bdXQAAAAAAAAAAAAAAAAAAAAAAALAdoofsd2cm+auFmZdV1Z23c3lV3SXJSxbGzs9yfPFQ8cIklyzM/Iskb6iq43e6rKpukuSNSW61MHpZkl/Z5ppXJPmbhZmvS/KmdYQPq+rYJK9Ossrf1PN2sOekJG9Kct2F0Xcm+f4DEVAAAAAAAAAAAAAAAAAAAAAAAADYVaKH7GvdfXGSJy6MHZvkLVV1v63cXVX/KsnvZzk6d0Z3/91W7t4r3X1BVgs03j3JeVV1t+3uqqp7Jzkvq8UBX9zdn9rOnu6+NMnTVhi9bZL3VtUPbmdPklTVyUnek2SVv6Wzu/vd29xzYpK3JLnBwui7knzfgf8fAAAAAAAAAAAAAAAAAAAAAAAAwK47aq8fANO6+zVV9fIkD95k7PpJzq6qVyR5end/+GCDVXXbJD+X5IErrD+ru1+7pQfvvacl+ddJbrUwd4skf1hVb0vyS0ne1t2XbfYLVXV0kvskOSMb4cRVfCrJz6w4ezC/luShSe66MHdcktdV1Z8meW6SNy4FA6vqqCR3S/KEbMQOa4X3fDnJY1eYu6p910vyB0lutDB6aZLXJHlg1SpPWotzu/vc3VoGAAAAAAAAAAAAAAAAAAAAAADAoU/0kKuLRyY5Jcm3bDJT2QgjPriq3pvkj5N8PMlFSY5N8o1JvjPJHVbc+eEkj9rug/dKd19cVadn4/uvtcKv3OvAzyVV9a4k70tyYZIvZOOf6fFJbpjkjknulOToLTzn0iSnd/eXt/A7X6O7r6yqByQ5J8mNV/iVOyV5VZKvVtW5Sd6T5PPZ+KYrshFHPD7J7bIRUrzOVp6T5KHd/ckt/M4/dnySr19h7ugkz9nmju36z0lEDwEAAAAAAAAAAAAAAAAAAAAAAPgHoodcLXT3RVV1nyTvTHKLFX7ljgd+tuuTSe7T3Rft4I49093nVdUPJfmdJMes+GvHJLnHgZ91uCzJA7r73eu4rLs/VVX3TfLWbIQDV3GNJN9x4Gctz0jymO5+/ZruAwAAAAAAAAAAAAAAAAAAAAAAgEPaEXv9ANgt3X1+klOTfGx41UeTnNrdnxzeM6q735zktCQX7MH6LyS5X3efvc5Lu/vcbEQZp/8GrsrfJXlId79oD3YDAAAAAAAAAAAAAAAAAAAAAADAnhA95Gqluz+a5E5Jfn9oxZuT3Km79yKqt3bd/Y4kp2Tju3bL25Kc3N1vnbi8uz+Q5NuT/PbE/QdxTpK7dPdZu7gTAAAAAAAAAAAAAAAAAAAAAAAA9pzoIVc73f233X1akn+b5II1XXtBkod19327+4truvOQ0N2f6u77JvmBJO8dXPW+JPfv7nt39/mDe9LdX+zuhyS5R5K3D676v0kekeTO3f3+wT0AAAAAAAAAAAAAAAAAAAAAAABwSBI95Gqru1+W5FZJHpPkz7d5zYcO/P43dvdvrutth6LufkN3n5zku5P8tySfW8O1FyZ5aZJTu/uk7n7tGu5cWXf/YXffM8kpSc5M8ldruPbLSV6V5PuT3Ka7f727r1zDvQAAAAAAAAAAAAAAAAAAAAAAAHDYOWqvHwB7qbu/kuSFSV5YVbdJclqSk5N8a5KvT3JskmsnuTgbMbtPZSN0eG6S3+vuj+ziWz+RpHZr3ybveEeSd1TVI5LcLsldD/zvbZLcLMlNsvHP7OgDv3JpNv75fTbJp5N8JMkHkvxJkvO6u3f1A65Cd5+bjX+nTzjwd/Cd2fimE7Pxd/B1Sa6T5Jhs/Du4NMnfJbkgyd8k+WiSDyZ5V5L3dPflg2/9RA6BvwMAAAAAAAAAAAAAAAAAAAAAAABYheghHNDdf5nkL/f6HYeLA7HC9x/42Tf8HQAAAAAAAAAAAAAAAAAAAAAAAMD6HLHXDwAAAAAAAAAAAAAAAAAAAAAAAAAA9ifRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAADGPixTAADBbElEQVQAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAAAAAAAAAAADBC9BAAAAAAAAAAAAAAAAAAAAAAAAAAGCF6CAAAAAAAAAAAAAAAAAAAAAAAAACMED0EAAAAAAAAAAAAAAAAAAAAAAAAAEaIHgIAAAAAAAAAAAAAAAAAAAAAAAAAI0QPAQAAAAAAAAAAAAAAAAAAAAAAAIARoocAAAAAAAAAAAAAAAAAAAAAAAAAwAjRQwAAAAAAAAAAAAAAAAAAAAAAAABghOghAAAAAAAAAAAAAAAA/H/27jtc1quqH/h3JYGEAIHQQpXQO4TeSwAxKFJEKUoTEFEpSlERUQREUER/gCggVem9d6lSQgu9hE7oEAIkgYSE9fvjnWtCuHfemTnznnPvyefzPOfhIXvPWnvOfWfO7P3uWRsAAAAAAACASSh6CAAAAAAAAAAAAAAAAAAAAAAAAABMQtFDAAAAAAAAAAAAAAAAAAAAAAAAAGASih4CAAAAAAAAAAAAAAAAAAAAAAAAAJNQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmISihwAAAAAAAAAAAAAAAAAAAAAAAADAJBQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmoeghAAAAAAAAAAAAAAAAAAAAAAAAADAJRQ8BAAAAAAAAAAAAAAAAAAAAAAAAgEkoeggAAAAAAAAAAAAAAAAAAAAAAAAATELRQwAAAAAAAAAAAAAAAAAAAAAAAABgEooeAgAAAAAAAAAAAAAAAAAAAAAAAACTUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiEoocAAAAAAAAAAAAAAAAAAAAAAAAAwCQUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJqHoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCUUPAQAAAAAAAAAAAAAAAAAAAAAAAIBJKHoIAAAAAAAAAAAAAAAAAAAAAAAAAExC0UMAAAAAAAAAAAAAAAAAAAAAAAAAYBKKHgIAAAAAAAAAAAAAAAAAAAAAAAAAk1D0EAAAAAAAAAAAAAAAAAAAAAAAAACYhKKHAAAAAAAAAAAAAAAAAAAAAAAAAMAkFD0EAAAAAAAAAAAAAAAAAAAAAAAAACah6CEAAAAAAAAAAAAAAAAAAAAAAAAAMAlFDwEAAAAAAAAAAAAAAAAAAAAAAACASSh6CAAAAAAAAAAAAAAAAAAAAAAAAABMQtFDAAAAAAAAAAAAAAAAAAAAAAAAAGASih4CAAAAAAAAAAAAAAAAAAAAAAAAAJNQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmISihwAAAAAAAAAAAAAAAAAAAAAAAADAJBQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmoeghAAAAAAAAAAAAAAAAAAAAAAAAADAJRQ8BAAAAAAAAAAAAAAAAAAAAAAAAgEkoeggAAAAAAAAAAAAAAAAAAAAAAAAATELRQwAAAAAAAAAAAAAAAAAAAAAAAABgEooeAgAAAAAAAAAAAAAAAAAAAAAAAACTUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiEoocAAAAAAAAAAAAAAAAAAAAAAAAAwCQUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJqHoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCUUPAQAAAAAAAAAAAAAAAAAAAAAAAIBJKHoIAAAAAAAAAAAAAAAAAAAAAAAAAExC0UMAAAAAAAAAAAAAAAAAAAAAAAAAYBKKHgIAAAAAAAAAAAAAAAAAAAAAAAAAk1D0EAAAAAAAAAAAAAAAAAAAAAAAAACYhKKHAAAAAAAAAAAAAAAAAAAAAAAAAMAkFD0EAAAAAAAAAAAAAAAAAAAAAAAAACah6CEAAAAAAAAAAAAAAAAAAAAAAAAAMAlFDwEAAAAAAAAAAAAAAAAAAAAAAACASSh6CAAAAAAAAAAAAAAAAAAAAAAAAABMQtFDAAAAAAAAAAAAAAAAAAAAAAAAAGASih4CAAAAAAAAAAAAAAAAAAAAAAAAAJNQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmISihwAAAAAAAAAAAAAAAAAAAAAAAADAJBQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmoeghAAAAAAAAAAAAAAAAAAAAAAAAADAJRQ8BAAAAAAAAAAAAAAAAAAAAAAAAgEkoeggAAAAAAAAAAAAAAAAAAAAAAAAATELRQwAAAAAAAAAAAAAAAAAAAAAAAABgEooeAgAAAAAAAAAAAAAAAAAAAAAAAACTUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiEoocAAAAAAAAAAAAAAAAAAAAAAAAAwCQUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJqHoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCUUPAQAAAAAAAAAAAAAAAAAAAAAAAIBJKHoIAAAAAAAAAAAAAAAAAAAAAAAAAExC0UMAAAAAAAAAAAAAAAAAAAAAAAAAYBKKHgIAAAAAAAAAAAAAAAAAAAAAAAAAk1D0EAAAAAAAAAAAAAAAAAAAAAAAAACYhKKHAAAAAAAAAAAAAAAAAAAAAAAAAMAkFD0EAAAAAAAAAAAAAAAAAAAAAAAAACah6CEAAAAAAAAAAAAAAAAAAAAAAAAAMAlFDwEAAAAAAAAAAAAAAAAAAAAAAACASSh6CAAAAAAAAAAAAAAAAAAAAAAAAABMYp+tHgDsbqpq3ySXTHLBJGdNsn+S45P8OMlRST7b3Sdu3Qh3T1V17iQXSnK+DL+3/ZJ0kp9m+N19M8nXuvt7WzbIJVXV2ZNcOMNzOluG51QZntOxSb6d5Kju/uZWjXEVVXVAkksnOU+Gf6szZng+P0zyxSRf7u6fb90IAQAAAAAAAAAAAAAAAAAAAAAA2C4UPYQkVXWtJLdOcvMkl0uy95zuJ1fVJ5O8Lskru/t9049w91NVl0/yG0kOTXKVJOde8HHfTvLhJG9P8tru/uRUY1xWVV00yW9meE5XS3KBBR/3gyRHZHhOr+/uD0w0xJVU1T4Z/q1ukeRXMxRynOcnVXV4ktcmeWl3f3HiIQIAAAAAAAAAAAAAAAAAAAAAALBNKXrI6VpV3SHJgzMU7VvU3kmuOPv5y6r6UJJ/6u4XTjDE3UpV7ZvkLknum+QKK4Y5KENxyZsneWxVfTTJE5L8d3efuJaBLqGq9k7y20nun+TaK4Y5MEOhxEOT/F1VfT7Jk5M8rbuPXctAV1BVZ0pyvwz/XgsVcJw5U5Ibzn4eU1WvTvLo7j58/aMEAAAAAAAAAAAAAAAAAAAAAABgO9trqwcAW6GqLl1V70jy/CxX8HBnrprkBVX1tqq61MZHt3uqqtslOTLJU7N6wcOduVKSpyf5bFXdeo1xR1XVTZJ8LMkLsnrBw525eJLHJ/liVd2jqmqNsRdSVb+e5FNJHpPlCh6e1l5JbpXkfVX1lKo62zrGBwAAAAAAAAAAAAAAAAAAAAAAwOmDooec7lTVbyX5QJIbrDn0jZJ8sKpus+a4W6qqzlpVL0jywiQXmjDVwUleXlXPqqr9J8yTqjpjVf1rkrckueyEqc6d5D+TvLaqzjVhnv9TVXtV1aOSvDbD73RtoZPcK8kHquoya4wLAAAAAAAAAAAAAAAAAAAAAADANqboIacrVfUnSV6S5CwTpThLkpdW1R9PFH9TVdVBSd6Z5PabmPauSf6nqs4xRfCqOkuS1yS5/xTxd+HmSd5TVReeMklV7ZXkuUkeOmGaSyR5X1Vdc8IcAAAAAAAAAAAAAAAAAAAAAAAAbBOKHnK6UVV3TfLEJDV1qiRPqqq7TJxnUlV1QJI3JzlkC9JfM8kbqmr/dQatqjMkeXmSX11n3AVdIsnbquo8E+Z4WpI7TBh/hwOSvL6qrrgJuQAAAAAAAAAAAAAAAAAAAAAAANiDKXrI6UJVXSNDQbhFCh6+J8l9klwlyTmSnGH2v1dLcr8k718kZZKnVdXVVxrw7uHZSa6wYN9jkzwnyb2SXDnJBZKcKcn+SS6Y5KpJ/ijJ85L8ZMGYV0/ylCXGu4h/THLTBfuemOTFSe47G8uFkpwlyX5Jzp/kiknunuTpSY5ZMOZFkry4qtb+3ltVfzobz5ifZSj8+HtJLpXkrEnOmOSgJIcmeUSSLy8Q58Akr6iqsy8/WgAAAAAAAAAAAAAAAAAAAAAAAE4v9tnqAcDUquqAJC/IULxwniOT/FF3v3UnbT9I8qHZzxOr6mZJnpzkYnPinTHJC6vqkO7+0fIj3zpVdcckt16g608yFMn79+7+4S76fH328+Ek/1FV58xQSPAhGX5H89ypql7U3a9eaOBzVNV1k9x/ga4nJ3l8kn/u7m/vos83Zz8fT/LMWcHBeyR5ZIYigvPcIMmfJHniAmNZSFVdNcljF+j6piT36e4jd9L2ndnP26vqkRkKWP5jkjPPiXeRDEUfb7vciAEAAAAAAAAAAAAAAAAAAAAAADi92GurBwCb4BEZirPN85YkV99FwcNf0t1vSnK1JG8b6XqRJA9fJObuoqr2SfKoBbp+Lsk1uvsxcwoe/pLu/n53PzzJdZJ8eYGHPKaq1vFe9Q9JaqTPt5Mc2t1/Pqfg4S/p7mO7+/8luXKSIxZ4yN9U1VkWjT/P7Hfz1IwXkHxsksN2UfDwF3T3Sd395CTXSHLUSPffqqpbLjRYAAAAAAAAAAAAAAAAAAAAAAAATncUPWRbq6rLJvmTkW7vTXKrZQr3JUl3H5PkN5McPtL1vlV1mWVib7HfTHLRkT7fSnKz7v7Eqkm6+0NJbpbkeyNdLzvrt7KqunKS6490Oy7Jr3f3u1bN091fyDDWL4x0PVeSO62a5zTumeQqI33+X3f/ZXf3MoG7+1NJbpLk6JGu/1JV+y4TGwAAAAAAAAAAAAAAAAAAAAAAgNMHRQ/Z7v42yT5z2o9OcvvuPn6V4N19XJLbJTlmTrd9kvzNKvG3yO8t0OfO3f2VjSbq7iOT3G2BrouMaaOP/9Pu/vAG86S7v5vkt5L8fA1jmquqzpDkYSPd3pvkQavm6O7PJbn7SLeLZrF/RwAAAAAAAAAAAAAAAAAAAAAAAE5nFD1k26qqiya57Ui3v+7ur20kz6z439+OdPudqjp4I3k2Q1VVkkNHur21u9+yrpzd/dok7xzpdpMNphl7/GeTPHODOf5Pd38syX+PdLtWVZ15g6l+N8kF57SfnORe3X3SRpJ09yuTvHyk2wOryt8UAAAAAAAAAAAAAAAAAAAAAAAAfoECVWxnf5Jk7zntRyZ56ppyPTnJF+e07z0bz+7u4CTnGOnznxPkHft3OF9VXWCVwFV1hiRXGOn2rO4+eZX4c4w9p32SHLLBHPcbaX9Od39igzl2+IskP5/TfokkN19TLgAAAAAAAAAAAAAAAAAAAAAAALYJRQ/Zlqpq7yR3HOn2L+sqdNfdJyV5wki3362q3f01d7GR9p8necsEed+cpEf6XHzF2BfO/OKXSfKmFWPP8/4kPxzps+pzSlVdJslV5nTpJI9bNf4vBes+MsmrRrrdaV35AAAAAAAAAAAAAAAAAAAAAAAA2B529wJssKobJznfnPafJvnvNed8dpIT57SfP8mN1pxz3Q4caf9md39v3Um7+ztJvjXS7Vwrhh97Tkny8RVj79KsEOanRrqt+pyS8QKD7+nusfzLeupI+y2r6ixrzgkAAAAAAAAAAAAAAAAAAAAAAMAeTNFDtqvfHGl/bXf/eJ0Ju/uYJK8f6TY2rq2270j72gsensp3R9rPtGLcsef0w+7+2Yqxx0z1nJLxa+kFG4i9K2/O/Gtg/yQ3mSAvAAAAAAAAAAAAAAAAAAAAAAAAeyhFD9mubjrS/tqJ8o7F/dWJ8q7LD0faj5sw91jsH60Yd9s9p6o6T5LLj3Rb+zXe3ScledNIt939GgcAAAAAAAAAAAAAAAAAAAAAAGATKXrItlNV50tymZFub5ko/ZtH2i9XVeedKPc6fH+k/ZwT5h6LPTa2VR93jhXjLmKq53STJDWn/Yvd/aUVY48Zu8bHCo4CAAAAAAAAAAAAAAAAAAAAAABwOqLoIdvRNUbav9bdX5sicXd/Ock3R7pdfYrca/KZJD2n/aAJc4/F/uKKcb+Z5Adz2verqrOvGHvMWIHLVZ/T2DX+vyvGXcR7RtovWVUHTJgfAAAAAAAAAAAAAAAAAAAAAACAPYiih2xHVxlp//DE+T840n7lifOvrLuPTvLJOV3OXlVXWHfeqrpSkrPN6fLF7h4rJrlT3d0ZLwJ4/VViz1NV50hy+TldfpLkQyuG38pr/MgkP5rTXkkOmTA/AAAAAAAAAAAAAAAAAAAAAAAAexBFD9mODhlp/9jE+cfi77ZFD2deN9L+6xPk/I2R9rduMP5WPKfDMv899t3dfeKKsQ8ZaZ/sGp8Vkfz4SLfd/RoHAAAAAAAAAAAAAAAAAAAAAABgkyh6yHZ0yZH2IyfO//mR9ktMnH+jnpzk5Dnt96+qM60rWVWdJcn9FxjTRvxXkmPmtN+1qs63wRz/p6r2SvKQkW4rPaeqOijJASPdXOMAAAAAAAAAAAAAAAAAAAAAAADsFhQ9ZFupqkpy8Ei3sYJtGzUW/+CJ829Id38lyXPndDlfkoetMeUjkpxnTvv/dPcRG0nQ3ccmecKcLmdK8riN5DiN+yS5/Jz2I5O8asXYFxlpPyHJUSvGXtTYNT42RgAAAAAAAAAAAAAAAAAAAAAAAE4nFD1kuzkoyX4jfb4x8RjG4p+5quYV+dsdPCDJt+e0P6Sq7rLRJFV17yR/NqfLT5P88UbzzPx9kk/Oaf/dqtpwMcequkWSx8/p0knu1d0/XzHFwSPt3+zuXjH2osaucUUPAQAAAAAAAAAAAAAAAAAAAAAASKLoIdvP+Rfo862Jx7BI/EXGuWW6+/tJ7pTkxDndnllVj66qfZaNX1X7VtW/Jvn3ka4P7u7PLht/Z7r7xCS/m+SYOd0eUVVPr6r9l41fVXtV1UOSvDzJ3nO6Pr67375s/FMZu3amvr4XybFbX98AAAAAAAAAAAAAAAAAAAAAAABsHkUP2W7OOdL+o+4+YcoBdPfxSY4d6TY2zi3X3W9JcrskJ+2iy15JHpLk41V1r0UKBVbVWavqPkk+meT+I90f1d1PWmbMY7r7Y0kOS/LjOd3unuQzVfWgqjpwLOasgONdknw4yaOTzCsC+ZwkD15iyDszdu18Z4PxF/HtkfYDqmpe4UcAAAAAAAAAAAAAAAAAAAAAAABOJ6q7t3oMsDZVdfskL5jT5ajuvtAmjOPrSc4/p8vtuvvFU49jHarqekmem+RXRrqemKHw3wcyFN77QZJKcmCSg5JcM8khmV8UMEl+luSh3f1Pq496vqq6bIbr5AojXU9O8vEk70vyrSRHz/7bgUnOneSqSa6eZN+ROJ3kX5M8uLtPXnngSarq35Pce06X/+7uO28kxwJjuESSz410O093f3fKcSyjqv4kyR9vQqpLZycFhffdd99c7GIX24T0sP0c+e2xOsIAAADsSS5x0Fm2eghwumN9BQAAYHuxvgJbwxoLAADA9mKNBTaf9RUAAIDtxfoKbMwXvvCFnHDCCTtr+nF3H7DZ49ksY8XHYE9z4Ej7jzdlFON5zrEpo1iD7n53VV0pySOT/EF2XeDvjEmuNftZ1SeT3KO737+BGKO6+1NVdY0kf5Xk/kl29Sa/d4ZCjYdsIN1Xkvxxd79uAzFObXe4xhfJcY4ku03RwwxFKi+7VclPOOGEfOpTn9qq9AAAAAC7jU99f6tHAAAAALBns74CAAAAsHHWWAAAAAA2xvoKTOaMWz2AKe211QOANdtvpP24TRlFMnbkzNg4dyvdfUx33zfJRZM8Jsnn15ziw0numOSKUxc83KG7f9rdf5Pk4CR/neTja07x2SR/lOQSayx4mOwe1/giRyrtUdc4AAAAAAAAAAAAAAAAAAAAAAAA01D0kO1mrErpSZsyivE8e2Q11e7+RpJHJnlQksPXEPKjSa7V3Vft7hd098/XEHMp3f2DJP+Y5IFJ3rCGkF9OcvPuvnR3/0d3/2wNMU9td7jGF8mxR17jAAAAAAAAAAAAAAAAAAAAAAAArJeih2w3u0NBuEXy7HEF4arq4lX1tCTfS/KKJNdYQ9grJfnfqnpXVd29qs6whpgLq6rzVdXjMjynNyU5bA1hD07y+qr6UFX9aVWdeQ0xT213uMYVPQQAAAAAAAAAAAAAAAAAAAAAAGAhih6y3Yxd0ydvyijG8+y9KaNYg6o6W1U9I8lnktwzyZnWnGLvJNdL8vQkX6iqe1dVrTnHL6iqfavqH5N8KckDkxwwQZqrJPmXJF+pqr9aY0HH3eEaXyTHHnONAwAAAAAAAAAAAAAAAAAAAAAAMJ19tnoAsGYnjbRv1jU/ludnmzKKDaqq6yf57yS/skkpL5Tk35Pctqru2t3fWHeCqrp8kuclucK6Y+/COZP8fYbn9Hvd/ZkNxtsdrvFFcuxu1/h3k3xqE/JcMqcUfDw5pxSIPDHJ1zYhPwAAe5aLJdl3J//9hCRf2OSxAAAAAOyJrK8AAAAAbJw1FgAAAICNsb4CAMCyLpTkjDv578ds8jg2laKHbDcnjrRv1jV/hpH2sXFuuaq6RZKXZudvjKf1tSRvTPKuJEckOTrJ95NUknNkKPx3SJLrJ/m1JBcciXfTJEdU1Y27+xMrDH+nquoaSd6U5GwLdP9ukjdkeE4fSvK9DM/rZxmezzmSXC7Dc7pZkkuMxLtKkg9V1S26+20rPYHB7nCNj13fyW52jXf3vyX5t60eBwAAnFpVfTLJZXfS9IXuvtxmjwcAAABgT2N9BQAAAGDjrLEAAAAAbIz1FQAAWIyih2w3PxtpX6SA3zrs0UUPq+qwLFbw8BNJHp3kxd190i76HJ/kqCQfTfLsqtonye2T/FV2PnHf4dxJ3lpVh3b3p5YZ/85U1ZWzWMHDryR5TJJndfdPd9HnG7OfTyR5YVVVkt/I8JyuPSf2/kleU1W/3t3vWGb8p7I7XON7XNFDAAAAAAAAAAAAAAAAAAAAAAAAtsZeWz0AWLNjR9rPsimjSM460j42zi1TVedL8tyMF897YpKrdvfz5xQ8/CXdfVJ3PzfJVZI8eaT7eZK8qqrOvGj8nZk9/kUZL3j4oiRX7O7/mFPw8Jf04DVJrpfkb5KcPKf7/kleVlXnXzT+aewO1/jY9Z3sxtc4AAAAAAAAAAAAAAAAAAAAAAAAm0fRQ7abo0faD9iUUYznGRvnVnpGknOM9Ll/d9+vu09cNUl3n9Ddf5LkgSNdL5bkn1fNM/O4JBcf6fP47r59d/9o1STd/fPufmSSOyTpOV3PkeTpK6bZHa7xRXLsztc4AAAAAAAAAAAAAAAAAAAAAAAAm0TRQ7ab74+0n30zBpHkbCPtY+PcElV14ySHjXR7Ync/YV05u/vxSf59pNsfVtUVV4lfVRdPcq+Rbq9M8uBV4u9Md78kyV+OdDusqm65Qvjd4Rofy3FSd/9wE8YBAAAAAAAAAAAAAAAAAAAAAADAbk7RQ7ab742071tVZ59yAFV1jiRnHOm2WxY9TPLAkfavJ/nzCfI+KMm3Rvo8YMXYf5b573XHJ7l3d/98xfi78k9Jjhjps8pzGrvGz7tCzGWN5Th6E8YAAAAAAAAAAAAAAAAAAAAAAADAHkDRQ7abry7Q56CJx7BI/EXGuamq6nxJbj7S7e+7+6frzt3dxyd51Ei3Oy5bsLKq9kly55FuT+rusYKLS+vuTvLXI91uWFWXXTL02LUz9fW9SI6vbMIYAAAAAAAAAAAAAAAAAAAAAAAA2AMoesi20t3HJvn+SLcLTzyMg0fav9Pdx008hlXcMEnNaT8pyfMmzP/cJCfPaT9jkusvGfNqSc460uc5S8ZcxuuSfG+kz02WjPnlkfaDqmq/JWMu6+CR9i9NnB8AAAAAAAAAAAAAAAAAAAAAAIA9hKKHbEdjBdcuMXH+i4+0764F4cYKCh7e3T+cKnl3H5PkAyPdbrBk2LHn9PXu/uSSMRfW3Z3kLSPdln1OY9dPJbnYkjGXtade4wAAAAAAAAAAAAAAAAAAAAAAAGwyRQ/ZjsaK2F1q4vxj8ScrsrdBFx1pP3wTxvD+kfZLLxlv7DmNFVlch7U+p+4+LslXRrq5xgEAAAAAAAAAAAAAAAAAAAAAANgtKHrIdvThkfYrT5z/KiPtH5k4/6rOOdL+3U0Yw1iOsTEu239PfE7JFl7jVbV/xgs17q7XOAAAAAAAAAAAAAAAAAAAAAAAAJtM0UO2o7GCcIdU1d5TJK6qfZJcaaTb7loQ7sCR9u9twhjGcixbIHA7Pqdk/Bq/6goxF3WlJPNePz9N8pkJ8wMAAAAAAAAAAAAAAAAAAAAAALAHUfSQ7eiDGQqv7cpZMl1RuGsk2X9O+0+TfGii3Bt18kj7vpswhv1G2nvJeNvxOSXJu0farzsrwDmFQ0fa39fdJ02UGwAAAAAAAAAAAAAAAAAAAAAAgD2MoodsO9390yT/O9LtVydKf9OR9nfNxrc7Om6k/dybMIaxHMcvGW87Pqckec/I4w5Ics0V4i5i7Bp/80R5AQAAAAAAAAAAAAAAAAAAAAAA2AMpesh2NVZ47bcmyvvbI+1vmijvOnxrpP2CmzCGC420f3vJeNvxOaW7T0zyzpFua7/Gq+o8SW4w0m13vsYBAAAAAAAAAAAAAAAAAAAAAADYZIoesl29ZKT9KlV1qXUmrKrLJ7nCnC6d8XFtpS+NtB+6CWO48Uj72BiX7X/tqtpvyZjLuslI+7LPaYcXj7Tfrqr2XjH2LmMmmRfzS939wTXnBAAAAAAAAAAAAAAAAAAAAAAAYA+m6CHbUnd/Icn7Rrrdd81p7zfS/p7u/vKac67TR0baD66qS0+VvKqukOSCI90+tmTYsee0X5IbLRlzYVV1YJJrjnRb9jnt8NIkP53TfsEkt1kx9i+pqkpyn5Fuz1tXPgAAAAAAAAAAAAAAAAAAAAAAALYHRQ/Zzp4x0v77VXW+dSSqqgsmufNIt2etI9eE3rNAnwdNmP8vFuizyBhP7fAkJ430mfI5PTDJPiN9ln1OSZLu/mGSl410e8isWOE6/FaSS80bUpLnrCkXAAAAAAAAAAAAAAAAAAAAAAAA24Sih2xn/5XkO3Pa90/ymDXlemyS/ea0f3s2nt1Wd386yZEj3e5aVZded+6qOiTJHUe6faW7P7ZM3O7+cZK3j3S7SVXddJm4i5gV1PzTkW7HJ3nrBtL880j7VZLcbQPxkyRVtV+Sfxzp9qru/txGcwEAAAAAAAAAAAAAAAAAAAAAALC9KHrIttXdP03y/0a63aWqbrORPFV1uyS/O9LtX7v7hA3mObiqeuTn4RvJkeR5I+37JHlZVZ19g3n+T1WdO8lLM/5+9IIVUzx3gT7/VVUXXjH+L6mqMyV5RZIzj3R9dXcft2qe7v5wkjePdPuXqrrIqjlmHp/koiN9HrvBHAAAAAAAAAAAAAAAAAAAAAAAAGxDih6y3f1rkq+N9Hl2VV1jleBVda0kTx/p9pWMF1/cXTw5yU9H+lwmySur6hwbTVZVByV5TcYL6p2Y5Ekrpnl+km+O9Dlvkteto/BhVZ01yYuSLHJNPX6j+ZI8OMnJc9rPluTVVXXOVYJX1X2S/NFItxd393tXiQ8AAAAAAAAAAAAAAAAAAAAAAMD2pugh21p3H5/kASPdzprkTVV1i2ViV9WtkrwxyVlGuj6wu3+yTOyt0t3fyWIFGm+Q5Iiquv6quarqpkmOyGLFAf+ju49aJU93n5DkEQt0vWySj1TVbVbJkyRVdZUkH0yyyLX0qu4+fNVcO3T3R5P8x0i3yyV5Z1VdatG4VbVXVT08yRNGuh6f5EGLxgUAAAAAAAAAAAAAAAAAAAAAAOD0RdFDtr3ufkmS5410O1uSV1XVc6vq0vM6VtVlq+oFSV6R5ICRuM/t7pcuPNjdwyOSfHGBfhfKUEjvLVV186o649gDqmrfqrplVb0jyZuTnHeBPEcl+esF+s3z1CTvXaDfgUleVlWHV9Xtqmr/sQdU1T5VdWhVvSpDwcNLLpDnx0nuu0C/Rf1lks+N9Llskg9X1SOq6ly76lSDmyZ5X5K/TVIjce/f3V9darQAAAAAAAAAAAAAAAAAAAAAAACcbuyz1QOATfKHSa6a5FJz+lSS303yu1X1kSTvSfKlJMcmOWuSiyS5bpIrLZjzM0nuveqAt0p3H19Vt83w/M+0wENuMvv5aVW9P8lHk3w/ydEZfqfnSHLOJFdOcvUk+y4xnBOS3La7f7zEY35Jd/+8qm6X5ENJzrPAQ66e5IVJflZVH85QzPB7GZ7TyRmKI54jyRWSXDvJmZcZTpK7rLNQYHcfW1W/k6FQ4bx/s/2TPCzJX1bVu5J8IMk3M/yeD0xymSQ3ylDQchH/3d3/ueq4AQAAAAAAAAAAAAAAAAAAAAAA2P4UPeR0YVYU7teSvCuLFXS78uxnVV9N8mvdfewGYmyZ7j6iqn4rycuT7Lfgw/ZLcsPZzzqcmOR23X34OoJ191FVdfMkb85QsHARZ0hyzdnPWoaR5E+6+xVrindK4O6PVdVtkrwqyRlHup8hyY1nP6t6Y5J7bODxAAAAAAAAAAAAAAAAAAAAAAAAnA4oesjpRnd/papunOQNSS42YarPJzmsu786YY7JdfcbquqwJC9Kcp5NTn90kjt095vXGbS7P1xVN0zyikx7DezMT5L8QXc/d6oE3f3GqrptkhckOfNUeZK8Lslvd/eJE+YAAIDTgycnOfdO/vt3N3sgAAAAAHso6ysAAAAAG2eNBQAAAGBjrK8AAMACqru3egywqarqwCTPT/JrE4R/Q5I7dvcx6w5cVQcn+dJIt7/r7oevOe8FkzwtyWHrjDvHW5Pco7u/MlWCqjp7kicmudNUOU7jQ0nu3t0f24xkVXWFJC/P+gs7dpLHJnlod/98zbEBAAAAAAAAAAAAAAAAAAAAAADYhvba6gHAZuvuH3T3YUnuluQ7awr7nSR37e6bT1HwcCt191HdffMkt07ykQlTfTTJb3f3TacseJgk3X1Md985yQ2TvH3CVF9Mcq8k19isgodJ0t0fT3LFJP+Q5MQ1hT0iyXW6+yEKHgIAAAAAAAAAAAAAAAAAAAAAALCo6u6tHgNsmao6c5K7JrlPksusEOJTSf4tybO6+/h1ju20qurgJF8a6fZ33f3wicdxwyR3SnKrJOfeYLjvJ3llkv/u7rdtdGyrqqqrJLlzktsmudAGw/04yeuSPDfJ67r75A3G25CqOn+G6/ueWf7f6+dJ3pbkCUleo9ghAAAAAAAAAAAAAAAAAAAAAAAAy1L0EGaq6pJJDktylSSXS3KBJGdNsn+S4zMUszsqQ6HDDyd5fXcfuTWj3XpVVUmukOTas/+9ZJLzJzkow+9s31nXEzL8/r6d5BtJjkzyiSTvS3JE72ZvQrPr4LoZntOlM1wH501y5iT7JakMz+knSb6T5JtJPp/kk0nen+SD3X3S5o98vqraK8k1kvxqkitmKPJ57gzX+BmTHJvkR0m+mOEaf0+SN3T397ZkwAAAAAAAAAAAAAAAAAAAAAAAAGwLih4CAAAAAAAAAAAAAAAAAAAAAAAAAJPYa6sHAAAAAAAAAAAAAAAAAAAAAAAAAABsT4oeAgAAAAAAAAAAAAAAAAAAAAAAAACTUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJiEoocAAAAAAAAAAAAAAAAAAAAAAAAAwCQUPQQAAAAAAAAAAAAAAAAAAAAAAAAAJqHoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCUUPAQAAAAAAAAAAAAAAAAAAAAAAAIBJKHoIAAAAAAAAAAAAAAAAAAAAAAAAAExC0UMAAAAAAAAAAAAAAAAAAAAAAAAAYBKKHgIAAAAAAAAAAAAAAAAAAAAAAAAAk1D0EAAAAAAAAAAAAAAAAAAAAAAAAACYhKKHAAAAAAAAAAAAAAAAAAAAAAAAAMAkFD0EAAAAAAAAAAAAAAAAAAAAAAAAACah6CEAAAAAAAAAAAAAAAAAAAAAAAAAMAlFDwEAAAAAAAAAAAAAAAAAAAAAAACASSh6CAAAAAAAAAAAAAAAAAAAAAAAAABMQtFDAAAAAAAAAAAAAAAAAAAAAAAAAGASih4CAAAAAAAAAAAAAAAAAAAAAAAAAJNQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmISihwAAAAAAAAAAAAAAAAAAAAAAAADAJBQ9BAAAAAAAAAAAAAAAAAAAAAAAAAAmoeghAAAAAAAAAAAAAAAAAAAAAAAAADAJRQ8BAAAAAAAAAAAAAAAAAAAAAAAAgEkoeggAAAAAAAAAAAAAAAAAAAAAAAAATELRQwAAADiVquo5Pw/fYOy3z4n99vU8A9hcrmsAAGAzVNXdRubsB2/1GAEAAAAAAAAAAAAAANg5RQ8BAAAAAAAAAAAAAAAAAAAAAAAAgEkoeggAAAAAAAAAAAAAAAAAAAAAAAAATGKfrR4AAACwOarqy0kuvOawP09yQpKfJPluku8k+XySzyQ5PMnh3X38mnMCAAAAAAAAAAAAAAAAAAAAAHsIRQ8BAICN2CvJmWY/50hyqSTXP1X7CVX1P0lekORF3f3TzR8iAAAAAAAAAAAAAAAAAAAAALBV9trqAQAAANvavklunuTZSY6qqr+oqjNt8ZgAAAAAAAAAAAAAAAAAAAAAgE2i6CEAALBZzpnkMUk+XlXX2+rBAAAAAAAAAAAAAAAAAAAAAADTU/QQAADYbBdL8raqutdWDwQAAAAAAAAAAAAAAAAAAAAAmJaihwAAwFbYJ8lTquoPt3ogAAAAAAAAAAAAAAAAAAAAAMB09tnqAQAAALuNZyZ5zxL9K8lZkpw1yTmTXD7JFZKce4kY/1ZVn+zudy/xGAAAAAAAAAAAAAAAAAAAAABgD6HoIQAAsMM7u/tZGw1SVddM8ntJ7pFk/5Hueyd5dlVdprtP3Ghu2N119422egwAAAAAAAAAAAAAAAAAAAAAm2mvrR4AAACwvXT3+7v7fkkum+QNCzzkoknuNe2oAAAAAACAZVVV7+Lny1s9NgAAAAAAAAAA7O8AAGDPoeghAAAwie7+SpLfTPLiBbr/0cTDAQAAAAAAAAAAAAAAAAAAAAC2gKKHAADAZLr7pCR3TvKFka6XrapLb8KQAAAAAAAAAAAAAAAAAAAAAIBNpOghAAAwqe4+IclfLtD1RhMPBQAAAAAAAAAAAAAAAAAAAADYZIoeAgAAm+HVSY4b6XOZzRgIAAAAAAAAAAAAAAAAAAAAALB59tnqAQAAANtfd59QVe9KcticbhfarPFsF1VVSS6c5DxJzpbkgCRnTfKzJMfPfo5N8vUkR3X3SVs0VACATVVVByY5OMPnox2fk/bKKZ+Rjk/yvSRf7u5jt2iYAADAHqyqzpDk4kkOzClzjzMl+WlOmXf8KMlXk3y7u3uLhsoGVNVZMxzadFCG9fczJPlxku8k+XR3f38LhwcAAHC6VFX7JblEhn1TZ01yxgx7pH6Y5PNJvmIeDgAAAJvDPB3YHdjfAQCw51D0EAAA2CxHjbSfZcrkVbVPkisluVySS89+LpLhy6g7firDF1J/nOQbGb6M+vEkH0ry9u7+8ZRjHDMrcnijJLdKcuUkh2QY9yJOrqqjMtwwPDzJe5O8r7u/u/6RzldVZ0ly7STXzfDvcZEkF0hy5iT7ZyjaeFySbyf5YoZ/g3cneaeiRNOZ/bscluSGSS6f5GIZvqh95iQnZPiC9leSfCbJe5K8tru/vjWj3bmqOkeS62S4tna8xs+b4TmcKcPzOC7JNzNcW0ckeVeSd3f3iVswZLaBqjpbkl9LcoMM72kXzfDaOUuG6+3oDNfb+5K8JcPfkw1v2qiqSya5RYa/B1dMcu5Z3h2bRI7K8Hp9Z5JXdvdXN5pzXapqryRXyPBavVqG39mFc8p7zl4ZfnfHJPlyks9l+Lv1tu7+8qYP+DSq6sIZ/gbv+DxxySTnzCmfJ3YUuPhJhr9lRyX5bJKPJHlXd39+80e9mj3pb8Ps3+UOSa6V4XVx4SUe+/0kX8rwd+G9GV6vn7bBCgDYXcw+Q18zw2ezK2b4HHquDJszO8kPknwryQczfC57WXf/aA15D8gw77hmhs/AOz63nzXDZ97v5ZS1pjdk+Ly723yGqqrzJrlehnWoS2aYe5w7w+fZ/TJ8Zj8uw2f2L2RYA3xXkvd398lbMeYdqurMGeZLl8kpc48L5JR5x1mSnJzhORyT4dCTHZ9pP5DkPXvKASizNc+rJ/n1DOu3l8kwxzpbhut7x1rOFzJca/+T5L1bca1V1ZkyrM3+aobXxOUzzMMX8dOq2jF/2jHv+GB3HzfBUNmgqjo0yW9luC4vOtL3y0len+S53f2/049ucVV1rgzvgdfM8D5y4fziWvxJGe6H/DjDe/pnknw6ySczvKcfswljrAyvpcvnlPe7iyU5e4a/Nwdk+CLCjvfsb2R43/5Ekg9nWGv73tTjBAAAtt7sfuBdk9w8w1rC3nO6Hzc7oPeVSV7U3UfvIuZ+GdaPduWT3f3NFYe8stkaxE2S3DjDPoRL5hfX5X6Q4T7t+5O8Pcnr17GeVVUXSvKbSa6aYZ3mvDnlkIcdazSfzbCP69Xd/ZmN5pyKNVUAAIBpmaebpy/DPJ2p2N9hfwcAsGcqn7sBAOD0YbYwO6/4y+9397MmzP/oJA+Z0+X13f3ra855SJKbJjk0yfUzLKCu6mcZCkf9Z5KXdvfPNjzABVXVgUn+LMldskQBnwV9KMmLkry4u7+05tj/p6r2TnLrJHfKcFNz3xXCHJ/k1Ume1N3vXt/oflFVzZso/113P3wDsd+eoXDUzryju2+02bGr6gpJHpzkdlnu36UzfMn8H7r7rUs8bq2qat8kd0zyuxluIs+7Wb4rxyR5WZL/190fW9/oTh+mvK7n5PzLJI/OUKx2V76d5De6+0MjsR6e5G931d7dO81RVVfJ8Nq5bYYbc4v6QpJ/TvKU7v75Eo/b8V56lyT3y3DTfVE/z3Bz9qHd/dFlcq5TVV0uyT2T/E6GG7CrODzJs5I8s7t/uqahzVVV50lyswyfJw7NUFB1I45M8twkT+vub2ww1lynh78NVXW7JH+U4XnOe09Y1neSvDTJizP8rpZ6vQIA61dVd0vyzDldLrLuItlVdeUkr01yvpGuf9Pdj5wT5+AMBel2ZafrcrM1oT9Ocp8MG5cXdVyS583G9a0lHrcj7yFJHpph0+9+Szz0i0keleTZW/X5aXZi910zrBVcO6t9RvxWkhdmWCeYbL3s1GYHttwwp8w7rp7l5pqn9aMkr0nyH939ro2PcNdWfW3ONpbfO8l9k1xwybRfTfKEJP/e3ccv+dilVdWlkzwwwxxp0UNoFvGzDAckvDjJK7r7B2uMvTZz1ku/0t0Hb+ZYljFyT+TZ3X23nTzmt5L8TYYvjKziI0n+orvfvOLjN2z22rpLhvWX62U4XGIVJ2dYh3ljkud39+fWM8Kkqi6VoXjooRne+865gXA/z/DloWcmeZ5CogAAsP3MDgR8dJLbZLU5znFJnpbkUd39/dPEPjgrrNuN2cB6ySUyrEHcOcOXmRf1jST/luSfu/uEJR63I++tkzwow0Gjy6ynvSvJw7r7HcvmXGBMB8eaKgAAwG7HPH0h5umnPM48fRPZ37EU+zsWYH8HADCVVT/4AAAALOssI+3HrCNJVV22qh5ZVZ/LsAD9TxlO69lIwcNk+JLxTZI8P8kXquousxNqJlVVd85w+s7Dsv6Ch8lw4tdjk3yxqi6z7uA1uFuG5/CSDIUPVyl4mAw3CW+f5F1V9fZZQSZWVFVnr6qnJvlohpuwy/67VIbXxFuq6uVVddC6xzg3edUZqurPMtw4fWaGmyirFDxMhlOl7p7kiNlzmeK1xhpU1V5V9W9J/iHzb+AfmeQ6YwUPVxzD2avq6RmKxt4hyxehuFiSJyf5wDLvu1V14wynsD0jyxU8TIY1wN9I8qGq+vtZ8cRNU1VXqKpXJvl4kj/N6gUPk+QaGX5/X6qqe071t3j273z3qnpTho0n/5XhfWKjBQ+T5BJJHp7ky1X1hNmpfLuFPelvQ1VdpqrelqEYzY2y3oKHSXKeDMUU/yfD5iwA4HSmqn41yTsyv+DhSRk27e6y4OEG8t81w0nij8pym36T4YTnP0jy2ar6gyVynquqnp/hZOXfznKbfpPhtOxnJHnnbLPzpqmqM1fVIzIUw3tilt/4fWrnTXL/JJ+rqqdP9Zl9Nsc9tKqekqHQ4lsybLi+TjZW8DAZCvP9boZ/i/dU1XU2GG+tZmueX8ywLrlswcMk+ZUkj0vyqdlrdRJVdaaq+vsMc6R7Zr0FD5Ph3/nmGV43n15zbJYwe/97aYbi96tuiE+SKyd5U1W9sKrOtp7RLaaqzlFVj0tyVIb3wRtkY/vC9s5QPPbhGf6evK2q7lhVK8WsqoOr6i+r6ogM9wuemOS3srEN8cnwHK+d5KlJvlJV96+qjb6HAgAAu4Gq2ruqHpbhPvlts/oc58wZ7lN/qqpus6bhrVVV7VdVj8mwPvCHWa6QQpKcP8nfJ/lEVV13ibxXqqoPJHl5kutm+fW06yd5e1U9raqWHfPaWVMFAACYjnn6UszTY57ONOzv2Cn7OwCAPY6ihwAAwGY590j79zaaoKr+KsMNtL/OUFBoKhdK8uwkb6iqZW+8LGT2Re1XJnlOhqI7m2GtRbCq6rJJ3p2hIN3F1xk7w+lAH66qv5mq4NV2VlVXyVB87A+yniJRt85QwO2Ka4g1avZF/SOSPD7zi08sHTrDc/lkVd1rjXFZg6o6U4Ybk3880vV9GQoefnGCMVw+Q6GDu68h3FWSvKeqDh3JudeswMKbk1xqgzn3TvJXSV5WVcvewF/abMPHP2XYNHDLrLco3XkznPL59qpa5/tAquoiSb6d5OnZWEHVMWdIct8M7zk3nyjHwvakvw1VdfcMfwdutO7Yu7DPJuUBAHYTVXWXJK/N/EM0jk3ym6ucID+Se9+qel6SZyU5xwbDHZDkqVX1+LFNjFV1owzznTtk458Hr5vk/TWcmj65qrpFho3mD8twsMG67JNh/veZqrrlGuPu8IYMRbbvlY1vCp3n2kneXVX/XFVnnDDPqKrav6penGHNcx3P+cIZ1mgfuIZYv6CqLpbhUJ2/SrIZvzfzji0yWwd4T4YN2utyuyQfqeHE+8lV1T2TfC7JA7PxA6B25UZJnpcVin9W1e9lOLzmH7KxLx2MOWeSf03y3qqa8h4RAAAwsao6IMkbkzwi65sznyfDvfKHrCneWlTVBZO8P8lfZOP3pi+e5K1VdfsF8t53lvdqG8yZDIdFvLWqNrqeuRJrqgAAANMyT1+Zebp5Omtkf8fCbhT7OwCA3ZyihwAAwGa5+kj7EWvIsfRi7AbdLMnh617Yrqp9k7wuQ4GoPVJV3SHJ4UmuM2GafZL8XZJXzIqhsYCq+vUk70pywTWHvlCSt0x9o6eq/izJO5JcdsI0Z07ylKr691VPttqVqrpRVfXIz93WmXM7qKpzJnlLhiJq87w6yU26e8OFdHcyhutkKOT6K2sMe/Ykr62qq+4i594ZCkH8Vda7jnfLJM+Zsmjs7GTD/03yoExbuOEGST5UVevY4LHDvtmcghY7nCfDdXCfTcz5C/akvw1V9UcZClJuabEWAGD7mm2mfnaGItW78u0kN+ruN6w59/5J3pTkjuuMm+TPkjxmTt5bZdicfv415jxPkjdW1UXXGPMXVNXeVfWPGeaCF5oqT4ZNlq+oqoeuOe5mrmVWkgckeX1VnWUT854ygGFu/84kv73m0HsleVxVPWBdAWebpN+djR8+wG6uqi6QYT48xQbqiyR515QHxVTVAbNCok/LtMVTN2qqjfq7ctUM926uu8l5AQCANaiqA5O8PclNJkrx6Kp6+ESxl1JVl8xwsOM65477Jnne7KCQXeV9TJInzPquy7WSvGq2723TWFMFAACYlnn6hpmnr4d5+umc/R2bxv4OAGBTKHoIAABMrqouneRiI90+sBljmcCFkryjqtZZBOuJGYo47ZGq6s+TPD9D4bjNcMsMN+MUPhxRVTdM8tIk+0+U4twZigCsPX4NnpTk8Zm2gNqp3TvJM9dd+JDlnKp43lgR1acmuU13Hz/BGC6doYjG2dYdO8mZkryyqs69k7b/SvJ7E+RMkt/JcDrc2lXV5TOcNHmVKeLvxPmSvHnNhQ83WyV5YlXde9MT70F/G6rq+kmetPEhAQD8slkBvScnefRI1yOTXKe7P7Tu/ElekOnWhB5cVb+7k7y3SPLiTFNU+jxJXlJV8wpIrqSqzpjkJUkevO7Yu0qZ5FFV9ahNyjeVG2cour6pRcRnhRZfl2Gj6lQeV1U33WiQ2Yb/VyY578aHxO5sNk99VZILTJjm3BmKja77kIFU1UGZppDodnH2JG+qqs1anwIAANagqvbLMFe78sSp/nZna2WbaTave0OmmZfulaGgwi8d6FBV/5DkLybImSTXTfLPE8X+JdZUAQAApmWevjbm6ethnn46ZX/Htnf22N8BAKc7m1UkAAAAOH172Ej757r745sykuTYJB9P8pkkP0jyw9nPiRmKWZ0tya8kuXqSi2f4QvOYg5K8uqqu0d0nbGRwVXWjJH+wQNeTkrw3w0lUn8jwhfvvJTkuyU+TnCXJARmezwUynDR2xSSHJLn8RsY4T1X9ReacnrUT30/y7iRfSnL07P/vn+FmwYUyfBH7oAXi3DTJs5LcfoncpytVdbEMX9jebxddTk5yeJKPJfnO7OeMGW7SXSLD7/iABVJdOkOBiD/d2Ih/yZMzFCFc1DczXFtfy3Bd/SDDiVPnyXAK140z3BgZc5ck307y50vkZk1mN61em/FCA3/b3Y+YaAxnzlDw8By76HJiknck+WySb2V4Lzsww5hvkORKC6S5QIb3znucKu9DMv+Ew69lODXzGxlerydleL+8eJJfy2IFGh9VVS/v7i8s0HchVXW5JG9Lcq4FH/KzJB9O8pEMr9XvZ/idnmf2c60Mv8Oxv8dnz3CT+ard/dXlR760n2f42/uxDO8RP0xyTIbPGfvPxnOuDH93D8lQ3HIRT6yqT3b3u9Y62l3Yk/42VNU+SZ6RxQ7y+UKGjUfvT/L5JF/J8BnpuCRnyCmf+c6R5HIZPiNdIck1Ml3xRwBgNzY7SOH5SW410vX9SW7R3d+bYBiPSvKbc9o/neQ9GeYd387wueagJJdNcpMs9pnz/1XV67v7B0ky20z93FmsnTk+w7zj87Ocx2T4DHX+DPPqX9qMvRNXzrA5e23FAmefDV+a5BZLPOzLGQrqfzPDvOOHGT637/hse6MsdoDHQ6vqG9395CVyb8R3knw0yRdzyjrmDzN8Lt7xufbSGYoJnm/BmDdI8oQst86yEZXkeRk+b+/KVzNs7v1Whud8fIb1wfMlOTSLndJeSZ5VVZfq7uM2MN6HZJgfjPlxkv9J8tYMawJfyPAaOS7DHH3H2uzZMhwItGPecbUM655svb/Prg9s+HGSF2aYW340w3vHjnX3i2R4zd0yyW8k2Xskz/kzHNpzre4+cQ3jTlWdJ8m7sthrY4efZJjjfzrDa+7HGeb9B85+zpPhPfuyGX9OU/hpkk/Ofo7OKe93P8kpr6fzZ3gNXWbBMe6f4Xd/yER/uwEAgPX7f0mut2Dfj2U4FONdGebmP8gwzzkgw1z8mhnmbjfJzu83/0eGed1W2DvD+tZFdtF+coZ9YTvuR38vw/PacQ/9mhm/b3rWDHttbrLjP1TVHZP85ZzHfCfDesdRs7w/ybAGeXCSwzKs14z546p64Sbd87amCgAAMC3z9IF5+mLM05mK/R32dwAA24yihwAAwKRmJyaNnbj1nxMO4YdJXp/hRJ8PJvl8d/ciD6yqAzMUnLpHdr04vsMVMxR3/OvVh5okGSvadXKGG13/2N1Hzem3Y8H4axmKIr5xR8Ps1KFbZriZdNOsaW5YVb+d5B8W6HpskqdlKFz0yXn/HlVVGX73D8pQ0HBe0avbVdXh3b1pJ5HtQc6Y4SbOzoqgHZnh5ttruvvoXQWYnUx26ySPza5v5u7wJ1X15O7+3GrD/aXcD8piX8T/XpInJXlud39+JObeGU6u+6sMBeLmeXBVvb+7X7rIeFmPqrpZhs0DZ5nT7aQk9+ruZ044lH/JUEjwtD6T5O+SvLa7f7yrB1fVRTIUNLzdSJ7fr6p/6+4Pzwrg7uym+ElJnprkqd390Tk5z5DkDkkel+GG5q7sm+EG8B1GxraQqjpXktdksYKHb86wEeZt3X38SNzzJrlThsITuyo+mVnel89uMv9ssVEv7OdJ3pfk5RkKcXxibNw7zN5vDk1yzyS3yfxTJPdJ8vSqulJ3/2RjQx61p/1tuEt2/lo8tfcleVh3v2VOn5My3FD/1uz/v3dHw6zY0U0zfEa6Vea/fgCAbaKqzpmh0Pm1R7q+OskdFv0cuKTrJfn9nfz3n2Q4cf1Z84qVz06zvmeSR2Z+UepzJXl4kvtX1X5JXraL/odn+Dz45u7+6Zy8V0zyrxk+787z51X1lO7+7ki/Rf1rFit4+NVZ3xePrKOlqvbNsIH6bzJsRp+bv6o+3N3vW2AMy/pqhnXM1yf5SHd/c9EHVtVFM1xHd0sydur4H1bVS0Y+O6/LA7LzTe0/SfLEJM/p7k/OC1BVl80wf731SK4LZDi44m+XH2ZSVWdL8sCRbkfPxvIfI+8HR89+kuSIDGscO/IcklPmHVddZaxs2FUyFME/rROT/FOSx+5iveeHGf49j8gwf7/IrP9tR/JdOcP76oYPVpm9f78yi22I/3mSVyR5ZpI3LbIpfzY3vmqG19vtMl2Rzp9kWB96RYaiwp/t7pMXeeDskJLfynDv5oYj3S+Q4W/BnVYdKAAAsDmq6tZJ7rVA108k+bM56xo7Dts7PMOhc5fKMHc77frEWTPsMdkKf5Fhz8ppfT3DPoSXdff3d/Xgqjoow/rHH2Z+UYUbV9VtuvvlVXXp7HqP3gsy3L8/vLt/vouceyX59Vm/i87JWRl+39ea02cdrKkCAABMyDw9iXn6MszTmYr9HbuOb38HALDHGqsaDwAAsLKqulWGk7rm+WqSf19z6pMz3OQ5LMm5u/uO3f387j5y0YKHSdLdP+juJ3f3VZPcOUNBtXn+vKp+ZdVBV9Ulk1x/TpcfJzmsu+839kXtebr7qNnzunmGm1uPy3Ba1cpmNx6fnflFCTvD4vOFuvsB3f2JsX+PHnyou++YYSH+MyNDedTs98gvunZ++QvUP01y3ySX7e7nzCtqlSTd/bPufnGGk6BeNJJvn4x/QXwhVXXDDMW05jkpyUOT/Ep3/91YwcMk6e6Tu/ud3X1YhqIGY0UE/n1WCINNUFV3zVA8b17Bw+OS3HLigodJ8gen+f8/y1Cw4Qrd/YJ5BQ+TpLu/1N23T3LXDDcid6WSPGBWaONp+eV1u/ckuVJ3/8m8goeznD/r7v/KcCraR+b1TfI7VXXwSJ9RsyK1z89wcuQ8H05y5e6+WXe/dpGCMd39re5+XIa/WU8d6X6VDMUR1+XjGTagnK+7r9vdj+vuw5cpdDN7v3lLd98hyRUyFE2c5xJJ7r/6kBe2p/1tuOdI+5OSXG8jRVu6+yfd/eruvleGm/6/n+FUVgBgm5ptZnxPxgsePjXJbSYqeJgMm/pOOwd4TZJLdffD5m36TZLuPr67n5Dk8km+NJZrVtjtoRk+x53ajzIUm77W7HPRLjf9zvJ+rLtvnGGD9zxnzWKHGYyqqjsn+ZORbsdl2Hh/se7+l0XW0br7hO5+XXdfK8PBG/PmemdI8oyqmlfQfBk/zLA+d7XuvnB333c2loULHiZJd3+xux+WoVj4ozPMX+d5wmwT/NTus5P/9pIkl+zuvxgreJgk3f2p7r5Nhn+bsed139lm+FXcPvPXIj6d4d/p8Rt5P+juI7r7kd19tSTXyLCWve7i/cx3hfzy++43k1y/u/96bL1nh9m6z29nWD86aaT7A6vqSssP9Zf8axb7Qsq7Mqwl3ba7X7PIhvjk/+bG7+7uByW5cIYv97ww89e1FtUZirreJsm5uvtW3f3M2Wt8oQ3xszEe193/1d03ynAf6CsjD/m9qpr6SzwAAMAGzObyT1ig69MyzM0XvifY3Z/t7ltmWJ867dztiouPcq1Ouw+hMxymeMnuflrPKaSQJN397e7+4wyHfM5dw8swH60kT0ly2jWTTye59mxv3ft6F4UUZjl/3t2vybAG+YaRnNesqnl74NbBmioAAMBEzNPN01dgns5U7O/YBfs7AIA9maKHAADA2lXVZarqRRlOcNl3TtdOcu/uPnZNqX+c4ZSqi81u8ryxu9fyZdHu/u8MC+Xziu6dIclfbiDNr4+033sjhXx2pru/1t0PzlDY54urxJh9OfoZ+eUbbqd2dIbiZH/W3ceskqe7P5LkOknePqfbflns5urp3dFJbtrdT+rusZs1v2B20+93M7y+57nT7ESmlc1ulj8j89cvvprhZtWju/snq+Tp7v9Jcs0Mxc125dxJ/n6V+Cynqh6a5FkZ3lN35TtJbtTdr9+UQZ3i+CS3mhXNWPa185wMxRLn+Z0k/5KhQMWpvSbDa/ZTS+Y8OsPmiXk3BvfKsMlgo/4wyU1H+jwxyXW6+4hVEnT3D7v7D5M8KPNvxD60qi68So4dqZK8NUOh4St291O7+zsbiHdK4O7PJblRhn/neR64gSIdq9pt/zZU1Tky/4b/m5Lcb5kb6GO6+8TuflZ3XynJP64rLgCw+6iqqyR5b5KxwxP+trv/cJ2fNRbwnxnmHl9b5kGz/jdN8oM53c6c5B/yy6dRfzfJobNNhgsfGjLL+/AkTx7p9gezTdsrq6rzZVj7m+fjSa4623S+1OfaHbr7RRk2gc77/V8m43O8MV/NML+5UHc/uLs/tMF4Sf6vgONDk9w4ybx118sk+e115FzSY5Pcrlc41GX2b3PHkW4HZiheuIp567PHJbl1d49trl9Kd3+ghwNf1rFZmtV9O8kNu/vwVR7c3f+Z4dqct16xV5LHrxJ/h6r6tQxrMGMeleH5fGIj+Xrwnh4OcrhMhjW7VTbHnzB77BW6+9e7+xW9pkLC3f3GDF98eddI179ZRz4AAGAyD8ywd2me/9fd9+ruE1ZJ0N1PyfjcbSucnOQe3f2QZedKs31kY2sl180wH73Baf774Rn227xvyZw/SXLbJB8c6XqvZeKugTVVAACA9TFPN0/fKPN0pmJ/x07Y3wEA7GkUPQQAAFZSgzNX1Xmr6nJVdbuqelRVfSTJpzIUbpqnk9xrnQWruvvvu/tPu3vs1JdV438rw5eFvzyn2502UKToGnPaPtXdz1sx7qjuPnYDi9B3z1CMcFeOy1A46jUrxv8/3f2DDF8+nlec7teqat54Tu+Oz1As7n9XDTAr9HCvzL+RuH+S31g1x8xfJbnonPZvZ3guS93Y3ZlT3Rz95pxud99gITXmqKq9q+rfM9y0m+fIDCcYjt2YX7fOUNxgI3+3npjkI3Paz5jkj07z396S5DYbKOr53SQPHum2oSIXs4J0jxnp9k/dfb9VN7acWnf/c5K/ndPljBlOblw1/me7+6azm6prN7uh/IAMJ3TuyrkybEDZLLv734arJZm3SeRhy25OWca6il4CALuP2abCdyQ5aE63kzJsYn7E5ozq//xXd/9BzzmtfZ7u/mKSsTH/UYbPzTvs+Dz44VVyzvxFhnn6rlwo89e+FvFPGQra7cpnMjyPz24wT7r740lulmFda1ceVFVn3UCO23X3P/eCJ46vEP/dGT5/zzsUZpHNtev0yO7+y418fu/ulyZ57ki3sbXpXZl3jT6nh0L2kzDv2FInJ/nt7j5yI0G6+yVJHj7S7cZVdd1V4lfVPhnWlsbcr7vXPk/u7s919+93949WeOwzZ4/95DrHdKr4x2Z4v5tXPPYwa7sAALB7mu2zuv9It7dk4wdQ7Ji7/d1G46zZH3b3M1d9cHe/IskrR7r96Wn+/yeT3KS7v79izuOT/EmGfRS78ptVdcY57etkTRUAAGBNzNPN09fAPJ2p2N+xAPs7AIA9gaKHAADADs+sql70J8NpL8dmKBD2iSQvzFBg6JAFcn0vye1mp+PsUbr7m0nuO6fLWZPcesXw84q7vXrFmJOa3fR62JwuneT23f2BdeWcFf76nQzX3648cF35tqE/nX1xf0NmhdTGitP92qrxq+rcmX+z/CdJfqO7v7RqjtOafbn8DhluhO3MGZLcb135OEVVnSnJS5Pce6Tr4UmuO7uRvdn+pbvfvJEAsxv3894zT+sHSe7W3SdtMO+LM7/Y4qU3eFPwL5KcbU77C2d91unRSeYVJbxrVZ1rzTnX7QFJvjqn/c6bNZDs/n8b5hbAXfWkRgDg9Kmq7prkNUnOMqfbcRlOG3/G5ozq/3w5wybkjfq3zC/qf1oP7u5PbSThbEPiP4x0O2zV+FV1uQynbO/KdzIcunH0qjlOq7s/k/kn3Z8zyd3WlW8K3f3OJE+Y0+XQqrrgJg3nXVnflxUelOTEOe2HLrtZf7Y2cb45XV61TDz2KE+ZFQldh39I8umRPqt++eceSS4x0uffu3uRjfPbzqyA7D2y67XdSvJ7mzciAABgCXfKsM6yKyck+aNVvyi/E49JsuFDM9bkZd399DXEWeZQwJ8ludNsPW9ls3u084o4nC3JtTeSY0FfjjVVAACAdTJP3zjz9I0zT2dn7O/YBuzvAAASRQ8BAIDN9aMMX7K93OxUnD1Sd78mydvmdFn1JsaBc9rmFUXaSndK8itz2p/d3a9dd9Lu/mySf53T5TdnRfP4RW/s7qetMd5/JZlXjO16G4h938wvQPHY7p53stNKZsUAnj+ny52r6gzrznt6NitM9z9JbjXS9TVJDp0VVdtsX85ymw/meVOSYxbs++fd/fU15X3hSPt1VglaVQdk/saEozNsbFn36XM/z/A+sasNM2fM8DdqtzU7VXNeEcwbzIpuTG1P+NuwJ35GAgB2Q1X10CTPSrLPnG7fyXDy9+s2ZVC/6N6zTX0b0t0/S/KyBbu/u7ufvNGcMy/Krj+jJyvOO2b+IvP3N/x5d39lA/F3qrufl2TeZtl7rDvnBP4uw7rszlSSm23CGE5Mcufu3tVm1aV097eSvH5Ol/2SXHXJsGcfaTf32J6OSfI36wo2O7jiz0a63XLZgxqqqjJ+0M+RGQqCnm5190eTPGdOF19AAQCA3dPYfd3Hdffn15Wsu0/M/ANvN8uPk/zhOgJ19yeTfHLB7v/Y3UesI2+SF4y0b2Q9cFHWVAEAANbLPH2DzNPN05nEMbG/Y9uwvwMAUPQQAACY2jEZTli6Q5Lzd/f9u/s7WzuktXjxnLYbT5Bv3pfxt9K95rQdm+SvJsz9hCQ/2UXbGZL81oS591SPWWewWfG5t8zpcolVCnZV1d5J7j6ny1FJ/mnZuEt4bJJdFWg7d6Z5jZ8uVdVFkvxvkmuNdH1aklvPisRthSd190/XEWh2A/41C3T9bobicesydtP/kBXj/m6SM89pf3h3/2DF2HN195GZ/7xuP0XeNXtFhhM6d2bfJNfdhDHsEX8b5thdPyMBALuRqtq7qv4jyaNGun4+yXW6+4ObMKzT+kR3v3GN8V6+YL/Hrythd38zyfvmdDlklbhVdWCS35nT5YOZvwlyo+Z9Zr5SVV1qwtwbNttM/oY5XTZjneP5ExSlnHdoRZJcac35zD22p8d39/fXGXD2Xv6OOV32SfLbS4a9SZJLjPT5/S1cO9udzLt3c61NOmACAABYUFWdL/MPTftZkn9Zd97ufnOSI9Ydd0nP7u7vrTHeIuuBJyZ54hpzvnYWc1cOWWOunbGmCgAAsEbm6ebpG2SezpTs79h+7O8AgNMxRQ8BAICpnT3JrZIcmuSaWzuUtXr9nLYLLHuSz8y8xfdDVog3qdkXuuf9m/737IbPJGZFlebdxLrZVLn3UEd099sniDvvhl4lWeWL/zdNcoE57U+a8gZPd38iyXvndFn52urut3d3jfw8a9X4e5KqumqG3/MlR7r+bXffq7tP3oRh7czxSZ6x5pgfXqDP07r7hHUlnBUInHdq4qpFOu46p+2HSZ6yYtxFPXVO29Wr6uwT59+Q7v5RkvfM6XLliYewp/xtmPcZ6ZJuaAMA88w+K7ws46exH56h4OEXph/VTj1pzfEWmXd8NcmrNjHveVb8jH67JPvNaX9cd+/q8IIN6+7XJvn6nC57whrUvLXMqecdyQRfesj8eUeSXHrJeGMbow9ZMh57hv/eori3WDLeHUfa39Pd/7tkzO3qf7LrL/KcIcnlN3EsAADAuJtmuH+4K69f95eZT2WqOeGi/m3N8RZZD3xJd397XQm7+9gkn5vTZerDQqypAgAArJd5+vqYp2+ceTqnZn/H9mN/BwCcjil6CAAAbIYLZvhi+Vur6gNVdZutHtAafC3Jz+e0X2GFmN+d03bbqjrvCjGnNLZw/4JNGMPb57TdcBPy70leNlHcj420X2iFmGPX1gtXiLmst89pc21tUFUdluF3fNCcbicluWd3P2JTBrVr7+zuH6w55qcX6PPKNeccy/srywarqnMnucacLi/v7nmnRa7D/2Y4NXRn9s7800Z3F1+Z0zb1jdo95W/DvM9IZ07y+0vGAwBOJ2aHUvxPkluOdH1tkkNnByxslVesM9hsHjO2Mfq1ExSY/9RI+9Jzj8xfJzguyatXiLmsead67wnrBPPmHZesqjNOmPuL3f3RdQft7q9kKLa/K0vNO2bz1x/N6fLHVWWPzfbyvu7+0kSxX5Jdb8xOkutV1d6LBKqqSvLrI92evOjAtrvZISLfmtPFpngAANi93GikfcqCB8/L/L1fU/pMd39mzTG3ah/CvPXAVdYCl/GKdQbbZmuqAAAAq7jRSLt5+uLM0zfIPJ1Tsb9jG7K/AwBO32zIBgAANtvVkrysql5RVefc6sGsanZT5Htzuhy8QtjD57QdkOQFVXWWFeJO5bA5bd9J8q5NGMM757Sds6pWKbi3XU11EtTYzd3zrBBz3rX1we7+8goxlzXv2rp8Ve2zCWPYlqrqbhkKUsx7Pzsuya26++mbMqj53jtBzC+MtP80yUcmyPv5OW3nXiHezTJ/ffElK8RcSncfn+RDc7pceeoxrMG8G7UHT5x7T/nbMO8zUpI8pqqus2RMAGCbq6qLZvi8c62Rrv+ZYf5x/PSj2qUvrfPk9lMZm3tMMd+ZN+9Ilpx7zIrxHTqny+s36d9u3jrBnj7v2CfJBSbMPeXp5J+d07bKmtS8ucc1k/zjCjHZfb18qsDdfUySt83pcrYkl1sw3BWSzDsQ6QdJXrxgrNOLrVxrAQAAljO2rjLvIIoN6e5vJvncVPFHbMU+hKnyzlsPPPuE+2usqQIAAKyfefr6mKevh3k6if0d25n9HQBwOuUL+gAAwA7PTPKeJR9TSc6aYQH37Ekum+QqSc6xwGNvleTqVXWL7p6isNOoqjooyfkz3HA4W5J9k5wxixeIn3dSz/lWGNJbkzxqTvsNk3ykqh6Q5DXd3SvkWIvZ6UNXn9PlI929GaesfWWk/QpJvrYJ49jdnZzk/RPF/uFI+9mWCVZV50hy8Tld5hU3W6d519a+SS6Z8RPcOI2qeliSR4x0+06SW3T3BzZhSIt43wQxfzzS/uHu/tkm513qtTpzzZH2zXy97qqQzRWmTl5V+ye5cIbPE+dMcqYMnycWXXu9zJy2VT5PLGqP+dvQ3V+vqs8mudQuupw1yf9U1T8l+afu/tEy8QGA7aeqrprktUkOGun68O7+u00Y0pgp5h3J+NxjK+Y7y849Lp/kzHPad4d1gotU1Zm7+7ipklfVXkkulOGaPneGgwT2TXKGDGu0Y8YOnzlfkqlORF92zXkZ8+Yeq8xz35rkpnPaH1hVhyR5QHd/bIX4k6mqG2RYr9pMZ6mqe04U+3PdPa/Y6Dp8eBPi/9qc9ssnWeQ6utpI+3u7+8SFR7UHmK0RXzDD+92BGd7v9s3i927OPqdtyrUWAABgCbP1jkvP6fLN7v7OxMP46MgYprL2dbnu/llVnZBh/rQz3+juKfZQzVsPrAwH/R49QV5rqgAAAGtknr5e5ulrs+3m6fZ3rMT+jt2U/R0AwKoUPQQAAHZ4Z3c/ax2BquqySe6e5C6Zf4LR+ZO8qapu1N2fXEfuOWM6W4YF6OsluUaGAkMHTJhy7IvEO/P+JJ/M/BOALp7kVUmOrKqnJ3lZdx+5Qq6Numjm36jZlGJw3f3Tqjo+yf676HLBzRjHHuDoCb94P3ZDb1c3aXdl7HTAzSo0+P2R9gtG0cOFVdXeSZ6c5F4jXT+f5LDuXuRkw80yxWaCY0faj5og51jeZV+ryfzX69HdPe/UtXWa93pd69+B2YaeayW5SYaij4dk+DyzSJGRVazyeWJRe9LfhiR5RpLHjsT86yR/VlXPS/LCJO/o7pNWyAUA7MGq6rAkL8n8QnknJbl3dz99c0Y1aqoDG7Zi7jGWczuuE1SSCyT53LqSVdWFkvx6kmtn2BB78az2OXpRU849vjph7Hlzj1V+X/+d5O8yFLLflZsk+WhVvTPJs5K8uru/t0Kudbt7krtucs5zJnnaRLGfnWTqTfEf3eL48+4DnNohI+1TfXlkU1TVmTIUG71BhrWWy2Wxg7VWNeX7HQAAsJzzZThQbleO2IQxHJHk9puQ57SmXA/c1ZrIVuxDSKZb07KmCgAAsF7m6etnnr5x23Gebn/H8uzv2A3Y3wEArJOihwAAwNp196eSPKiqHp7kcUn+cE73cyV5S1Vdsbu/u85xVFUluXmSeyc5LMkZ1hl/xLybfTvV3V1Vj0zyggW6XyLJY5I8pqo+neStSd6eoXjlWn+PuzB2etp5JzwF6rR+NqftAps0ht3dDyaMPXZDb96XxHdm7Nq6+CZdW3uPtLu2FlRV+2d4X/vNka6HJ7nFJr2HLWOK189YobmpXrPz8i77Wk3mv15/vIl/Bw6e07aW12pVXSzD54m7JDnPOmIuaOnPE0vYk/42JEPh1Adn+Ow4z5mT/MHs5wdVteMz0tuTfKq7e4XcAMAeoqp+P8lTM/8++HFJbtfdr9ucUS1kK+YAJ3T3TzY5Z7L+dYIrV9VmfEY/aKR9w0UPZ/Pn38vwWfbqG4m1gu0491h63tHdR1XVMzLM/8bcYPZzclW9L8nbMsw73tvdxy+bm033rU1YgxrbFL/oQQ0XG2l//4JxditVdf0Mr7XbZNr3oNPazFwAAMB85x9p34xDYNd2iMWSplwP3NWXgbdiDTJZ7d7wIqypAgAArJd5+vqZp2+ceTr2d2wx+zsAgCkoeggAAEymu49Ncu+qemeS/05Su+h63iT/luR268pdVYdmKLh4lXXFXNJKJzd19wur6rZJfmeJh11m9nOfJKmqI5O8N8m7k7y7uz+9ylhGXGik/Y6zn612wFYPYDdx9FSBZ8U653WZ27gTY9fWfZeMNxXX1mIOTPI/GU7xmue1GYqO7I5FAdZ+A36B181UN/3XVuytqvbL/OJzF850pwEuY0Ov1VnxlEcnuVvGi6FOYarTNJM9629DuvvYqrp7klcu8fgDk/z27CcZiiC+L8l7MnxOev9Em10AgK3xVxmKxc3z3SS/0d0f2ITxLGMr5gBbNe9Y9zrBw5eMN5WV5x6zg1v+IMNzOd+6BrSkPXLukfnX29LzjpmHJLlJhkNnFrF3kuvOfv46yUlV9dGcMu94V3d/c8WxMJ1vbEKOsX/3RV/vY++DW/XFn5VU1ZWT/HOSQ7doCFO+3wEAAMsZO8jih5swhh9tQo6dsR64cX6HAAAA62Wevn6npzmm3yFTsb9ji9jfAQBMaa+tHgAAALD9dffzMnxZdJ7fqarf3GiuqjpDVT0xyVuzdQUPk40VRvr9JO/bwOMvkeQuSZ6a5FNV9fWqenpV/UZVretEqbFT3HYXTvUZnLDVA1iCa2t7uWLGCx4+Pcmtd9OCh+nurXj97Amv2W3/Wq2qWyT5ZJJ7ZGsKHibTrt/uCdfZL+juVyd5cFYv4HlgkpsneWSStyU5uqreWFV/XFXnXdMwAYCtM1bw8AtJrrMbFjxMtuaz2Z7yeXBbzz1mn0PfmuQp2bqCh8m0c5495VpLknT3MUluleTbK4bYJ8lVMxzc8cIk36iqj1fVY6vq6usZJWuwGV+W+XHmz1/PuWCcg0baj1kwzpaqwUOTvD9btyE+2bo1HgAA4JftP9K+GXO3rSqmYD1w4/wOAQAA1ss8ffvnnJLfIVOxv2OT2d8BAGwGRQ8BAIDN8k8ZCgfN8+cbSVBV+yd5fZL7ZA8+Oam7j0tyoyTPWlPI8ye5e5LXJPl6VT2+qi68wZhn3fiwNoVTffY8rq3Tl3d09z27+6StHghL29av1ar6oySvTHKu9Q6Hjeruf07yO0mOXUO4/ZLcLMm/JTmqql5TVYetIS4AsPs5KcnNu/vzWz0QlrZt5x5VdYkMB59s5eZQdqK7P53kaknWVST18hnWvg+vqs9W1QOrak+5treryTfFd/fPkxw3p8t+C4aa9+WiztZ98WdhVbV3kuckeVSSM2zxcAAAgN3H2HrKjzdhDLv9nAoAAAA2iXk6sDuyv2MT2d8BAGwWRQ8BAIBNMVsAfsRIt+tV1ZVWiV9VeyV5UZKbrPL43U13n9Ddv5/kN5J8fI2hz5Xkz5J8vqqeUlXnWTHOmdY4pintscUvT8dcW6cvN6yqv9nqQbCSPeW1urSqukOGInjWTndT3f3SJP+fvTsPm+8e7wf+vpNIJBJBInaJJdbYxS6CIvbaSi2torbS0lYXvypFtdXW2qqdFkVRKtQusRaxi32NWGLPvif3748zIU3znZnneebMs3xfr+ua6yKfz9yfe77PzJw5Z868zzUzfKF+9oLK7pjhc9c7quqTVXWrBdUFADaGnZK8tqoutt6NsGKbZd9jRccJquoSSd6dZK0XJmEk3f29JDdP8vtJfrLA0ldJ8g9JvltVT6iqnRdYm/kt4wc5yfQT1ucNS532Pnj85LuXje6fkjxwvZsAAAA2nV7CGpthnwoAAAA2AvvpwHpwfsdyOb8DAFiKnda7AQAAYLvy1iSnZfrB3vsk+dwqav9JhqCaeXSGIMFPJvlikm8n+WGSHyc5cXI7I8mZ3b3NL+aq6jsZ+YfJ3f3fVfXOJPdM8pAkt88QyrNWOyV5eJJ7VtVDu/utK7y/q/UwFs+treXHSS6SZNoP+P+qqnbp7v+3nJZYkC35Wq2qKyV5aeYPLPlRko8l+WySbyU5evLfjs3wBfvpGT5PnDVlzackefJqe95edfd3k/x2VT0zySOS3D/JXgsqf4Mkh1fVC5I8vrtPX1BdAGBc30ty2SnjN0hyWFX9WncvMsCMcW3JfY8kr0yy35xzT0tyRJJPJ/lahmOZxyT5aYarjJ+UYb/jzG0VqKr9JvdjBbr7jCTPr6pXJnlwkt9Jcr0Flb9IkmcmeWBV3a+7v7yWYt394Aw9LlxVbesY+VHdvd8Yay7Bst5bpq2zGU5mX7PJxSUeuYK7fC3JJ5IcmeFYyw8yHGs5Ib/67uaMGd/dHJ7ExQwAAGDjO23G+IWX0MOeS1gDAAAANgP76bAEzu9YMed3LInzOwCAZRJ6CAAALE13n1pVH09y0JRpN19p3aq6bJKnzDH1a0mel+QN3f3jla5zfksvoMZMkyv5vDHJG6vq0knumuS2SQ5OcvE1lt87yVuq6pHd/eIV3G/WF5qwWp5bW8uXk/xdkv9McsEp855YVRfs7j9aTlsswFZ9rT43yYVmzDk5ycuSvLK7P72ANZfyeWKr6u4vJvn9qvrjDOHQt0tymyTXzNr/bR+d5GpVdUfBhwCwKfx6khcnuf6UOdfOEG582+4+ZildsVZbbt+jqu6d5JA5pr4/yQuSvKO7T17rsmu8/3atu09I8vwMAYjXSnKXDPsdN0uy2xrLXzvJ/0zelz61xlrMbxk/yJm1zqlz1jglye7bql9VO0y+Q9hwqupCGY61zPL9DK+x104ucrDmpRdQAwAAGN+s4x3CFAAAAGB57KcDG5HzO5bA+R0AwLIJPQQAAJbt05keenijqtqxu89aQc0/TbLLjDl/n+TPV1h3lqV/odbdP0jyoiQvqqrKEOhzUJJbTm6XWUXZSvLPVfWd7n73nPeZ9YXm73b3S1fRC8x6bt2uu9+7lE5YiO5+R1XdJclbMz0I4A+rapckj512JS82jFmv1Y909y2W0smCVNUNktx5xrRPJ7lndx+1wKWdoLMAk1DCt01uqaqLZ/iMdFCSWyS5TpIdV1H6NklemuS3FtMpADCin2W4SMQ7k9x4yrxrJPlAVd2mu7+/lM5Yi1n7Hvt39zeW0sni/OWM8ZOSPKy7X7fANe13LEh3fyHJF5L8TVXtnORG+dXx2Ztmdf/WeyY5tKpu1N3fW1izTLPH2AtMnh/Tvrc4ac5SJ2fbJ8VXhhPvj52/s6V6ZJJ9Zsx5dZKHd/cpC1zXex4AAGwOsy5Yu4wfNC/rR9MAAACw0dlPBzYi53csh/M7AICl2mG9GwAAALY7P5sxvluSvectNgnIesCMaY/v7j9ZZOBhVe2Qdf5CrQdHdvcLuvs3u/uySfZL8jtJXpfk5ysot1OSV0+uzDOPWX/HC65gbTg3z60tqLvfl+SOSU6cMfX3MoS6Oma18W3F1+pDZ4x/LMnNFxx4mCQXXXA9knT3T7r7Td39B919gyQXyfA+9A9JvrjCcg+qqt9YdI8AwOJ197FJbpfkwzOmXiXJB6tq39GbYq221L5HVd0oybWmTDkjyW0WHHiY2O8YRXef3t0f7u5ndPcdk1wsyXWT/GGSdyc5bQXlLpXkxYvvkm1YxknTs74/mPWjoXMcM2P8InPWWQ+zjrU8t7sftOAT4hPveQAAsFn8cMb4/kvoYRlrAAAAwGZgPx3YiJzfsRzO7wAAlsoPyAEAgGWb9UPlZPhx6LwOyvQDnO/u7uesoN68LprhKjsbSncf1d2v7O7fTHKJJHfIEIB45hx3v3iSP5hzqe/OGJ91dR/YFs+tLaq7P5ghfOS4GVN/N8krBB9ueMckOX3K+GZ8rd5tytjJSR7U3aeOsO5KPvewSt19Yne/s7uf0N0HZAg6empmn6R1jqd4XwKAzaG7T0hySJL3z5h6xQzBh1cavyvWYKsdJ7j7jPGndPcnRljXfscSdPfZ3f257n52d98hw/HW30ryoTlL3LGqbjZeh5zLFZewj3eVGePz7o8evcZ11kVV7Z/k6lOmHJnkT0da3nseAABsDj9MMu371+suoYdlrAEAAACbgf10YCNyfsfInN8BAKwHP9IEAACWbZ6gwJVcpeUWM8b/fgW1VuKKI9VdmO4+s7vfPQlAvHKSt8xxt0fPWf5bM8b3m7MOnJfn1hbW3R9LctskP58x9beSvKaqdhq/K1ajuzvJd6ZMuXRVXWBJ7axZVV0hyWWmTHlzd39jpOU3/GeKrai7v97dT86wXfmjJLOuOnj1JLcZuy8AYDG6+6Qkd0nyrhlTL5/kA1V11fG7YpW22nGCaccyT03y/JHWtd+xDrr7hO5+VXcflOFvf+Qcd5v3+Cxrs1uS/Ude4zozxr85Z51Z8248Z51lm/XdzXO7+7RFL1pV+yTZfdF1AQCAxevus5J8ZcqUS08+44/puiPXBwAAgE3BfjqwQTm/Y3zO7wAAlk7oIQAAsGx7zTHnrBXUu8aUsZ8lOWwFtVbi5iPVHUV3H9Xd90jywhlTL1NV067Oc47PZ/rfadYBf9iWz8wY99za5Lr7U0luneQnM6beL8nrN1Nw3nZo2ut1xyQHLKuRBZj2eSJJ3jDGolV10Uy/Kh4j6+7Tu/tZSe6c6VeoTZLbLaElAGBBuvuUJHdPcuiMqZdJcnhVXXP8rliFrXacYNq+xzu7+4SR1t1UxzK3ou7+SIa/wxEzpv7aEtphMPb7x6z6X5yzzqdnjN9kzjrLNu397qwk/znSujcbqS4AADCOWcd+Dhpr4aq6ZJKrjFUfAAAANiH76cBG5PyOcTm/AwBYOqGHAADAsu09x5wTV1Bv3yljX51cbWwMm/WHwn+Q5Osz5txyVpHuPinTD9pfcxLkBCv11STHTRm/aVU5nrHJdffnkxyc5JgZU++Z5D+rapfRm2I1Pj5jfNYV3zaSaZ8nkuRLI617syQ1Um1WoLsPS/J3M6bN/IwEAGwskysM3yvJG2dMvWSG4MPNFqC3PfjEjPFNs99RVbtl+rHRsfY7EieJbgjdfXySByY5Y8q0S1SVH3Isx9jH+GfV/+ycdT45Y/ymVbXznLWWadqxlh91989HWnezfncDAADbq8NnjD9wxLV/M35PAwAAAOd2+Ixx++nAenB+x7ic3wEALJ2dPwAAYNkOnGPO0Suot8eUsVlhWqtSVbsmuc0YtcfW3acnecmMaZeas9x7p4ztmOTOc9aBX+ruTvK+KVMuHj/U3xK6+0sZrvb4vRlT75LkrZP3XjaWaduBJLn7UrpYjGmfJ5KRPlNkeH6zcfxLkp4yPu9nJABgA+nuM5LcL8m/z5i6d5LDquqG43fFvCYnTU67Cvb1q+pyy+pnjdZlv6OqbpzheAobQHd/Lcl7Zkyz77Ec9xnr4ipVdc0kB0yZ8o3u/sE8tbr7i0m+P2XKRZPcZwXtLcvSv7uZ8L0EAABsLu/N9O/n7lhVFxtp7QeNVBcAAAA2K/vpwEbk/I5xOb8DAFg6oYcAAMDSVNWFksz64fhPuvv4FZSddoWbs1ZQZyUelGSsL+qW4SMzxvees86bZ4z/7px14Lw8t7YT3f31DMGH35kx9fZJ3j7ZjrBBdPcXknxzypRbV9WVltXPGs26Yt7CP1NU1UXiBJ0Npbt/lOQbU6bM+xkJANhguvusDJ+9XjFj6kWTvLeqbjp+V6zArOMED1tKF2u39P2OiT8YqS6rt6jjs6zNpZIcPFLtB8wYf/8K6719xvijV1hvGZb+3U1V3T7J1ceoDQAAjGPyg+Fp+8k7Z4RjG1V12yTXW3RdAAAA2MzspwMblPM7xuX8DgBg6YQeAgAAy3SvJBeYMeejK6x5ypSxfVZYa6aqqiSPXXTdJfvpjPFd5qzz0STfnjJ+UFXdfM5acG5vTXLilPH7VdUVl9UM4+rub2cIPpwWNJYkt07yzqqadhUxlu81U8Z2SPJny2pkjaZ9nkhG+EyRIZhFkOfGM+1z0ryfkQCADai7z07y0CQvnDF1zyTvrqqDxu+KOb02ydlTxh9dVXsuq5k1WPp+R1VdOsm9F12XNVvU8VnW7pGLLlhVuyb5rRnT3rTCsq+eMX6zDXgsfqnf3Uz8/kh1AQCAcU37zjlJ/mSR54hU1QWSPH9R9QAAAGCLsZ8ObETO7xiP8zsAgKUTeggAACxFVe2Y5C/mmPreFZb+yZSx61bVTiusN8tjkhyw4JrLtveM8ePmKTIJC3jBjGnPm3wJCXPr7uOT/OuUKTsnec5yumEZuvvoJLdK8pUZU2+R5D1VdZHRm2JeL0xyxpTxh1TVgctqZg2mfZ5IkoU+hqq6XJL/t8iaLMy0z0lzfUYCADauHjwqyXNnTN09yTsmV5JnnXX3N5O8Y8qUvZM8bUntrMUvkpw5ZXyMfafnZfZFaFi+hRyfZSHuM8LJ5E9Icpkp4z9O8r6VFOzuDyX58oxpL6+q3VZSd2TTjrVcvqoWemJ8Vd01yZ0XWRMAAFiaVyX5+ZTxCyZ5weRCtYvwJ0muvqBaAAAAsNXYTwc2Iud3jMf5HQDA0gk9BAAAluUvk+w/Y86ZSf5jhXW/OWVszyS3WWG9baqqqyX5u0XVW0c3mDH+rRXUekmSn00Zv362xr8Zy/esJKdPGb9rVbmy0xbS3T/IEHz4hRlTb5zkfVV1sfG7Ypbu/mGmh5TukOS1m+DvNe3zRJLcY1ELTU7yeWWSiyyqJotRVXsl2W/KlJV8RgIANrDuflxmH6/YLcnbquqQ8TtiDn8zY/wxVXX3pXSySt19VpKjpky5dVXtuaj1quq3k9xrUfVYqEUen2XtnlNVCzl/qqoum+RPZ0x70eT9YKVmbbeukuTvV1F3LNOOtVSSX1/UQlV18SQvXVQ9AABgubr7pAwXbpjmDkn+Ya1rVdU9kjx1rXUAAABgq7KfDmxgzu8Yh/M7AIClE3oIAACMrqoekiH0cJbXd/ePV1j+iBnjT13EFcSq6qIZAhl3XWutbdS/QlU9rKouMEb9c62zY5LfnTHtk/PW6+7jkjxlxrTHV9VfzFtzrapqp6q627LWYxzd/a0kz58x7dmTH/AvRVXtWlV3XGONg6uqZ9wevKCWN53JNuDWST49Y+r1kxw2+cKL9fekJCdOGb9SkndMAuWWoqquW1VXWMFdPpNk2hfSv1FVB6yxrXM8NQsMZd4eVNXvT04sGNtDk0z7LDb3ZyQAYOPr7j/L7BOnL5jkvxxnWH/d/ZEkb5oypTIErt9uSS2lqi5SVSv9bD/tWOauSf7fGlr6paq6Xmb/CIFzqarbV9WvLWGdyya565QpxyX5+th98L/cMMlfrbXI5Nj+v2cIzd2WU5P88yqXeFWSL82Y8+iqeswq6y/arO9u/l9VXXCti1TVLklem2SftdYCAADW1T8k+f6MOX9YVf9SVTuvZoGqeliGc7/8hgYAAACms58ObETO7xiH8zsAgKWzIwgAAIymqvaoqpcmedkc00/P6g48v3vG+I0zO5Rvqqq6ZJL3JLnWWurMsGeSlyT5elU9vqr2HGmdZ2T64zgmyadWWPOFST4xY87TqupNIz6uc55vj0nytSQvGGsdluqpSb4zZXyHJK+sqn+efPkxiqq6eFU9Mcm3s6DwAbatu3+W5LZJPj5j6rWTfGDyHs066u5jMvu1caMkn6mqm47ZS1XdtqreliHE8Erz3m9yVdKPTJmyY5LXrGU7VoOnJ1laEPAW8pAk36yql1TVKJ/HJqEwsz6Lvm2MtQGA9dPdT07yxBnTdk7yxqq69xJaYro/THLslPFdk7yzqp60qKt6n5+qunxV/W2SozJ8Vl2JWccyH1dVd1hdZ4PJfte7k1x4LXW2Q9dI8p6q+p+q+o2q2mnRC0x+7PHvmR62/q7uPnPRazPTX6zlZPLJ8+Xfk9xyxtRndfePVrNGd5+d5PeS9Iypz6uqv1rExaDOraquUlUvr6p531s+nOSUKeOXT/LCtfRZVXskeUuGY4kAAMAmNvm+9nFzTH1kkk+s5EIUVbV/Vb05w3lZ593f//zcTQIAAMB2wn46sIE5v2MG53cAAJuB0EMAAGDhquqaVfWsDAFhD53zbn/V3V9f6Vrd/e0kH50x7S+r6q9X80PVqrpzhhDAG5zP8FkrrTeHfZM8K8n3JgeYb7WIg9dVtVtVvSjJn8yY+tLJwfW5TX6E+4AkJ86Yes8kn66q31zUj4araoeqOriqXpLkB0men+QKi6jN+uvu4zM8t2a91h6d5GNVdedFfdlTVReY1HttkqOT/HWSSyyiNrN197FJbpfhy7Nprp7kg1V12dGbYpbnJ3nHjDmXy/D3+ttFhlVW1b5V9edV9ZUk701y51WWevWM8WsneXdV7bfSwlV1mQyBeecXDjnG54mtaOckD0vy+ar6YFU9dAVfxE9VVfdIcniSaVcgPCrJuxaxHgCwsXT332QI05vmAkleV1X3X0JLbEN3fzfDCfPT7JDhIgrvq6pbLGrtqtq1qu5bVYcm+VaSP83qQgXfnOTkKeMXSPKmqrrPKnrcsar+OMlhSfY+z7D9jvndJMnrk3ynqp5RVVdbRNGqulyG/Y5ZJ02/eBHrMdP5nVj+/Kp6zkovrlJVl85wPGJWOO4PkvzNSmqfV3cfnuSfZrWU5C8zXCzkmmtZb3IBh5tNjpF+OcnvZM5zzSY/hHrzjGm/neRlVbXbKnq7SZIjkhxyPsPe8wAAYBPq7jcmefkcU6+T4djPZ6rqiVV1UFVdsqp2rqqdqupiVXXDqvq9qnpnkq8k+fXzqXNCkscu7hEAAADA1mE/HdggnN8xB+d3AACbzcKvTA8AAGxaB60iiK6S7J7hB74XTXKNDOGAF1thnbcl+bsV3ufcnpXkZjPmPDHJIVX1jCRv6e5tHhSd/DvcIcnjs+0ryLw6ww9U9115u3PZPcMB5t9J8qOqeluS9yT5cHd/f94ik2Cm+yT5o8wObPt5kmevptnu/kZV3S/DVXemPY+umOGKSH9bVf+U5J1JjuzuWVcv+qWqunKSmyb5tQwHvPdZTc9sDt390ar6vSQvnDH1uhneS740eW69dyVBqlW1Q4YAvZtlCNu7XZKLrKZnFqO7T6iqQ5K8Ncm0qz/unyFI79bdfdRyuuO8urur6gEZgiqvMWXqThnCQR5XVa9J8tok/zP5onQuVXWRJDdOcqskd8zw+l+EVyd5eqZvV26U5HNV9Q9JXtTdP55WcBKO8fAMwSy7ns+UnyZ5U5JHrKrj7dctJ7cXVNXhSf47yQeTfH7aZ7xzq6oLJbl9hivRHjTHXZ680mBoAGDz6O5nV9VpGU403FaY/o5JXlVVO3f3K5fWHP9Ld7++qq6b5M9mTD04yYeq6uNJXpDksO4+et51JscHr53k5hk+N94myYpP2jyv7j62ql6eZNoVxy+U5D+q6k1J/qG7Pzaj1wsn+Y0kT0hylW1M+5skf7GKlrdnl0ny50n+vKq+nOTQDIGSH51cqGMuVXWDJL+VYd9wWtB6MjxP37fKflmZ/8qwj3/p8/z3P0hy16r66yT/3t2nbqtAVV0iyaMyHHvffcZ6neTB3T3rwkHz+OMk10syK9j1lhkuHPCWJC9L8p7uPmNW8araNcn1M/zQ6DeSXH4NvT47yazA4N9JcsvJdzevnfFvXhn24R+b4SJL57fN/uDkv88KGAUAADamxyS5WmafB5YM3xNfdw1rPSrJd2fMmft8JgAAANiC7KcD6835Hdvg/A4AYDMTeggAAJzjnIC9ZTssyf3mDag5P939pqr6UGYf5Lx+kjcm+XlVfTTJ5zIE/Z2UIYjo0hmCmm6RZM8pdb6b4cu7z6225xW6RJKHTm6pqmOSfDHJtzJcPegXSU7N8OP73TMEtV0lyQFJrrqCdX6vu3++2ia7++1V9bAMV3ObdTWgyyd55uT2i8nf46gMj+XnSU5MsnOGH5NfPMmlklw5w+O5yGp7ZHPq7hdV1T5JnjrH9GtkCDNIVf0oyUeTfD+/em6dnGSXDAEC+2R43e+f4bl1oYU3z5p090lVdZcMVw27w5SpV8gQfHib7v7mcrrjvLr7F5Ogyg8m2W/G9F2SPGRyO7OqPp3kCxlep7+Y3CpDIMRFk1wyQ9DwVTNsQ7YVRLOW/k+pqicmeemMqRfO8H70pKr6TJKPJflhkmMzbIsvluF95aYZtl3TPDzDFU5ZnZ0zBNDcfvL/T56EkXw1w3v/jzO875+V4TPF7hmem1fPcALBrMCRc7wjyb8trGsAYEPq7hdMgg9fnG0f19ghycurapfuftHyuuPcuvvPq+rimRwrm+HGk1uq6qgMn99/lF/te5ya4XPh7hmOwV0mw3G1/TP/58WV+qskD8iwrzPNvZLcq6q+k+H4xtcy9Hz6pN99M/xg4EYZPhtvy39lOCFW6OHqXX1y+5MkZ1fVt5N8KcNx4h8kOSHJaRn2dXfPcMzpahn29y415xonRCD+Mh2X4YT2/zqfsStmeM08r6o+kOTzGfb7T82v9itvmOG1t+Oc6/1dd79njT0nSbr79Kq6R4YLT8w6/r9DhpPH75lhn/mIDM/dozIcgz8rw3vRRTM8b6+X4fjqQs4n6+5PVtWrkzxwxtQrZ/he4blV9bEkn85woYgTMryuLpHhNXiLDN8XbMtxSX47ySvX1jkAALBeJt/Z3i3J+zNckGIsT+3u10wuJjvNmSP2AAAAABua/XRgA3B+h/M7AIAtSOghAACwnl6d5GHdfdoCaj04yWcyBBHNcrEkd5ncVurYJHfu7uOGi86si0tObrddYM2/6e7XrbVId/9rVZ2Y5DUZDlbP46JJ7rzWtdnauvtpVXVskudkdqjmOS6R5B5j9cRyTE6WuHuSNyS565Spl8+vgg+/upzuOK/uPrqqbp4hJG7ek1t2yvBF8o1Ga2xO3f2yyfNt2nPtHBfI2vr+i+5+c1UJPVyc3ZLcYHJblC8meWB3uzosAGwHJp8HT8twMt22TnSsJC+cBB8+b2nNcV6/myG48AkruM++k9u66u6fVtXDM+znzmO/zA6W35ZPJ3lQkr1WeX/+rx2SXGlyW5Qzkty/u7++wJrM0N1vnVx9/onbmHKhJHea3NbijVPWWJXJ+8itkrwvyTXnvNtuSW41uS3THyQ5KMOxu1n2SHK7yW2lTk9yj+7+zjp+dwMAACxAd/+sqg5K8pYkB4+wxJO6++mT/z3rohenjrA+AAAAbBr204H15vyOpXF+BwCwNPOGBAAAACzST5P8dnc/aEGBh+nub2W4ms1C6m3DsUnu0t1HjrjGenhady/soHx3vynDQe5vLqomJEl3Pz9DWOmP17sXlmuyrbhXhi8Rp7l0kg9U1QHjd8W2dPcPMlyZ7TXr3csqPTBDMMiY/r67/3rkNVi7I5Lctrt/vt6NAADL092vTnL/zL46/HOraiWBeyxQD/4kw4VQTlzndlasu9+Y5EkjL3Nkkjt19wkjr8PanJTk3t39tvVuZDv1F0n+bcT6b0zygDGC9Lv7RxmOvxy66NqLNNmnvkuSX4y4zGlJfrO7DxtxDQAAYIm6+7gkv5bkqUnOWlDZnya5z7mCFJLhQq3TCFMAAABgu2c/HdgAnN8xMud3AADLJPQQAABYpl8k+ZskV+7uhR9o7u73Jbltkh8sunaSbyS5SXd/ZITa6+WoJHfv7r9cdOHu/kSS6yX558wOCVi0kzJcRY4tqLvfkeTaSV63Dsv/PMl/r8O6JOnuM5LcL8m/z5h6iSSHVdV1R2+KberuE7r7gUl+M+Nsl2f5aJLvrOaO3X18ktskeesiG5o4I8mjJuEsbFxnJPmHJAdNTjIAALYz3f0fSe6d4arC0zyzqv5iCS2xDd39rxmOQb17HZb/QZL3r/bOkxP3H5XZz7PV+O8kN/N5dsP7cJIbd/cY+5/MYXKy+oMz7AMutHSSf0xyv+4e4zU+LNJ9bJK7J3lckg0bcNrdX0hy8yRfGaH8j5Pcprv/c4TaAADAOurus7r7yUkOSPLmJGevstQpGc5fusbkQhTndpEZ9/3ZKtcEAACALcV+OrCenN+xHM7vAACWReghAAAwtlMz/Mj2wUku191PnFzlaxSTUMLrJ3nPgkqemeTvk1ynu7+6oJrn54tJ7pbkxUm+P+I6yfA3eXqSq4/5g9pJ4NVjMgTU/UfGDT/sJIdleJ5dcrIuW1R3/6i7fzPJTZO8M8PffyxnZria1r2TXKq7nzHiWszQ3WcleVCSV8yYuneS91fVgeN3xTTd/bok+yd5UpJjRl7uqAzbt/27++bd/Y3VFpp8Vvn1JH+exV0V9CNJrt/dL1xQve3JAzM8hz6Rcd/zk+S9GT73PaG7XREWALZj3f1fGT4TzvpM8LSqetr4HbEt3f2N7r5DkjtlCEAf0ykZLsRwSJLLd/fL11Jssn9wiwwXXFmEnyV5WJK7dPeGPUF2g/rXJL+T5E1Jjh95rR8m+a3uvmV3f3HktZihB0/IcLX67y2g5DeT3Km7/3hyLGtUk/6fm+RqSV6a5LQRljkzw4WGDpmciL9i3f3lJAdm9gVN5i6Z4RjhNbp77Pd+AABgHXX3V7r7nkmunOSvMnxnOGt/66QMF8l4bIZjOI/p7p+cz7yLzqgz9nfcAAAAsKnYTwfWi/M75uL8DgBgU9hpvRsAAAA2vbMyHGQ9OcMVV47JcND3y0k+meQT3T3GQdht6u4fJbl9VR2U5IlJ7rCKMj9P8sok/9Td397GnM8n+ek2xrZ1n/PV3WdkCFY7NEmq6npJfi3JzTIEu11iJfXOb4kkH0ryqiRvGDN48v8sPBzsvm9VXSZDKOE9MwRTrtVRSd6XIZzofd394wXUZBPp7o8luWNV7Z/hR+n3yPDlz1p9Nb96bh222i96GEd3n11VD82w7XnklKkXTfLeqrqjL8bWV3efnOTpVfXMDNuA+ya5fZLd1lj6pCQfzK9er5+fXMFvISa1/raqXpnkj5I8IskeKyxzVoaTdJ6X5F3b6O8HST61hla3vO4+MsmRGZ5H+2T4bHeLDJ+TrpG1X9zn6CSvSfKq7v7SGmsBAFtId7+jqu6S5K2Z/vn1L6rqgpOTKlkn3f2OJO+oqusn+a0MoZX7rrVshmOA753cPjjZx1mY7j6iqq6e5DeT/FmGz7gr9Y0kL0jy8m0c9zst0/c7tnWMc7vR3b/IcDz4lVV1gSQHJbl1hmOzN0qy+xqXODXDsd9XJXnn5HgwG0h3v72qrprkoUkel+SKKyzx1STPT/KS7j59we3N1N0/SPK7VfWkDOGn901ywBpKnprhO4V3JnndpP5aezwxyQMmx4iemOFCMyvdpz8pyWuTPHdyvOD8fDXbfs2OeXErAABgRJPzt56S5ClVtWuSq2Q49rNHkgskOTHJcRnOW/v2nN8dX2fK2NkRpgAAAADny346sF6c3/F/OL8DANh0aoG/AwYAANiQJmF7t57cDkiy1+S2R5JTMhxE/WGGg6FHZghP+sQyrtIzr6q6YpLrZbga2pWTXCnJpTI8ht0nt9OTHD+5HZchePEzST6b5FMbKRSwqi6d4QfDBya5apLLZ3g8uyfZNcMPyk+Y3I5P8rMkX0/ylcnti9191PI7Z6ObvFZunOSGGb44v3ySS2YIp9g1w1WrTjjX7cdJvpZfPbe+0N2+DIeRTU5uOTBDcMS1k+yX5HJJ9szwer1Ahu3zOduB45N8J796rX45w7ZgaSERk55vluHzxM0yBBLvleRiGU6kOTHJLzKEjXwlyUeSvFdw6riq6sL51eeJcz4j7ZvhuXTO56QdMzyXjsvwXPpRhuCaz0xuX15kYCYAABvHJEzwxklukF99Vrx4fnWc4JzjaeccJzgmwzHCcx8n+NmSe75mhv2OgzOclHvOscxdMuwnnZjhYiBfzXDc793d/ZVl9ri9qaodk1xrcrtShn2PK2Z4Lp1zbHa3DBcGOufY7LEZnkPn7Hd8prtPWnbv27Oq+k62HXz6r9394Bn3v26SOya5boYw0ktk2M/cKcPr8McZ/sZHZDhx/FMbbd9ycqz0FhmOv+yf4fjL3hmerzvnV+8p57z/fXlyOzLJR7v7lJH72zvDe92tM3z/sXeG97s9M4S1nvPv/LUkX0pyWJIPr8ePDgAAgK2pqg5PcqttDH+zu6+8xHYAAABgu2Y/HbZPzu9wfgcAsP0ReggAAAAAAAAAAGwZaz0pHgAAgK1tcpGD45JcaBtT3trdd19iSwAAALDdsp8O2y/ndwAAbH92WO8GAAAAAAAAAAAAAAAAYEnunm0HKSTJx5bVCAAAAGA/HQAAYHsh9BAAAAAAAAAAAAAAAIDtxWNnjH9gKV0AAAAAif10AACA7YbQQwAAAAAAAAAAAAAAALa8qjowycFTpvw0ySeW0w0AAABs3+ynAwAAbF+EHgIAAAAAAAAAAAAAALClVdWFkrx6xrQ3dPeZy+gHAAAAtmf20wEAALY/Qg8BAAAAAAAAAAAAAADYsqqqkrwwyVVmTH3pEtoBAACA7Zr9dAAAgO2T0EMAAAAAAAAAAAAAAADWVVW9tKoOHqHu7knelOSBM6a+t7s/vej1AQAAYDOynw4AAMCiCT0EAAAAAAAAAAAAAABgvf1aksOq6n+q6l5VdcG1Fqyq2yT5aJJ7zJh6dpInrnU9AAAA2ELspwMAALBQQg8BAAAAAAAAAAAAAADYKG6S5I1JflRV/1ZVd6uqvee9c1XtU1X3q6r/SfK+JNea427/0t1HrLJfAAAA2MrspwMAALAQO613AwAAAAAAAAAAAAAAAHAeF07yoMktVfXdJJ9K8t0kv0hybJLTk1xscrt4kgOTXG2F63w6yRMW0jEAAABsXfbTAQAAWBOhhwAAAAAAAAAAAAAAAGx0l5/cFunbSX69u09ZcF0AAADY6uynAwAAsCI7rHcDAAAAAAAAAAAAAAAAsGRfSnJQdx+93o0AAAAA9tMBAAC2OqGHAAAAAAAAAAAAAAAAbE9ekeTA7v7eejcCAAAA2E8HAADYHgg9BAAAAAAAAAAAAAAAYL0dmuT4kdf4eJKDuvsh3X3yyGsBAADAZmY/HQAAgIUSeggAAAAAAAAAAAAAAMC66u7HJtknyd2SvCzJ1xZU+qdJXpHkZt19k+7+0ILqAgAAwJZlPx0AAIBF22m9GwAAAAAAAAAAAAAAAIDuPi3JoZNbqmqvJDdNcs0k+yW5QpLLJblwkt2SXCjJBZKcnuSUJD9LcnSSbyX5bJIjkhzR3Wcv8WEAAADAlmA/HQAAgEWq7l7vHgAAAAAAAAAAAAAAAAAAAAAAAACALWiH9W4AAAAAAAAAAAAAAAAAAAAAAAAAANiahB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAAAAAAAAAAAAAAAKMQeggAAAAAAAAAAAAAAAAAAAAAAAAAjELoIQAAAAAAAAAAAAAAAAAAAAAAAAAwCqGHAAAAAAAAAAAAAAAAAAAAAAAAAMAohB4CAAAAAAAAAAAALElVXa+q/qiqXl1VR1TV96rqpKo6s6p6W7cp9Q6fcr/Dl/jQgAWqqoOnvSdU1cHr3SMAAAAAAAAAAMxrp/VuAAAAAAAAAAAAADaqqtoryX5J9k6y2+S2c5LTkpyS5BdJfpjk+9194jq1yQZXVbsleWSSRye50jq3AwAAAAAAAAAAAEsl9BAAAAAAAAAAAACSVNVlkhyc5CZJbprkqkl2X8H9v5/kyCSfSvKBJB8VhEhVHZzklUn2XddGAAAAAAAAAAAAYJ0IPQQAAAAAAAAAAGC7VVX7JLl/kvtkCDqsNZS7zOR2hyRPTHJ6Vb0vyZuSvLG7j1tju2wyVfXAJK+I8zUBAAAAAAAAAADYju2w3g0AAAAAAAAAAADAslXVVarqRUmOSvLsJDfL2gIPz8/OSe6Y5KVJflBVL6uq6yx4DTaoqrpVBB4CAAAAAAAAAACA0EMAAAAAAAAAAAC2H1W1Z1U9L8mXkjw8yQWXtPRuSR6S5DNV9Wbhh1tbVe0SgYewaVXVflXVU24PXu8eAQAAAAAAAABgM3EyHQAAAAAAAAAAANuFqrp7khcn2Wc920jy60nuVlUvSvIX3f3zdeyHcTwsyRVmzOkkH0/yySTfS3JckjNH7gsAAAAAAAAAAACWTughAAAAAAAAAAAAW1pV7ZTkGUmesN69nMsOSR6V5IJJHrLOvbB4j5wx/r4kj+zubyyjGQAAAAAAAAAAAFhPQg8BAAAAAAAAAADYsqpq1yRvTnKHFd71+CT/k+QjSb6c5NtJvp/k5CQnJdk5yR5JLpLkSkn2T3JgklsludwK1tlhhX2xwVXV/kkOmDLlE0kO6e4zl9QSAAAAAAAAAAAArCuhhwAAAAAAAAAAAGxJVbV7krdlCCKcx1lJ/jPJq5O8s7tPnzL3lMntx0m+luQd51r3qknuP7ldeeWds8kdPGP8LwUeAgAAAAAAAAAAsD0ReggAAAAAAAAAAMCWU1UXSPLWzB94+JokT+nub6x17e7+apInV9VTktw1yZ8nucla67JpXGfK2IlJ3rPIxbr74EXWAzaG7j48Sa13HwAAAAAAAAAAsAg7rHcDAAAAAAAAAAAAMIJ/TnLrOeZ9N8ntuvuBiwg8PLcevLW7b5rk7km+s8j6bFhXnDL2he4+e2mdAAAAAAAAAAAAwAYg9BAAAAAAAAAAAIAtpaoekeR355j6oSQ37O73jtxSuvutSa6R5O+T9Njrsa4uMWXsmKV1AQAAAAAAAAAAABuE0EMAAAAAAAAAAAC2jKq6QpJ/nGPqu5Pcrrt/MnJLv9Tdp3T3nyQ5JMmPlrUuS7f7lLETl9YFAAAAAAAAAAAAbBBCDwEAAAAAAAAAANhKXpbkQjPmfCzJPbr7tCX0839097uT3CDJ59djfUa385Sxs5fWBQAAAAAAAAAAAGwQQg8BAAAAAAAAAADYEqrq7kluPWPaz5L8RnefvISWtqm7v5/klknev559MIpa7wYAAAAAAAAAAABgI9lpvRsAAAAAAAAAAACAtaqqHZI8bY6pj+zuo8fuZx7dfXySf1vvPmBRqmrvJFdNcokkuyc5K8nxSX6a5Mvdfez6dQewtUw++1wpyV5J9kxy4SQXSnJakpMntxOTHJ3kB9199jq1CgAAAAAAAAAg9BAAAAAAAAAAAIAt4R5JrjVjznu6+43LaGa9VdUFktw4yS2SXC/JFZJcNkMQ3W5JTk9yUoYwum8l+UqSDyf5UHf/dD16XqmqOiDJXTI8vmsm2SdD4NMOGYKefpThsX0yyeFJDu/usxa09sOmDO8+ZWz/Gff9P7r7pSuZv0xVVUluk+R+SW6XZN8Z849O8o4kb07y7m0FcFXVFZNccVt1uvu9q+2Z/6uqLprh73ezDKGVV05ykSR7ZHg9HZ/k+0mO7O4HrGGdXZLcIMnVk1xtcrt8htfthSfrdZJTkxw3WfOoJJ/L8Dr+UHefutr1WR3bk3G3Jyvsc6ckhyS506TXa2f4G8zj9Ml78NeSfGxy+3h3HzdGrwAAAAAAAAAA51Xdvd49AAAAAAAAAAAAwJpU1XuT3HbGtOt392eW0c96qarrJ3lkknsludgqSpyV5H1J/jXJ68cKdaqqg5McNmXKrbv78PO53wWT/HaSx2cIZ1uJnyR5YZLndPfPV3jf8/axtJMvu7tm9HJ4klttY/gD3X3wonuahB3+RpKnJrnKKst8Nckzuvvfzqf+U5I8eVt3nPVvsi2rfd4twliPaVL78KzwOTD5G94xyWOT3D5DuNssx3X3RVbQVyW5aYZgzFtnCFW84Lz3Px+nJHlPkhcnece2QjNXqqpemeF9ZVmO6u79Zk1az+frZH3bk21b2PZkHlV1mSR/nOT+GQIZF+XsDAGVb0jyxu4+ZoG1AQAAAAAAAAD+l3lOUAIAAAAAAAAAAIANq6qulCFUa5r3bOXAw6q6elW9PcmnkvxuVhdQlSQ7ZghAe02Sr1XV/RfU4ppV1SFJvpIhaGqlAVVJcvEkT8oGe1ybTVVdNsn7k7wuqw88TIa/4b9W1Xur6vILaY65VNX1kvxPkrcnOSQLPp+4qg6sqmclOTrJR5I8LcN79FoCD5Nk1yR3S/K2JF+oqrussR7nw/ZkLkvZnlTVjlX1+Ay9Pi6LDTxMhtf+QUmen+T7VbXbgusDAAAAAAAAAPyS0EMAAAAAAAAAAAA2u/snqRlz/mkZjSxbVe1QVU9K8tkkd1pw+SsmeU1VvauqLrXg2nObhD49L8k7kuy7gJJ7ZXhcz66qWc8bzqWqbpnkc0kOXmDZ2yb5+CSIj5FV1e8nOSLJjUeq/+Ikn0jy+CSXGWONiWskObSqXl1VFx5xne2G7cmqjLY9qaqLJ/lQkmcl2X2Rtbdhh/htAQAAAAAAAAAwIicmAAAAAAAAAAAAsNndecb4sUneuYQ+lmoS9PXWJE9NsvOIS90+ySer6kYjrnG+quqCSQ5N8tgRyj8uyXNHqLslVdUdk7w7ycVGKH/JJB+oqmuPUJuJqnpOhuf8jiMus+wAwgck+VBVXXLJ624ptidr9rgscHtSVRdL8oEkN11UTQAAAAAAAACA9Sb0EAAAAAAAAAAAgE2rqvZOcuCMaYd29+nL6GdZqmqPJO/K7MDHRbl0kvdV1c2XtF6qaqck/5HkjiMu89iqesiI9beESUDZG5NccMRl9khyaFVdYsQ1tltV9ZdJ/mC9+xjJtZMcVlUXWe9GNiPbk4VZyPakqirJa5Ncfe0tAQAAAAAAAABsHDutdwMAAAAAAAAAAACwBrfK7AsAv38ZjSxLVV0gyaFJbjLnXc5O8snJ7dtJTkyyc5JLJDkgycFJLjxHnd2TvKOqbtHdn19h26vxnCR3nTL+4ySHJfl+kp8kOT7JXhke10FJrjXnOs+rqnd19/dX3+rWVVV7JXlzkt3mmH5GkrcleXuSTyX5Tn71fNs7Q4jXwUnum+QK53P/yyd5UZLPrq1rzq2q7pTkr6ZMOTXJx5N8Icl3k5yQ4RzjPZNcNcmNk1xlgS39Isnnk3w9ybFJjpvczp6suWeSKyW5YZJ956x5tQxBcWOG2m05tie/tJG2J7+d5PZzzDs1yQeTvCfJlzO8nn6e5OQkp2UIkj3n9bRvhnDQaye5fpIrr6E/AAAAAAAAAIBVEXoIAAAAAAAAAADAZnaDOeYcPnYTS/aPGcIeZzl+MvfF3X3MtiZNQq/ukeTPk1x3Rs09kvxnVd2wu4+dq9vVuVeS3zuf/35WkpcleUmST3V3b6tAVV0+yV8m+Z1MD8a8UJJnZAiamkt315R1v5NtB7X9a3c/eN51NoiXJbn0HPNenuRJ3f2D8xk7M0OY3neTvKuqnpjkPkn+PkPQ4bndPecfiMjqXCTD6+X8HJnkmUn+s7tPmlakqg5I8ohV9vDjDGGYb0vy6e4+at47VtWlkjwoyUMzO3jxkKp6WHe/dIX9vTzJh8/z3/ZK8rdT7vOKJB9d4TrnOGGV9xuD7ck6b0/Os84OSZ48Y9opGd47n9vdP58y79jJLRlCRg891zr7J7nb5HbLJNvcpgEAAAAAAAAALEpNOT8DAAAAAAAAAAAANrSqekeSQ6ZM+Xl377WsfsZWVbdP8q45pr4zyYO7+0crqL1Dkj/KENg066LKr+ru35q39vmsdXCSw1Z4t8OTPKq7v7LCtQ7KELa2x5RpZya5YncfvcKezm+972SJoYdVdXi2HVr2ge4+eA2175rkrTOmnZTk/t09a9751b9IklcluctK7jctdHLGegdn+vPu1t19+Gpqz7H2UzIlzGy1j2lS+/DMF1x3jtOS/EmS508LeltlL69Lct8kpyb5tySvTvKR7j57jXV3SPL7SZ6eIVhuW36WZL/uPnGN6+2X5NtTpvxOd79yLWvM0cPBGfH5anuy8bYnc/xNfpDkLt39mZXWnrLm1ZM8LkO46D5rfe0AAAAAAAAAAGzLtCtNAgAAAAAAAAAAwEZ3rRnjX1pKF0tQVRdM8oI5pr4wyZ1WElCVJN19dnf/fZK7Zggsm+ZBVXWbldRfo1cnuf1KA6qSpLs/mCEY87Qp03ZK8rBV9rYlVdWOSf5xxrRTMvxdVhx4mCTdfWySeyR582ruz6ocl+Q23f28RQceTvwkyVOTXL67H9HdH1pr4GHyy/en5yS5YZJjpkzdK8mj17reVmd7smG3J3eaMnZ2kvstMvAwSbr7y939iCSXy/CeDgAAAAAAAAAwCqGHAAAAAAAAAAAAbEpVtVOSS82YtuJQow3s4UmuNGPO67v7UWsJM+vudya5f5JZNf52tWus0L8l+a3uPmO1Bbr7o0n+Yca0+6y2/hZ1ryT7z5jziMm/7ap195lJHpjki2upw1zOSnL3tf7Npunux3b3k7v7JyPV/0qS2yQ5dsq0h4+x9hZje7JKI29PbjRl7F3d/aFV1p2pu3/W3WeNVR8AAAAAAAAAQOghAAAAAAAAAAAAm9WlM/s8uB8so5GxVdXOSZ4wY9rRSR6yiPW6+81J/mnGtAOr6vaLWG+KryV59FpCt87lr5P8aMr41avqcgtYZ6t43Izx93f3qxaxUHefnOT3FlGLqZ7b3R9Y7ybWqru/nORJU6Zcqaputqx+Nhvbkw29PbnilLG3rqIeAAAAAAAAAMCGIfQQAAAAAAAAAACAzeoyc8w5ZvQuluOuSS47Y87jJ+Fxi/KXSX48Y86jFrje+fnt7j5pEYW6+5Qkr5sx7RaLWGuzq6orJbnplClnJHnMItechPH9+yJr8r98J9ODAjebf0nyjSnjhyyrkU3I9mSNRtyeXHTK2HdXUQ8AAAAAAAAAYMMQeggAAAAAAAAAAMBmteccc346ehfL8aAZ44d195sWuWB3H5vkiTOm3bmq9l7kuufyge7+2IJrvnbG+HUWvN5mdZ8Z44d295dHWPeZI9Rk8KwFh9itq+4+K8l/Tplym2X1sgnZnizGsrcnOy24HgAAAAAAAADAUgk9BAAAAAAAAAAAYLO64BxzTh29i5FV1a5JDpkx7Z9HWv7VSY6dMn6BJHceae1nj1Dz00lOnzJ+tRHW3IxuN2P81WMs2t2fS3LkGLW3c6cn+ff1bmIE75gydp2qqqV1sknYnizUGNuTn00Zu+4q6gEAAAAAAAAAbBhCDwEAAAAAAAAAANisdp1jzqYPPUxyyyS7TBn/RZK3jbFwd5+W5D9mTPu1EZY+NdMDzValu89I8pUpUy636DU3m6raOclNp0w5NsnbR2zhVSPW3l69rbunhaltVkdNGds9yX5L6mMzsT1ZkJG2Jz+ZMvaQqtptFTUBAAAAAAAAADYEoYcAAAAAAAAAAABsVjvNMefM0bsY3y1njL9pEiY1llkhdLP6W40juvv0Eeom00Oq9hlpzc3kapkeKPo/I/5tkuQDI9beXn1wvRsYyTEzxvdbRhObjO3JYi16e/KJKWP7Jnl5Vc3z2QcAAAAAAAAAYMNx0gMAAAAAAAAAAACb1TzBTLuM3sX4rjNj/MMjr//xJKcn2Xkb4/tW1Z7dfdwC1/zoAmud17Q+9xxx3c3imjPGPzvy+l9IcnZc2HuRPr3eDZyfqqokl05yqSR7J7lwhvfsnZPUApa41AJqbDW2J4u16O3J+5I8bMr4fZNcuaoe390fWkV9AAAAAAAAAIB1I/QQAAAAAAAAAACAzeqUOeZshdDDA2aMf2bMxbv7jKr6YpLrTZl2rSw2LOu7C6x1XidMGdsKz5e1uuKM8c+OuXh3n1xVX09y1THX2Y50xg+qnEtVXTzJnZLcLMmBGf7Gu4245F4j1t6sbE8Wa9Hbk0OT/CTJxafMuUGSD1bVZ5K8PMlbuvt7q1gLAAAAAAAAAGCpXAUXAAAAAAAAAACAzWqe0MPdR+9iRFVVSS43ZcppSb68hFZmBWHtu+D1frHgeud24pSxnUdcd7O49Izxry+hh68tYY3txS+6e1ow26iqaqequl9VvT/JMUlemeThGULvxgw8TJJdR66/qdiejGKh25PuPinJP845/XpJnp/ku1X1qar6+6q6c1XtudJ1AQAAAAAAAACWYaf1bgAAAAAAAAAAAABW6adzzNln9C7GtU+mn+v33e4+Ywl9zAq6mxWUt1I/X3C9c+sRa28Fs14zxy2hh+OXsMb2Yhl/r/NVVfdK8rdJrrxOLeyyTutuVLYnizfG9uTZSe6W5GZzzq8k15/c/jjJ2VX1pSQfTfKRJB/q7m+P0CcAAAAAAAAAwIoIPQQAAAAAAAAAAGCz+t4ccy45ehfj2nvG+LICzWatc/EFr3fagusxv91mjC8jkFDo4eIs/d+yqvZI8rIk91n22uex4zqvv9HYnmwC3X36JDD0g0n2X0WJHZIcMLk9PEmq6ptJ3pHkTUk+2N1nL6hdAAAAAAAAAIC57bDeDQAAAAAAAAAAAMBqdPdPk5w6Y9q+y+hlRLvOGF9WoNmskKpZfbJ57DJj/IQl9CD0cHGW+m9ZVfsk+VDWP/CQ/8v2ZJPo7mOS3ChDUOEiXCnJY5IcluSoqnpKVS06XBIAAAAAAAAAYCqhhwAAAAAAAAAAAGxm35oxfo2ldDGeC84Y3yghVbP6ZOvoJaxx9hLW2F4s7d+yqi6U5O1JrrOsNVkR25NNpLuPTXLnJA9K8p0Flr5skidnCD/8m6raY4G1AQAAAAAAAAC2SeghAAAAAAAAAAAAm9lnZ4xfpap2WkYjsEWcNmP8wkvoYc8lrMHi/WOSG84596wkRyT5lySPS3K3JDdKcoUkF0+yW5Kdu7um3Rb/EGDj6MGrk1w1yUOSfGiB5XdN8mdJvlBVN1tgXQAAAAAAAACA8+VETgAAAAAAAAAAADazzyS5/5TxXZLcIMnHl9POwp06Y3wZAXTJ7BC6WX2yeZw8Y/zCSX46cg9CDzeZqjowySPmmPrJJP+c5M3dfdwa1xR6uDK2J5tUd5+e5BVJXlFVV0py1yS3SXJQ1v5+uW+Sw6rqnt399jXWAgAAAAAAAADYph3WuwEAAAAAAAAAAABYg0/MMefgsZsY0SkzxjdKSNWsPtk8fjxjfBnPuWU9r1mcv5wxfmaSP+zuA7v7lWsNPJwQjrkytidbQHd/s7uf0913S7JXkgOT/FGStyT5ySrL7pzkdVV1ncV0CQAAAAAAAADwfwk9BAAAAAAAAAAAYDP7nyTHz5hzyDIaGcmsAKONElI1KyiPzeOHM8b3X0IPy1hj2XZZ7wbGUlWXSnLHGdN+o7ufveClL7rgelud7ckW091ndfcnu/tZ3X2P7t4nydWSPCrJfyU5cQXldk/y6qry+wIAAAAAAAAAYBROSgAAAAAAAAAAAGDT6u4zkrxnxrSDJqFcm9FPkpw5ZfzyVbXTEvq48ozxWUF5bB7fmjF+3TEXr6pdM17oYc9afqR1k2SvEWuvt7sk2XHK+Eu6+80jrHuxEWpuZbYn24Hu/mp3v7C7fz3JxZPcM8l/Z/b7X5IckOR+I7YHAAAAAAAAAGzHhB4CAAAAAAAAAACw2b11xvgOSR6wjEYWrbs7yXenTLlgkqsvoZXrzRg/agk9sBxfnDE+67mwVtfO9AC9tZgW+JYku420brK1A/puMWP8mSOte8WR6m5Jtifbn+4+tbvf3N13zvDe+qE57vbokdsCAAAAAAAAALZTQg8BAAAAAAAAAADY7N6Y5PgZcx5bVTsto5kRHDljfNQQuqq6QJIDZkz7wpg9sFRfTnLqlPGbVNXOI65/0Ii1T5sxfuER177siLXX2zWmjH22u78x0ro3H6nuVmZ7sp3q7iOT3DbJ22ZMvXFV7bGElgAAAAAAAACA7YzQQwAAAAAAAAAAADa17j45yWtmTLt8kgctoZ0xfG7G+NjBXzdMMi3k7qjuPm7kHliS7j49yf9MmXLRJHcasYUHjlh71vN0zzEWrapdM3KY3Drbd8rYl0ZcV+jhytmebMe6+4wkD07y8ynTdkpy06U0BAAAAAAAAABsV4QeAgAAAAAAAAAAsBX8U5KzZ8z566raYxnNLNgHZ4zfu6qmhUit1aywyA+MuDbr4z0zxkcJJqyqaye59hi1J348Y/xqI617YKYHvW12095Xjxljwaq6TJLrjlF74qwZ4xcYce0x2Z5s57r7Z0leN2PapZbRCwAAAAAAAACwfRF6CAAAAAAAAAAAwKbX3V9K8u8zpl0qyTOW0M7cqmrXOaZ9OMmpU8YvluROi+nof5uEX913xrT3jrE26+oNM8bvVlVXHWHdJ4xQ85e6+7gkx0+ZMlbg4j1GqrtRTAvJmxUeuFq/l2SnkWonyekzxud5796IbE9Iko/MGN97KV0AAAAAAAAAANsVoYcAAAAAAAAAAABsFU9OcsaMOY+pqnsuo5lZquqQJM+cNa+7T03yjhnTHrWQpv6v+2UIwdqW05P890hrs066+xtJPj5lygWSPH+Ra1bVLZM8cJE1t+GrU8ZuUFW7LHKxqrpwkocusuYGdMqUsX0WvdgkLPZ3F133PE6YMX7hkdcfhe0JEz+dMb7Q90EAAAAAAAAAgEToIQAAAAAAAAAAAFtEd38ryV/PMfWVVXXg2P1MU1WPTHJokj3mvMurZozfvqrutrau/req2iPJ386Y9rbu/tki12XDeM6M8dtV1QMWsVBVXTDJPy+i1hw+O2XswknutOD1fi/zv843q59MGRvjvfbpSfYeoe4vdffJSU6eMuWKY64/MtsTZr1+jltKFwAAAAAAAADAdkXoIQAAAAAAAAAAAFvJM5J8bsacPZK8q6pusIR+/pequnBVvTbJvyTZaQV3fVuSo2fMefYkPG5RnpLkUjPmvGCB67GxvCHJN2bMeXFV3Xgti1TVjkn+Lcm11lJnBT40Y/whi1qoqq6d5MmLqreBfXPK2DWq6iqLWqiqDk7y+EXVm2Hae+41ltTDGGxPmPX551tL6QIAAAAAAAAA2K4IPQQAAAAAAAAAAGDL6O4zkjwwyYkzpl40yQer6r7jdzWoqrtkCGS830rvO3lcz5wx7YpJXriK1v6PSa+PmzHtY939vkWsx8bT3WclecKMabsleW9V3Xk1a1TVhZP8Z5L7rOb+q/TuJGdPGb9LVd1urYtU1R5JXptkl7XW2gSOmDH+9EUsUlX7Jnl1klpEvTl8ZcrYgVV18SX1sVC2JxtTVd2wqu5dVaM+vyfvTQ+cMqWTfGrMHgAAAAAAAACA7ZPQQwAAAAAAAAAAALaU7j4yyf0zPdgsGQLbXldVr6qqvcfqp6oOqKq3Jjk0yX5rKPWSJF+fMee3q+pZa1gjVXXbJK/P7HMM/2wt67DxdfdbkrxtxrTdkxxaVS+pqkvNU7cG984QAnq385ny+RU1ugLd/aMkH5wx7YVVdcnVrjH5d/hQkmustsYm8+4Z4/epqoesZYGqumqS9ye5zFrqrNDHp4ztkM39Hmh7svFcNskbkhxZVQ+tqgsueoFJoOKLk+wzZdoR3f3jRa8NAAAAAAAAACD0EAAAAAAAAAAAgC2nuw9N8rg5pz8wyder6mmLCj+chLodXFX/lSHA7a5rrdndpyV59BxTH19Vb66qvVZSv6p2qKrHJXlHhkDIaV7Z3R9YSX02rYckOWbGnErysCTfqao3VNXvVNW1q2rPqtqxqi5YVZerqttV1dMzhK29IecfAvrWJG9e5AM4Hy+cMX7FJO+vqsuutHBVHZLkY0mus5rGNqkPJjl6xpwXV9VjVlO8qn47QwDhFc8zdNZq6q3Ae2eMP66q/raqLjpyHwtne7KhXSPJS5N8r6qeV1U3XETRqrpYkrckud+MqS9exHoAAAAAAAAAAOcl9BAAAAAAAAAAAIAtqbufn+QxSXqO6RdJ8hcZQobeUlX3r6pLr2S9qrrQJNTtmUmOSnJYkrtlCIRbiO5+b5JnzzH11zMEOf75rCDHqtqpqu6V5IhJ7QvMqP31JH8wRw9sAd39kyT3SHLKHNN3TnLvJC9P8rkkxyY5c3Lf7yZ5d5L/l+RK27j/0UkevraO5/LGDM/jaa6e5EtV9SdVtee0iZNQx0Oq6n0ZQt4uf54pxyX5t1V3u8F191lJnjtj2o5Jnl9V76uq21fV1PfFyb/p/arqk0lemeT8/gZ/s6qG59TdRyT51pQpOyT50yTHTB7XcyfPl9+rqodNud13zL7nZXuy4e2V5LFJjqiqb08CEO++0nDmqrp6VT0tw3P5bjOmfy1b+L0KAAAAAAAAAFhfO613AwAAAAAAAAAAADCW7v7nqjohyYuT7DLHXXZJcvfJLVV1VJIvJfl2kh8kOSnJyRnC3XZPctEMAW77ZwhJmxXwtAh/kuQ6SW4zY95FkzwjydOq6hNJPpXkO0lOzND/PkkOSHJwhtDHeRyf5J7dffxKm2bz6u6PVdVvZAgLnOd1tBonJLlrd/9oRh7emWtdqLvPqqo/THLojKl7JPm7JE+vqg8l+UySH0963SvJxTO89m+VZLcpdR6W4bW2lf1TkkcmufKMebeZ3H5YVR9N8sUkv8gQjHmhJJdNcq0kN8v0f9NPJXlqhrDaMT0nyfNmzNk5v3pc8zgqyevX0NMi2Z5sDvtlCEB8bJJU1dEZXjvfyfDZ5Lgkp2b4W1wow/vTVZNce3LfeZyZ5KHdfcbi2gYAAAAAAAAA+BWhhwAAAAAAAAAAAGxp3f1vVXVkhsC2K6zw7vtObmM4PclHV3qn7j6zqu6e5N1JbjrHXXaczJtn7jQnJrljdx+5xjpsQt39tqq6Q5K3ZP5Qs3n9KMmduvtzk/9/wSlzT13EgpPH87IkD51j+gWyslC7c/uz7n5jVW3p0MPuPq2qHpTkQ5nv/ORLJbnX5LZSRyW5W3efMSMgcxFelOQRSa459kLrwfZk07rc5LZIj+nuDy+4JgAAAAAAAADAL+2w3g0AAAAAAAAAAADA2Lr700munyHA6ux1bidJ/jvJAd394tXcubtPTHKHJG9baFfb9v0kt+nuFYc0snV09weSXCdDsN2iHJ7kJpPX6DkuOmX+QkIPJx6bxT6W83pqd//diPU3lO7+WJKHJOkRl/lekkO6+wcjrvFL3X16knsm+cky1lsPtifbvTOTPLK7X7TejQAAAAAAAAAAW5vQQwAAAAAAAAAAALYL3X1sdz8yyY0ybtDZNO9JcvPuvnN3f30thbr7hCR3T/KkJKcvorlteFeSA7v7iBHXYJPo7u8muVWS+ydZy3P4G0kemiH87DvnGbvIlPv9bA1r/i/dfUqSu2R4XS7S6Uke2t1PXnDdDa+7X5UhJPC4EcofkeRG3f2VEWpvU3d/LclNknx8mesuk+3JduvIJAcLPAQAAAAAAAAAlkHoIQAAAAAAAAAAANuV7v5Udx+U5KZJ3pjkzJGXPC7JC5Ic0N237+6PLqpwd5/d3U9Pct0k/72ouhPfSvLA7j6ku3+44NpsYj14bZKrJrlDklckOXqOu34vycuS3DXJVbv75d3d5zPvolNqHLPSfqfp7uOT3DHJU7KYsLePJ7lBd798AbU2pe5+S5IDk3xqQSVPTvJnSW62Xu9F3f2tDNuM+yX5cJLze95uarYn6+49Se6b5FVJfjryWscmeXyS63X3R0ZeCwAAAAAAAAAgSbLTejcAAAAAAAAAAAAA66G7P5bkPlW1V5JfT3LvJLdMcqEFlP92kvcneXOS93T3IsLUtqm7v5zkzlV1vSSPyvBYpgXHbctZGfp+RZL/6O6zFtflduOH2fb5mT9fZiNjmwQWvntyS1XtkyEIcZ8ku2d4Pp2QIcDrS939izlLX3vK2PdX3fA2TJ7nf1VVr07yp0nun5W/D3wsyd8neUt3n73gFjed7v56VR2YIeDyz5PcZBVlfpDkxUle2N0/2sacacGKP1jFmts0eb6/Psnrq+qSSW6dIdzxqkkun+TiSS6cZJds4gvT256sj+4+Kcl/JPmPqtohw2vmthnCNm+S1f0Nzu3MDO/Vr87wPnXKGusBAAAAAAAAAKxInf8FcgEAAAAAAAAAAGD7U1U7ZQhcu3GSqyXZL8m+GcKsdpvcdkpyWpJTkvwiQ8jd95J8OcmRSY7o7qOX3fu5VdXOGUKSbpHkOkmulOSyGYLodk1yRpKTkvwkybcy9P7RJB/s7p+sR8+QJFW1X4bQ0G15Unc/feQedktyxwwhqNfN8D6wd4bXzulJjs8QvvjFJJ9Icmh3f2fMnja7qrpShpDAW2cICdxrctstyckZwjG/n+SrST6f5L1JPttOdF53tifrr6oqw2eS6yS5coa/wZWSXCLJHhn+Frtl+GxyXIb3qOOSfD3JZya3T3f3scvuHQAAAAAAAADgHEIPAQAAAAAAAAAAANgQqup+SV47Zco9u/vNy+oHAAAAAAAAAACAtdthvRsAAAAAAAAAAAAAgIn7zxj/2FK6AAAAAAAAAAAAYGGqu9e7BwAAAAAAAAAAAAC2c1V1hSTfyLYv6v317r7KElsCAAAAAAAAAABgAbZ1UhgAAAAAAAAAAAAALNMfZfq5rW9fViMAAAAAAAAAAAAsTnX3evcAAAAAAAAAAAAAwHasqm6f5J1Jasq0G3f3J5bUEgAAAAAAAAAAAAsi9BAAAAAAAAAAAACAdVNVl0nyqSSXmDLtC9197SW1BAAAAAAAAAAAwALtsN4NAAAAAAAAAAAAALBxVdWNquofqurSI9Q+MMknMj3wMEn+btFrAwAAAAAAAAAAsBxCDwEAAAAAAAAAAACYZrckf5TkW1X1oqq67loLVtVuVfVHST6YZFaY4meTvG6tawIAAAAAAAAAALA+qrvXuwcAAAAAAAAAAAAANqiqOjjJYef5z19L8vokhyb5XHefPkedSnJAkrsn+YMke8+x/NlJbtbdH19BywAAAAAAAAAAAGwgQg8BAAAAAAAAAAAA2KZthB6e2xlJvpDk80l+kuQXSY5NskuSi01ul09y88n/Xok/7e5nrvA+AAAAAAAAAAAAbCA7rXcDAAAAAAAAAAAAAGxqF0hy/cltkV4i8BAAAAAAAAAAAGDz22G9GwAAAAAAAAAAAACA83hukkesdxMAAAAAAAAAAACs3U7r3QAAAAAAAAAAAAAATByX5JHd/br1bgQAAAAAAAAAAIDF2GG9GwAAAAAAAAAAAABgQzs6yUeS9IhrnJ7khUn2F3gIAAAAAAAAAACwtVT3mOefAQAAAAAAAAAAALAVVNWlktwryR2T3CTJxRZQ9vNJ/iPJS7v7RwuoBwAAAAAAAAAAwAYj9BAAAAAAAAAAAACAFamqSnLVJDdKsn+SKyTZL8klk1woyW6TWyU5JclJSY5JcnSSryT5dJKPdPd3l907AAAAAAAAAAAAyyX0EAAAAAAAAAAAAAAAAAAAAAAAAAAYxQ7r3QAAAAAAAAAAAAAAAAAAAAAAAAAAsDUJPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAAAAAAAAAAAAAAAAAAAAAGAUQg8BAAAAAAAAAAAAAAAAAAAAAAAAgFEIPQQAAAAAAAAAAAAAAAAAAAAAAAAARiH0EAAAAAAAAAAAAAAAAAAAAAAAAAAYhdBDAAAAAID/384dCwAAAAAM8rcexb4CCQAAAAAAAAAAAAAAAAAAWEgPAQAAAAAAAAAAAAAAAAAAAAAAAICF9BAAAAAAAAAAAAAAAAAAAAAAAAAAWATwsoh+BBCJdwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}